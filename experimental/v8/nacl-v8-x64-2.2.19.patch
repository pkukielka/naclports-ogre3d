Index: test/cctest/test-heap-profiler.cc
===================================================================
--- test/cctest/test-heap-profiler.cc	(revision 4925)
+++ test/cctest/test-heap-profiler.cc	(working copy)
@@ -24,7 +24,11 @@
   v8::Script::Compile(v8::String::New(src))->Run();
 }
 
+bool operator== (const JSObjectsCluster& a, const JSObjectsCluster& b) {
+  return memcmp(&a, &b, sizeof(JSObjectsCluster))==0;
+}
 
+
 namespace {
 
 class ConstructorHeapProfileTestHelper : public i::ConstructorHeapProfile {
Index: test/cctest/test-macro-assembler-x64.cc
===================================================================
--- test/cctest/test-macro-assembler-x64.cc	(revision 4925)
+++ test/cctest/test-macro-assembler-x64.cc	(working copy)
@@ -1743,7 +1743,7 @@
     CHECK(Smi::IsValid(result));
     __ Move(r8, Smi::FromInt(result));
     __ Move(rcx, Smi::FromInt(x));
-    __ SmiShiftLeftConstant(r9, rcx, shift);
+    __ SmiShiftLeftConstant(r9, rcx, shift, exit);
 
     __ incq(rax);
     __ SmiCompare(r9, r8);
@@ -1751,7 +1751,7 @@
 
     __ incq(rax);
     __ Move(rcx, Smi::FromInt(x));
-    __ SmiShiftLeftConstant(rcx, rcx, shift);
+    __ SmiShiftLeftConstant(rcx, rcx, shift, exit);
 
     __ incq(rax);
     __ SmiCompare(rcx, r8);
@@ -1760,7 +1760,7 @@
     __ incq(rax);
     __ Move(rdx, Smi::FromInt(x));
     __ Move(rcx, Smi::FromInt(shift));
-    __ SmiShiftLeft(r9, rdx, rcx);
+    __ SmiShiftLeft(r9, rdx, rcx, exit);
 
     __ incq(rax);
     __ SmiCompare(r9, r8);
@@ -1769,7 +1769,7 @@
     __ incq(rax);
     __ Move(rdx, Smi::FromInt(x));
     __ Move(r11, Smi::FromInt(shift));
-    __ SmiShiftLeft(r9, rdx, r11);
+    __ SmiShiftLeft(r9, rdx, r11, exit);
 
     __ incq(rax);
     __ SmiCompare(r9, r8);
@@ -1778,7 +1778,7 @@
     __ incq(rax);
     __ Move(rdx, Smi::FromInt(x));
     __ Move(r11, Smi::FromInt(shift));
-    __ SmiShiftLeft(rdx, rdx, r11);
+    __ SmiShiftLeft(rdx, rdx, r11, exit);
 
     __ incq(rax);
     __ SmiCompare(rdx, r8);
Index: test/cctest/test-heap.cc
===================================================================
--- test/cctest/test-heap.cc	(revision 4925)
+++ test/cctest/test-heap.cc	(working copy)
@@ -843,8 +843,8 @@
       Heap::AllocateFixedArray(n_elements));
 
   int index = n_elements - 1;
-  CHECK_EQ(flags_ptr,
-           HeapObject::RawField(array, FixedArray::OffsetOfElementAt(index)));
+  CHECK_EQ(flags_ptr, reinterpret_cast<intptr_t*>(
+           HeapObject::RawField(array, FixedArray::OffsetOfElementAt(index))));
   array->set(index, Smi::FromInt(0));
   // This chould have turned next page into LargeObjectPage:
   // CHECK(Page::FromAddress(next_page)->IsLargeObjectPage());
Index: samples/hello.cc
===================================================================
--- samples/hello.cc	(revision 0)
+++ samples/hello.cc	(revision 0)
@@ -0,0 +1,34 @@
+#include <v8.h>
+
+using namespace v8;
+
+int main(int argc, char* argv[]) {
+
+  // Create a stack-allocated handle scope.
+  HandleScope handle_scope;
+
+  // Create a new context.
+  Persistent<Context> context = Context::New();
+
+  // Enter the created context for compiling and
+  // running the hello world script. 
+  Context::Scope context_scope(context);
+
+  // Create a string containing the JavaScript source code.
+  Handle<String> source = String::New("'Hello' + ', World!'");
+
+  // Compile the source code.
+  Handle<Script> script = Script::Compile(source);
+
+  // Run the script to get the result.
+  Handle<Value> result = script->Run();
+
+  // Dispose the persistent context.
+  context.Dispose();
+
+  // Convert the result to an ASCII string and print it.
+  String::AsciiValue ascii(result);
+  printf("%s\n", *ascii);
+  return 0;
+
+}
Index: run.sh
===================================================================
--- run.sh	(revision 0)
+++ run.sh	(revision 0)
@@ -0,0 +1,140 @@
+#!/bin/bash
+
+export NACLDYNCODE=1
+export NACL_DANGEROUS_ENABLE_FILE_ACCESS=1
+export NACL_ALLOW_DYNCODE_REPLACEMENT=1
+
+TOP=$(pwd)
+if [[ "$(uname -s)" == "Linux" ]]
+then
+  HOSTOS="linux"
+elif [[ "$(uname -s)" == "Darwin" ]]
+then
+  HOSTOS="mac"
+else
+  echo Unknown OS: $(uname -s)
+  exit 1
+fi
+
+export NACLSDK="$TOP/../nacl/native_client/toolchain/${HOSTOS}_x86/bin"
+CXX="$NACLSDK/nacl-g++"
+GDB="$NACLSDK/nacl-gdb"
+CXXFLAGS="-m64 -g"
+LDFLAGS="-Wl,--section-start,.rodata=0x20000000"
+
+TESTMODE="shell"
+MODE="debug"
+VERBOSE="off"
+ARCH="x64"
+OS="nacl"
+V8="v8_g"
+SCONSFLAGS="" # updated below
+SEL_LDR_DBG="$TOP/../nacl/native_client/scons-out/dbg-${HOSTOS}-x86-64/staging/sel_ldr"
+SEL_LDR_OPT="$TOP/../nacl/native_client/scons-out/opt-${HOSTOS}-x86-64/staging/sel_ldr"
+SEL_LDR=$SEL_LDR_DBG
+SEL_LDR_FLAGS="--"
+V8_SHELL="shell_g"
+
+while ! test -z "$1"
+do
+  case "$1" in
+
+    sunspider)
+      TESTMODE="sunspider"
+    ;;
+
+    benchmark)
+      TESTMODE="benchmark"
+    ;;
+
+    test)
+      TESTMODE="test"
+    ;;
+
+    release)
+      MODE="release"
+      V8="v8"
+      V8_SHELL="shell"
+      if test "$OS" == "nacl"
+      then
+        SEL_LDR=$SEL_LDR_OPT
+      fi
+    ;;
+
+    hello)
+      TESTMODE="hello"
+    ;;
+
+    native)
+      CXX=g++
+      OS=$HOSTOS
+      if [[ "$OS" == "mac" ]]
+      then
+        OS+="os"
+      fi
+      SEL_LDR=""
+      SEL_LDR_FLAGS=""
+      echo "Not supported"
+      exit 1
+    ;;
+
+    nacl)
+    ;;
+
+    *)
+      echo Unknown option: "$1" >&2
+      exit 1
+    ;;
+  esac
+  shift
+done
+
+SCONSFLAGS="mode=$MODE sample=shell verbose=$VERBOSE arch=$ARCH os=$OS debuggersupport=off -j2"
+TESTFLAGS="--report --time --mode=$MODE --arch=$ARCH -S verbose=$VERBOSE -S os=$OS -S debuggersupport=off -S -j2 es5conform message cctest mozilla mjsunit"
+
+
+function run() {
+  echo "$@" >&2
+  eval "( $@ )" || exit 1
+}
+
+case "$TESTMODE" in
+
+  shell)
+    run scons $SCONSFLAGS
+  ;;
+
+  hello)
+    run scons $SCONSFLAGS
+    run $CXX $CXXFLAGS $LDFLAGS -o hello samples/hello.cc -L. -I./include -l$V8 -lpthread
+    run $SEL_LDR $SEL_LDR_FLAGS ./hello
+  ;;
+
+  test)
+    if ! test -z "$SEL_LDR"
+    then
+      TESTFLAGS+=" --special-command=$SEL_LDR%20-a%20-c%20--@"
+    fi
+    run ./tools/test.py $TESTFLAGS
+  ;;
+
+  benchmark)
+    run scons $SCONSFLAGS
+    run "cd benchmarks && $SEL_LDR $SEL_LDR_FLAGS ../$V8_SHELL run.js"
+  ;;
+
+  sunspider)
+    run scons $SCONSFLAGS
+    cat > SunSpider/v8 << EOF
+#!/bin/sh
+export NACLDYNCODE="1"
+export NACL_DANGEROUS_ENABLE_FILE_ACCESS="1"
+export NACL_ALLOW_DYNCODE_REPLACEMENT="1"
+echo $SEL_LDR $SEL_LDR_FLAGS ../$V8_SHELL --expose-gc \$@ >&2
+exec $SEL_LDR $SEL_LDR_FLAGS ../$V8_SHELL --expose-gc \$@
+EOF
+    chmod +x SunSpider/v8
+    run "cd SunSpider && perl sunspider --runs 1 --shell ./v8"
+  ;;
+
+esac
Index: src/api.cc
===================================================================
--- src/api.cc	(revision 4925)
+++ src/api.cc	(working copy)
@@ -2606,6 +2606,8 @@
     return;
   }
   i::Handle<i::PixelArray> pixels = i::Factory::NewPixelArray(length, data);
+  self->set_map(
+      *i::Factory::GetSlowElementsMap(i::Handle<i::Map>(self->map())));
   self->set_elements(*pixels);
 }
 
@@ -2659,6 +2661,8 @@
   }
   i::Handle<i::ExternalArray> array =
       i::Factory::NewExternalArray(length, array_type, data);
+  self->set_map(
+      *i::Factory::GetSlowElementsMap(i::Handle<i::Map>(self->map())));
   self->set_elements(*array);
 }
 
@@ -4261,6 +4265,9 @@
 
 
 Local<Value> Debug::GetMirror(v8::Handle<v8::Value> obj) {
+#ifdef NACL
+  return Local<Value>();
+#endif
   if (!i::V8::IsRunning()) return Local<Value>();
   ON_BAILOUT("v8::Debug::GetMirror()", return Local<Value>());
   ENTER_V8;
Index: src/naclcode.h
===================================================================
--- src/naclcode.h	(revision 0)
+++ src/naclcode.h	(revision 0)
@@ -0,0 +1,79 @@
+// Copyright 2006-2010 the V8 project authors. All rights reserved.
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+//     * Redistributions of source code must retain the above copyright
+//       notice, this list of conditions and the following disclaimer.
+//     * Redistributions in binary form must reproduce the above
+//       copyright notice, this list of conditions and the following
+//       disclaimer in the documentation and/or other materials provided
+//       with the distribution.
+//     * Neither the name of Google Inc. nor the names of its
+//       contributors may be used to endorse or promote products derived
+//       from this software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+#ifndef V8_NACLCODE_H_
+#define V8_NACLCODE_H_
+
+#include <stddef.h>
+
+namespace v8 {
+namespace internal {
+
+class Code;
+
+class NaClCode {
+public:
+  typedef Code* Tag;
+
+  // Allocate space to place the code
+  static uint8_t* Allocate(size_t bytes, Tag backpointer = 0);
+  
+  // Deallocate previously allocated code
+  static void Deallocate(uint8_t* ptr, size_t bytes);
+
+  // Install Code in a previously allocated space
+  static void Install(uint8_t* dst, uint8_t* src, size_t bytes);
+  
+  // Retrieve the Code* given at allocation time
+  static Tag GetBackpointer(uint8_t* inst);
+  
+  // Retrieve the Code* given at allocation time
+  // With an arbitrary pointer inside the region
+  static Tag Search(uint8_t* inst);
+  
+  // True if the target code is nacl text
+  static bool IsProtectedCode(uint8_t* dst);
+
+  // Patch a subset of a single bundle, target region cant cross bundle
+  static void Modify(uint8_t* dst, uint8_t* src, size_t bytes);
+
+  // Change the value of a single target word
+  template<typename T>
+  static inline void PatchWord(T* dst, T val){
+    Modify(reinterpret_cast<uint8_t*>(dst),
+           reinterpret_cast<uint8_t*>(&val),
+           sizeof(T));
+  }
+
+private:
+  NaClCode();
+};
+
+}} //v8::internal
+
+#endif// V8_NACLCODE_H_
+
Index: src/heap.cc
===================================================================
--- src/heap.cc	(revision 4925)
+++ src/heap.cc	(working copy)
@@ -45,6 +45,7 @@
 #include "regexp-macro-assembler.h"
 #include "arm/regexp-macro-assembler-arm.h"
 #endif
+#include "naclcode.h"
 
 
 namespace v8 {
@@ -126,6 +127,12 @@
 int Heap::linear_allocation_scope_depth_ = 0;
 int Heap::contexts_disposed_ = 0;
 
+int Heap::young_survivors_after_last_gc_ = 0;
+int Heap::high_survival_rate_period_length_ = 0;
+double Heap::survival_rate_ = 0;
+Heap::SurvivalRateTrend Heap::previous_survival_rate_trend_ = Heap::STABLE;
+Heap::SurvivalRateTrend Heap::survival_rate_trend_ = Heap::STABLE;
+
 #ifdef DEBUG
 bool Heap::allocation_allowed_ = true;
 
@@ -582,7 +589,30 @@
 }
 #endif
 
+void Heap::UpdateSurvivalRateTrend(int start_new_space_size) {
+  double survival_rate =
+      (static_cast<double>(young_survivors_after_last_gc_) * 100) /
+      start_new_space_size;
 
+  if (survival_rate > kYoungSurvivalRateThreshold) {
+    high_survival_rate_period_length_++;
+  } else {
+    high_survival_rate_period_length_ = 0;
+  }
+
+  double survival_rate_diff = survival_rate_ - survival_rate;
+
+  if (survival_rate_diff > kYoungSurvivalRateAllowedDeviation) {
+    set_survival_rate_trend(DECREASING);
+  } else if (survival_rate_diff < -kYoungSurvivalRateAllowedDeviation) {
+    set_survival_rate_trend(INCREASING);
+  } else {
+    set_survival_rate_trend(STABLE);
+  }
+
+  survival_rate_ = survival_rate;
+}
+
 void Heap::PerformGarbageCollection(AllocationSpace space,
                                     GarbageCollector collector,
                                     GCTracer* tracer) {
@@ -604,6 +634,8 @@
 
   EnsureFromSpaceIsCommitted();
 
+  int start_new_space_size = Heap::new_space()->Size();
+
   if (collector == MARK_COMPACTOR) {
     if (FLAG_flush_code) {
       // Flush all potentially unused code.
@@ -613,16 +645,36 @@
     // Perform mark-sweep with optional compaction.
     MarkCompact(tracer);
 
+    bool high_survival_rate_during_scavenges = IsHighSurvivalRate() &&
+        IsStableOrIncreasingSurvivalTrend();
+
+    UpdateSurvivalRateTrend(start_new_space_size);
+
     int old_gen_size = PromotedSpaceSize();
     old_gen_promotion_limit_ =
         old_gen_size + Max(kMinimumPromotionLimit, old_gen_size / 3);
     old_gen_allocation_limit_ =
         old_gen_size + Max(kMinimumAllocationLimit, old_gen_size / 2);
+
+    if (high_survival_rate_during_scavenges &&
+        IsStableOrIncreasingSurvivalTrend()) {
+      // Stable high survival rates of young objects both during partial and
+      // full collection indicate that mutator is either building or modifying
+      // a structure with a long lifetime.
+      // In this case we aggressively raise old generation memory limits to
+      // postpone subsequent mark-sweep collection and thus trade memory
+      // space for the mutation speed.
+      old_gen_promotion_limit_ *= 2;
+      old_gen_allocation_limit_ *= 2;
+    }
+
     old_gen_exhausted_ = false;
   } else {
     tracer_ = tracer;
     Scavenge();
     tracer_ = NULL;
+
+    UpdateSurvivalRateTrend(start_new_space_size);
   }
 
   Counters::objs_since_last_young.Set(0);
@@ -718,6 +770,13 @@
 
 
 Object* Heap::FindCodeObject(Address a) {
+#ifdef NACL        
+  //NACL_CHANGE: try the NaClCode heap
+  Code* c = NaClCode::Search(a);
+  if(c != NULL){
+    return c;
+  }
+#endif  
   Object* obj = code_space_->FindObject(a);
   if (obj->IsFailure()) {
     obj = lo_space_->FindObject(a);
@@ -1217,7 +1276,7 @@
   map->set_code_cache(empty_fixed_array());
   map->set_unused_property_fields(0);
   map->set_bit_field(0);
-  map->set_bit_field2(1 << Map::kIsExtensible);
+  map->set_bit_field2((1 << Map::kIsExtensible) | (1 << Map::kHasFastElements));
 
   // If the map object is aligned fill the padding area with Smi 0 objects.
   if (Map::kPadStart < Map::kSize) {
@@ -2301,7 +2360,12 @@
                          Code::Flags flags,
                          Handle<Object> self_reference) {
   // Compute size
+#ifdef NACL 
+  int inst_size = RoundUp(desc.instr_size, Code::kCodeAlignment);
+  int body_size = RoundUp(desc.reloc_size, kObjectAlignment);
+#else
   int body_size = RoundUp(desc.instr_size + desc.reloc_size, kObjectAlignment);
+#endif  
   int sinfo_size = 0;
   if (sinfo != NULL) sinfo_size = sinfo->Serialize(NULL);
   int obj_size = Code::SizeFor(body_size, sinfo_size);
@@ -2319,7 +2383,11 @@
   HeapObject::cast(result)->set_map(code_map());
   Code* code = Code::cast(result);
   ASSERT(!CodeRange::exists() || CodeRange::contains(code->address()));
+#ifdef NACL  
+  code->set_instruction_size(inst_size);
+#else
   code->set_instruction_size(desc.instr_size);
+#endif  
   code->set_relocation_size(desc.reloc_size);
   code->set_sinfo_size(sinfo_size);
   code->set_flags(flags);
@@ -2328,6 +2396,12 @@
   if (!self_reference.is_null()) {
     *(self_reference.location()) = code;
   }
+
+#ifdef NACL
+  //NACL_CHANGE: split instructions from main code object
+  code->set_external_instructions(NaClCode::Allocate(inst_size, code));
+#endif
+
   // Migrate generated code.
   // The generated code can contain Object** values (typically from handles)
   // that are dereferenced during the copy to point directly to the actual heap
@@ -2368,8 +2442,12 @@
 
 
 Object* Heap::CopyCode(Code* code, Vector<byte> reloc_info) {
-  int new_body_size = RoundUp(code->instruction_size() + reloc_info.length(),
-                              kObjectAlignment);
+#ifdef NACL        
+  //NACL_CHANGE: dont include instruction_size
+  int new_body_size = RoundUp(reloc_info.length(), kObjectAlignment);
+#else
+  int new_body_size = RoundUp(code->instruction_size() + reloc_info.length(), kObjectAlignment);
+#endif
 
   int sinfo_size = code->sinfo_size();
 
@@ -2545,6 +2623,7 @@
   map->set_inobject_properties(in_object_properties);
   map->set_unused_property_fields(in_object_properties);
   map->set_prototype(prototype);
+  ASSERT(map->has_fast_elements());
 
   // If the function has only simple this property assignments add
   // field descriptors for these to the initial map as the object
@@ -2598,8 +2677,8 @@
   // properly initialized.
   ASSERT(map->instance_type() != JS_FUNCTION_TYPE);
 
-  // Both types of globla objects should be allocated using
-  // AllocateGloblaObject to be properly initialized.
+  // Both types of global objects should be allocated using
+  // AllocateGlobalObject to be properly initialized.
   ASSERT(map->instance_type() != JS_GLOBAL_OBJECT_TYPE);
   ASSERT(map->instance_type() != JS_BUILTINS_OBJECT_TYPE);
 
@@ -2623,6 +2702,7 @@
   InitializeJSObjectFromMap(JSObject::cast(obj),
                             FixedArray::cast(properties),
                             map);
+  ASSERT(JSObject::cast(obj)->HasFastElements());
   return obj;
 }
 
Index: src/objects.cc
===================================================================
--- src/objects.cc	(revision 4925)
+++ src/objects.cc	(working copy)
@@ -2222,7 +2222,12 @@
 Object* JSObject::NormalizeElements() {
   ASSERT(!HasPixelElements() && !HasExternalArrayElements());
   if (HasDictionaryElements()) return this;
+  ASSERT(map()->has_fast_elements());
 
+  Object* obj = map()->GetSlowElementsMap();
+  if (obj->IsFailure()) return obj;
+  Map* new_map = Map::cast(obj);
+
   // Get number of entries.
   FixedArray* array = FixedArray::cast(elements());
 
@@ -2230,7 +2235,7 @@
   int length = IsJSArray() ?
                Smi::cast(JSArray::cast(this)->length())->value() :
                array->length();
-  Object* obj = NumberDictionary::Allocate(length);
+  obj = NumberDictionary::Allocate(length);
   if (obj->IsFailure()) return obj;
   NumberDictionary* dictionary = NumberDictionary::cast(obj);
   // Copy entries.
@@ -2243,7 +2248,10 @@
       dictionary = NumberDictionary::cast(result);
     }
   }
-  // Switch to using the dictionary as the backing storage for elements.
+  // Switch to using the dictionary as the backing storage for
+  // elements. Set the new map first to satify the elements type
+  // assert in set_elements().
+  set_map(new_map);
   set_elements(dictionary);
 
   Counters::elements_to_dictionary.Increment();
@@ -5293,13 +5301,66 @@
 
 
 void Code::Relocate(intptr_t delta) {
+#ifndef NACL        
+  //NACL_CHANGE: with split code/data, no longer need to do anything on relocate
   for (RelocIterator it(this, RelocInfo::kApplyMask); !it.done(); it.next()) {
     it.rinfo()->apply(delta);
+ }
+ CPU::FlushICache(instruction_start(), instruction_size());
+#endif 
+}
+
+#ifdef NACL
+void Code::CopyFrom(const CodeDesc& desc) {
+  //NACL_CHANGE:
+  // rewrote this function to be "3rd person"
+  Address final_inst = instruction_start();
+  Address tmp_inst = desc.buffer;
+
+  // copy reloc info
+  memcpy(relocation_start(),
+         desc.buffer + desc.buffer_size - desc.reloc_size,
+         desc.reloc_size);
+
+  //temporarily swing code pointer to writable buffer
+  set_external_instructions(tmp_inst);
+  
+  //pad up to 32 byte boundary with nops 
+  memset(tmp_inst+desc.instr_size, 0x90, instruction_size()-desc.instr_size);
+
+  // unbox handles and relocate
+  intptr_t delta = final_inst - desc.buffer;//based on final inst
+  int mode_mask = RelocInfo::kCodeTargetMask |
+                  RelocInfo::ModeMask(RelocInfo::EMBEDDED_OBJECT) |
+                  RelocInfo::kApplyMask;
+  Assembler* origin = desc.origin;  // Needed to find target_object on X64.
+  for (RelocIterator it(this, mode_mask); !it.done(); it.next()) {
+    RelocInfo::Mode mode = it.rinfo()->rmode();
+    if (mode == RelocInfo::EMBEDDED_OBJECT) {
+      Handle<Object> p = it.rinfo()->target_object_handle(origin);
+      it.rinfo()->set_target_object(*p);
+    } else if (RelocInfo::IsCodeTarget(mode)) {
+      // rewrite code handles in inline cache targets to direct
+      // pointers to the first instruction in the code object
+      Handle<Object> p = it.rinfo()->target_object_handle(origin);
+      Code* code = Code::cast(*p);
+      //extra offset since jump targets are relative
+      it.rinfo()->set_target_address(code->instruction_start(), 
+                                     final_inst-tmp_inst);
+    } else {
+      it.rinfo()->apply(delta);
+    }
   }
+
+  //install instructions 
+  NaClCode::Install(final_inst, tmp_inst, instruction_size());
+
+  //swing code pointer to executable buffer
+  set_external_instructions(final_inst);
+  
   CPU::FlushICache(instruction_start(), instruction_size());
 }
-
-
+#else
 void Code::CopyFrom(const CodeDesc& desc) {
   // copy code
   memmove(instruction_start(), desc.buffer, desc.instr_size);
@@ -5340,7 +5401,30 @@
   }
   CPU::FlushICache(instruction_start(), instruction_size());
 }
+#endif // NACL
+  
+#ifdef DEBUG
+void Code::CodeDumpAsm(const char* filename){
+  byte* p = instruction_start();
+  byte* q = p+instruction_size();
+  FILE* fp = fopen(filename, "w");
+  CHECK(fp!=0);
+  fprintf(fp, ".global _start\n_start:\n");
+  if(p < q) {
+    fprintf(fp, ".byte %d", *p++);
+    while (p < q) {
+      fprintf(fp, ", %d", *p++);
+    }
+  }
+  fclose(fp);
+}
+#endif
 
+#ifdef NACL
+void Code::NaClOnDelete() {
+  NaClCode::Deallocate(instruction_start(), instruction_size());
+}
+#endif
 
 // Locate the source position which is closest to the address in the code. This
 // is using the source position information embedded in the relocation info.
@@ -5473,14 +5557,18 @@
 #endif  // ENABLE_DISASSEMBLER
 
 
-void JSObject::SetFastElements(FixedArray* elems) {
+Object* JSObject::SetFastElementsCapacityAndLength(int capacity, int length) {
   // We should never end in here with a pixel or external array.
   ASSERT(!HasPixelElements() && !HasExternalArrayElements());
-#ifdef DEBUG
-  // Check the provided array is filled with the_hole.
-  uint32_t len = static_cast<uint32_t>(elems->length());
-  for (uint32_t i = 0; i < len; i++) ASSERT(elems->get(i)->IsTheHole());
-#endif
+
+  Object* obj = Heap::AllocateFixedArrayWithHoles(capacity);
+  if (obj->IsFailure()) return obj;
+  FixedArray* elems = FixedArray::cast(obj);
+
+  obj = map()->GetFastElementsMap();
+  if (obj->IsFailure()) return obj;
+  Map* new_map = Map::cast(obj);
+
   AssertNoAllocation no_gc;
   WriteBarrierMode mode = elems->GetWriteBarrierMode(no_gc);
   switch (GetElementsKind()) {
@@ -5508,7 +5596,15 @@
       UNREACHABLE();
       break;
   }
+
+  set_map(new_map);
   set_elements(elems);
+
+  if (IsJSArray()) {
+    JSArray::cast(this)->set_length(Smi::FromInt(length));
+  }
+
+  return this;
 }
 
 
@@ -5595,7 +5691,7 @@
 
   Object* smi_length = len->ToSmi();
   if (smi_length->IsSmi()) {
-    int value = Smi::cast(smi_length)->value();
+    const int value = Smi::cast(smi_length)->value();
     if (value < 0) return ArrayLengthRangeError();
     switch (GetElementsKind()) {
       case FAST_ELEMENTS: {
@@ -5617,12 +5713,8 @@
         int new_capacity = value > min ? value : min;
         if (new_capacity <= kMaxFastElementsLength ||
             !ShouldConvertToSlowElements(new_capacity)) {
-          Object* obj = Heap::AllocateFixedArrayWithHoles(new_capacity);
+          Object* obj = SetFastElementsCapacityAndLength(new_capacity, value);
           if (obj->IsFailure()) return obj;
-          if (IsJSArray()) {
-            JSArray::cast(this)->set_length(Smi::cast(smi_length));
-          }
-          SetFastElements(FixedArray::cast(obj));
           return this;
         }
         break;
@@ -5633,7 +5725,8 @@
             // If the length of a slow array is reset to zero, we clear
             // the array and flush backing storage. This has the added
             // benefit that the array returns to fast mode.
-            initialize_elements();
+            Object* obj = ResetElements();
+            if (obj->IsFailure()) return obj;
           } else {
             // Remove deleted elements.
             uint32_t old_length =
@@ -6092,12 +6185,8 @@
     if (new_capacity <= kMaxFastElementsLength ||
         !ShouldConvertToSlowElements(new_capacity)) {
       ASSERT(static_cast<uint32_t>(new_capacity) > index);
-      Object* obj = Heap::AllocateFixedArrayWithHoles(new_capacity);
+      Object* obj = SetFastElementsCapacityAndLength(new_capacity, index + 1);
       if (obj->IsFailure()) return obj;
-      SetFastElements(FixedArray::cast(obj));
-      if (IsJSArray()) {
-        JSArray::cast(this)->set_length(Smi::FromInt(index + 1));
-      }
       FixedArray::cast(elements())->set(index, value);
       return value;
     }
@@ -6216,13 +6305,11 @@
         uint32_t new_length = 0;
         if (IsJSArray()) {
           CHECK(JSArray::cast(this)->length()->ToArrayIndex(&new_length));
-          JSArray::cast(this)->set_length(Smi::FromInt(new_length));
         } else {
           new_length = NumberDictionary::cast(elements())->max_number_key() + 1;
         }
-        Object* obj = Heap::AllocateFixedArrayWithHoles(new_length);
+        Object* obj = SetFastElementsCapacityAndLength(new_length, new_length);
         if (obj->IsFailure()) return obj;
-        SetFastElements(FixedArray::cast(obj));
 #ifdef DEBUG
         if (FLAG_trace_normalization) {
           PrintF("Object elements are fast case again:\n");
@@ -7526,14 +7613,18 @@
     }
     // Convert to fast elements.
 
+    Object* obj = map()->GetFastElementsMap();
+    if (obj->IsFailure()) return obj;
+    Map* new_map = Map::cast(obj);
+
     PretenureFlag tenure = Heap::InNewSpace(this) ? NOT_TENURED: TENURED;
     Object* new_array =
         Heap::AllocateFixedArray(dict->NumberOfElements(), tenure);
-    if (new_array->IsFailure()) {
-      return new_array;
-    }
+    if (new_array->IsFailure()) return new_array;
     FixedArray* fast_elements = FixedArray::cast(new_array);
     dict->CopyValuesTo(fast_elements);
+
+    set_map(new_map);
     set_elements(fast_elements);
   }
   ASSERT(HasFastElements());
Index: src/regexp-macro-assembler.h
===================================================================
--- src/regexp-macro-assembler.h	(revision 4925)
+++ src/regexp-macro-assembler.h	(working copy)
@@ -68,6 +68,7 @@
   // stack by an earlier PushBacktrack(Label*).
   virtual void Backtrack() = 0;
   virtual void Bind(Label* label) = 0;
+  virtual void Bind_Aligned(Label* label) { Bind(label); }
   virtual void CheckAtStart(Label* on_at_start) = 0;
   // Dispatch after looking the current character up in a 2-bits-per-entry
   // map.  The destinations vector has up to 4 labels.
Index: src/conversions.cc
===================================================================
--- src/conversions.cc	(revision 4925)
+++ src/conversions.cc	(working copy)
@@ -25,6 +25,8 @@
 // (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 // OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
+#include <stdlib.h>
+#include <stdio.h>
 #include <stdarg.h>
 #include <limits.h>
 
@@ -103,7 +105,15 @@
 }
 
 
+#ifdef NACL
+// this special case can be remove when the following bug is fixed:
+// http://code.google.com/p/nativeclient/issues/detail?id=702
+inline double gay_strtod(const char* s00, char** se){
+  return strtod(s00,se);
+}
+#else
 extern "C" double gay_strtod(const char* s00, const char** se);
+#endif
 
 // Maximum number of significant digits in decimal representation.
 // The longest possible double in decimal representation is
@@ -737,12 +747,59 @@
   return InternalStringToDouble(str, end, flags, empty_string_val);
 }
 
-
+#ifndef NACL
 extern "C" char* dtoa(double d, int mode, int ndigits,
                       int* decpt, int* sign, char** rve);
 
 extern "C" void freedtoa(char* s);
+#endif
 
+#ifdef NACL
+//emulate dtoa with snprintf
+void nacl_dtoa(char* decimal_rep, int decimal_rep_len, double value, int p, int* decimal_point, int* sign) {
+  memset(decimal_rep, 0, decimal_rep_len);
+  *decimal_point = 0;
+  *sign = 0;
+
+  //generate fmt dynamically:
+  //char fmt[8];  //max size "%21e\0" = 5
+  //sprintf(fmt, "%%.%de", p-1); //build format like: "%.3e"
+  //or:
+
+  //use a lookup table:
+  static const char* fmts[] = {
+    "%.0e",  "%.1e",  "%.2e",  "%.3e",  "%.4e",  "%.5e",  "%.6e",
+    "%.7e",  "%.8e",  "%.9e",  "%.10e", "%.11e", "%.12e", "%.13e",
+    "%.14e", "%.15e", "%.16e", "%.17e", "%.18e", "%.19e", "%.20e"
+  };
+  CHECK(p >= 1 && p <= 21);
+  const char* fmt = fmts[p-1];
+
+  char tmp[32]; //max size "-1." + 20 digits + "e+NNNN\0" =  30
+  sprintf(tmp, fmt, value);
+  //scan through tmp, copying mantissa digits to decimal_rep
+  for(char *i=tmp,*o=decimal_rep; i<tmp+sizeof tmp; ++i) {
+    if('0'<=*i && *i<='9') {
+      //copy digit
+      *o++ = *i;
+    } else if(*i=='.') {
+      //ignore
+    } else if(*i=='-') {
+      *sign = 1;
+    } else if(*i=='e') {
+      //store exponent+1 to decimal_point and exit loop
+      CHECK(sscanf(i+1, "%d", decimal_point)==1);
+      *decimal_point+=1;
+      break;
+    } else {
+      UNIMPLEMENTED();
+    }
+  }
+}
+#endif
+
+
+
 const char* DoubleToCString(double v, Vector<char> buffer) {
   StringBuilder builder(buffer.start(), buffer.length());
 
@@ -767,6 +824,9 @@
       int decimal_point;
       int sign;
       char* decimal_rep;
+#ifdef NACL
+      char decimal_rep_buf[32];
+#endif
       bool used_gay_dtoa = false;
       const int kV8DtoaBufferCapacity = kBase10MaximalLength + 1;
       char v8_dtoa_buffer[kV8DtoaBufferCapacity];
@@ -777,7 +837,12 @@
                         &sign, &length, &decimal_point)) {
         decimal_rep = v8_dtoa_buffer;
       } else {
+#ifdef NACL
+        decimal_rep = decimal_rep_buf;
+        nacl_dtoa(decimal_rep_buf, sizeof decimal_rep_buf, v, 21, &decimal_point, &sign);
+#else
         decimal_rep = dtoa(v, 0, 0, &decimal_point, &sign, NULL);
+#endif
         used_gay_dtoa = true;
         length = StrLength(decimal_rep);
       }
@@ -815,7 +880,9 @@
         builder.AddFormatted("%d", exponent);
       }
 
+#ifndef NACL
       if (used_gay_dtoa) freedtoa(decimal_rep);
+#endif
     }
   }
   return builder.Finalize();
@@ -946,8 +1013,6 @@
   return builder.Finalize();
 }
 
-
-
 char* DoubleToExponentialCString(double value, int f) {
   // f might be -1 to signal that f was undefined in JavaScript.
   ASSERT(f >= -1 && f <= 20);
@@ -961,6 +1026,15 @@
   // Find a sufficiently precise decimal representation of n.
   int decimal_point;
   int sign;
+#ifdef NACL
+  char decimal_rep[32];
+  if (f == -1) {
+    nacl_dtoa(decimal_rep, sizeof decimal_rep, value, 21, &decimal_point, &sign);
+    f = StrLength(decimal_rep) - 1;
+  } else {
+    nacl_dtoa(decimal_rep, sizeof decimal_rep, value, f+1, &decimal_point, &sign);
+  }
+#else
   char* decimal_rep = NULL;
   if (f == -1) {
     decimal_rep = dtoa(value, 0, 0, &decimal_point, &sign, NULL);
@@ -968,6 +1042,7 @@
   } else {
     decimal_rep = dtoa(value, 2, f + 1, &decimal_point, &sign, NULL);
   }
+#endif
   int decimal_rep_length = StrLength(decimal_rep);
   ASSERT(decimal_rep_length > 0);
   ASSERT(decimal_rep_length <= f + 1);
@@ -977,12 +1052,13 @@
   char* result =
       CreateExponentialRepresentation(decimal_rep, exponent, negative, f+1);
 
+#ifndef NACL
   freedtoa(decimal_rep);
+#endif
 
   return result;
 }
 
-
 char* DoubleToPrecisionCString(double value, int p) {
   ASSERT(p >= 1 && p <= 21);
 
@@ -993,9 +1069,14 @@
   }
 
   // Find a sufficiently precise decimal representation of n.
-  int decimal_point;
-  int sign;
+  int decimal_point = 0;
+  int sign = 0;
+#ifdef NACL
+  char decimal_rep[32]; //max size is p+1
+  nacl_dtoa(decimal_rep, sizeof decimal_rep, value, p, &decimal_point, &sign);
+#else
   char* decimal_rep = dtoa(value, 2, p, &decimal_point, &sign, NULL);
+#endif
   int decimal_rep_length = StrLength(decimal_rep);
   ASSERT(decimal_rep_length <= p);
 
@@ -1040,7 +1121,9 @@
     result = builder.Finalize();
   }
 
+#ifndef NACL
   freedtoa(decimal_rep);
+#endif
   return result;
 }
 
Index: src/spaces.h
===================================================================
--- src/spaces.h	(revision 4925)
+++ src/spaces.h	(working copy)
@@ -31,6 +31,10 @@
 #include "list-inl.h"
 #include "log.h"
 
+#ifdef NACL
+#define NACL_MAP_PAGE  1U<<16
+#endif
+
 namespace v8 {
 namespace internal {
 
Index: src/runtime.cc
===================================================================
--- src/runtime.cc	(revision 4925)
+++ src/runtime.cc	(working copy)
@@ -1522,6 +1522,7 @@
   CONVERT_NUMBER_CHECKED(int, offset, Int32, args[1]);
 
   Code* code = fun->code();
+  
   RUNTIME_ASSERT(0 <= offset && offset < code->Size());
 
   Address pc = code->address() + offset;
@@ -6544,7 +6545,12 @@
 
     WriteBarrierMode mode = array->GetWriteBarrierMode(no_gc);
     for (int i = 0; i < length; i++) {
-      array->set(i, *--parameters, mode);
+#ifdef NACL
+      // 8 byte data on stack
+      parameters--;
+#endif
+      parameters--;
+      array->set(i, *parameters, mode);
     }
     JSObject::cast(result)->set_elements(FixedArray::cast(obj));
   }
@@ -6787,8 +6793,13 @@
 // allocated by the caller, and passed as a pointer in a hidden first parameter.
 #ifdef V8_HOST_ARCH_64_BIT
 struct ObjectPair {
+#ifdef NACL
+  Object* x __attribute__ ((aligned (8)));
+  Object* y __attribute__ ((aligned (8)));
+#else
   Object* x;
   Object* y;
+#endif
 };
 
 static inline ObjectPair MakePair(Object* x, Object* y) {
@@ -7104,6 +7115,7 @@
   ASSERT(args.length() == 0);
   NoHandleAllocation ha;
   PrintTransition(NULL);
+  Top::PrintStack();
   return Heap::undefined_value();
 }
 
@@ -7449,7 +7461,7 @@
   uint32_t index_limit_;
   // Index after last seen index. Always less than or equal to index_limit_.
   uint32_t index_offset_;
-  bool fast_elements_;
+  const bool fast_elements_;
 };
 
 
@@ -7766,13 +7778,14 @@
     // The backing storage array must have non-existing elements to
     // preserve holes across concat operations.
     storage = Factory::NewFixedArrayWithHoles(result_length);
-
+    result->set_map(*Factory::GetFastElementsMap(Handle<Map>(result->map())));
   } else {
     // TODO(126): move 25% pre-allocation logic into Dictionary::Allocate
     uint32_t at_least_space_for = estimate_nof_elements +
                                   (estimate_nof_elements >> 2);
     storage = Handle<FixedArray>::cast(
                   Factory::NewNumberDictionary(at_least_space_for));
+    result->set_map(*Factory::GetSlowElementsMap(Handle<Map>(result->map())));
   }
 
   Handle<Object> len = Factory::NewNumber(static_cast<double>(result_length));
@@ -7822,9 +7835,19 @@
   ASSERT(args.length() == 2);
   CONVERT_CHECKED(JSArray, from, args[0]);
   CONVERT_CHECKED(JSArray, to, args[1]);
-  to->SetContent(FixedArray::cast(from->elements()));
+  HeapObject* new_elements = from->elements();
+  Object* new_map;
+  if (new_elements->map() == Heap::fixed_array_map()) {
+    new_map = to->map()->GetFastElementsMap();
+  } else {
+    new_map = to->map()->GetSlowElementsMap();
+  }
+  if (new_map->IsFailure()) return new_map;
+  to->set_map(Map::cast(new_map));
+  to->set_elements(new_elements);
   to->set_length(from->length());
-  from->SetContent(Heap::empty_fixed_array());
+  Object* obj = from->ResetElements();
+  if (obj->IsFailure()) return obj;
   from->set_length(Smi::FromInt(0));
   return to;
 }
@@ -9045,7 +9068,7 @@
   ASSERT(args.length() == 3);
   CONVERT_ARG_CHECKED(JSFunction, fun, 0);
   Handle<SharedFunctionInfo> shared(fun->shared());
-  CONVERT_NUMBER_CHECKED(int32_t, source_position, Int32, args[1]);
+  CONVERT_NUMBER_CHECKED(int, source_position, Int32, args[1]);
   RUNTIME_ASSERT(source_position >= 0);
   Handle<Object> break_point_object_arg = args.at<Object>(2);
 
@@ -10119,8 +10142,8 @@
 
   HandleScope scope;
 
-  limit = Max(limit, 0);  // Ensure that limit is not negative.
-  int initial_size = Min(limit, 10);
+  limit = Max<int32_t>(limit, 0);  // Ensure that limit is not negative.
+  int initial_size = Min<int32_t>(limit, 10);
   Handle<JSArray> result = Factory::NewJSArray(initial_size * 3);
 
   StackFrameIterator iter;
@@ -10190,6 +10213,7 @@
   return Heap::undefined_value();
 }
 
+static int cache_miss = 0;
 
 static Object* CacheMiss(FixedArray* cache_obj, int index, Object* key_obj) {
   ASSERT(index % 2 == 0);  // index of the key
@@ -10198,6 +10222,10 @@
 
   HandleScope scope;
 
+  //PMARCH debug
+  cache_miss++;
+  printf("CacheMiss %d\n", cache_miss);
+
   Handle<FixedArray> cache(cache_obj);
   Handle<Object> key(key_obj);
   Handle<JSFunction> factory(JSFunction::cast(
Index: src/jump-target-light-inl.h
===================================================================
--- src/jump-target-light-inl.h	(revision 4925)
+++ src/jump-target-light-inl.h	(working copy)
@@ -36,16 +36,20 @@
 // Construct a jump target.
 JumpTarget::JumpTarget(Directionality direction)
     : entry_frame_set_(false),
+      direction_(direction),
       entry_frame_(kInvalidVirtualFrameInitializer) {
 }
 
 JumpTarget::JumpTarget()
     : entry_frame_set_(false),
+      direction_(FORWARD_ONLY),
       entry_frame_(kInvalidVirtualFrameInitializer) {
 }
 
 
 BreakTarget::BreakTarget() { }
+BreakTarget::BreakTarget(JumpTarget::Directionality direction)
+  : JumpTarget(direction) { }
 
 } }  // namespace v8::internal
 
Index: src/objects.h
===================================================================
--- src/objects.h	(revision 4925)
+++ src/objects.h	(working copy)
@@ -1191,6 +1191,7 @@
   // case, and a PixelArray or ExternalArray in special cases.
   DECL_ACCESSORS(elements, HeapObject)
   inline void initialize_elements();
+  inline Object* ResetElements();
   inline ElementsKind GetElementsKind();
   inline bool HasFastElements();
   inline bool HasDictionaryElements();
@@ -1367,7 +1368,7 @@
   // The undefined object if index is out of bounds.
   Object* GetElementWithReceiver(JSObject* receiver, uint32_t index);
 
-  void SetFastElements(FixedArray* elements);
+  Object* SetFastElementsCapacityAndLength(int capacity, int length);
   Object* SetSlowElements(Object* length);
 
   // Lookup interceptors are used for handling properties controlled by host
@@ -2727,6 +2728,13 @@
   inline int instruction_size();
   inline void set_instruction_size(int value);
 
+#ifdef NACL
+  //NACL_CHANGE: add external_instructions field:
+  // [external_instructions]: Pointer to the native instructions
+  inline byte* external_instructions();
+  inline void set_external_instructions(byte* value);
+#endif
+
   // [relocation_size]: Size of relocation information.
   inline int relocation_size();
   inline void set_relocation_size(int value);
@@ -2784,7 +2792,7 @@
 
   // Returns the address of the first instruction.
   inline byte* instruction_start();
-
+  
   // Returns the size of the instructions, padding, and relocation information.
   inline int body_size();
 
@@ -2812,15 +2820,22 @@
   static int SizeFor(int body_size, int sinfo_size) {
     ASSERT_SIZE_TAG_ALIGNED(body_size);
     ASSERT_SIZE_TAG_ALIGNED(sinfo_size);
+
+#ifdef NACL    
+    //NACL_CHANGE: don't make room for instructions
     return RoundUp(kHeaderSize + body_size + sinfo_size, kCodeAlignment);
+#else    
+    return RoundUp(kHeaderSize + sinfo_size, kCodeAlignment);
+#endif
   }
 
   // Calculate the size of the code object to report for log events. This takes
   // the layout of the code object into account.
   int ExecutableSize() {
     // Check that the assumptions about the layout of the code object holds.
+
     ASSERT_EQ(static_cast<int>(instruction_start() - address()),
-              Code::kHeaderSize);
+           Code::kHeaderSize);
     return instruction_size() + Code::kHeaderSize;
   }
 
@@ -2837,7 +2852,13 @@
 #ifdef DEBUG
   void CodePrint();
   void CodeVerify();
+  void CodeDumpAsm(const char* filename);
 #endif
+
+#ifdef NACL
+  void NaClOnDelete();
+#endif
+
   // Code entry points are aligned to 32 bytes.
   static const int kCodeAlignmentBits = 5;
   static const int kCodeAlignment = 1 << kCodeAlignmentBits;
@@ -2845,7 +2866,13 @@
 
   // Layout description.
   static const int kInstructionSizeOffset = HeapObject::kHeaderSize;
+#ifdef NACL  
+  //NACL_CHANGE: add external_instructions field:
+  static const int kExternalInstructionsOffset = kInstructionSizeOffset + kPointerSize;
+  static const int kRelocationSizeOffset = kExternalInstructionsOffset + kIntSize;
+#else
   static const int kRelocationSizeOffset = kInstructionSizeOffset + kIntSize;
+#endif  
   static const int kSInfoSizeOffset = kRelocationSizeOffset + kIntSize;
   static const int kFlagsOffset = kSInfoSizeOffset + kIntSize;
   static const int kKindSpecificFlagsOffset  = kFlagsOffset + kIntSize;
@@ -2987,6 +3014,19 @@
     return ((1 << kIsExtensible) & bit_field2()) != 0;
   }
 
+  // Tells whether the instance has fast elements.
+  void set_has_fast_elements(bool value) {
+    if (value) {
+      set_bit_field2(bit_field2() | (1 << kHasFastElements));
+    } else {
+      set_bit_field2(bit_field2() & ~(1 << kHasFastElements));
+    }
+  }
+
+  bool has_fast_elements() {
+    return ((1 << kHasFastElements) & bit_field2()) != 0;
+  }
+
   // Tells whether the instance needs security checks when accessing its
   // properties.
   inline void set_is_access_check_needed(bool access_check_needed);
@@ -3010,6 +3050,16 @@
   // instance descriptors.
   Object* CopyDropTransitions();
 
+  // Returns this map if it has the fast elements bit set, otherwise
+  // returns a copy of the map, with all transitions dropped from the
+  // descriptors and the fast elements bit set.
+  inline Object* GetFastElementsMap();
+
+  // Returns this map if it has the fast elements bit cleared,
+  // otherwise returns a copy of the map, with all transitions dropped
+  // from the descriptors and the fast elements bit cleared.
+  inline Object* GetSlowElementsMap();
+
   // Returns the property index for name (only valid for FAST MODE).
   int PropertyIndexFor(String* name);
 
@@ -3111,6 +3161,7 @@
   // Bit positions for bit field 2
   static const int kIsExtensible = 0;
   static const int kFunctionWithPrototype = 1;
+  static const int kHasFastElements = 2;
 
   // Layout of the default cache. It holds alternating name and code objects.
   static const int kCodeCacheEntrySize = 2;
Index: src/SConscript
===================================================================
--- src/SConscript	(revision 4925)
+++ src/SConscript	(working copy)
@@ -61,6 +61,8 @@
     dtoa.cc
     execution.cc
     factory.cc
+    fast-dtoa.cc
+    fixed-dtoa.cc
     flags.cc
     flow-graph.cc
     frame-element.cc
@@ -68,8 +70,6 @@
     full-codegen.cc
     func-name-inferrer.cc
     global-handles.cc
-    fast-dtoa.cc
-    fixed-dtoa.cc
     handles.cc
     hashmap.cc
     heap-profiler.cc
@@ -83,6 +83,7 @@
     log.cc
     mark-compact.cc
     messages.cc
+    naclcode.cc
     objects.cc
     oprofile-agent.cc
     parser.cc
@@ -197,6 +198,7 @@
     x64/register-allocator-x64.cc
     x64/stub-cache-x64.cc
     x64/virtual-frame-x64.cc
+    x64/nacl-sandbox-x64.cc
     """),
   'simulator:arm': ['arm/simulator-arm.cc'],
   'simulator:mips': ['mips/simulator-mips.cc'],
@@ -206,6 +208,7 @@
   'os:android': ['platform-linux.cc', 'platform-posix.cc'],
   'os:macos':   ['platform-macos.cc', 'platform-posix.cc'],
   'os:solaris': ['platform-solaris.cc', 'platform-posix.cc'],
+  'os:nacl':    ['platform-nacl.cc', 'platform-posix.cc'],
   'os:nullos':  ['platform-nullos.cc'],
   'os:win32':   ['platform-win32.cc'],
   'mode:release': [],
@@ -237,6 +240,9 @@
   'os:solaris': [
     'd8-posix.cc'
   ],
+  'os:nacl': [
+    'd8-posix.cc'
+  ],
   'os:win32': [
     'd8-windows.cc'
   ],
Index: src/objects-debug.cc
===================================================================
--- src/objects-debug.cc	(revision 4925)
+++ src/objects-debug.cc	(working copy)
@@ -539,6 +539,9 @@
              (map()->inobject_properties() + properties()->length() -
               map()->NextFreePropertyIndex()));
   }
+  ASSERT(map()->has_fast_elements() ==
+         (elements()->map() == Heap::fixed_array_map()));
+  ASSERT(map()->has_fast_elements() == HasFastElements());
 }
 
 
@@ -589,6 +592,10 @@
     case JS_BUILTINS_OBJECT_TYPE: return "JS_BUILTINS_OBJECT";
     case JS_GLOBAL_PROXY_TYPE: return "JS_GLOBAL_PROXY";
     case PROXY_TYPE: return "PROXY";
+#ifndef ENABLE_DEBUGGER_SUPPORT
+    case DEBUG_INFO_TYPE:       return "DEBUG_INFO_TYPE";
+    case BREAK_POINT_INFO_TYPE: return "BREAK_POINT_INFO_TYPE";
+#endif
 #define MAKE_STRUCT_CASE(NAME, Name, name) case NAME##_TYPE: return #NAME;
   STRUCT_LIST(MAKE_STRUCT_CASE)
 #undef MAKE_STRUCT_CASE
Index: src/utils.h
===================================================================
--- src/utils.h	(revision 4925)
+++ src/utils.h	(working copy)
@@ -315,7 +315,12 @@
 class Vector {
  public:
   Vector() : start_(NULL), length_(0) {}
+#ifdef NACL
+  // inlining of this constructed causes seg faults in NaCl
+  Vector(T* data, int length) __attribute__ ((noinline)) : start_(data), length_(length) {
+#else
   Vector(T* data, int length) : start_(data), length_(length) {
+#endif
     ASSERT(length == 0 || (length > 0 && data != NULL));
   }
 
@@ -391,6 +396,7 @@
   // Factory method for creating empty vectors.
   static Vector<T> empty() { return Vector<T>(NULL, 0); }
 
+
  protected:
   void set_start(T* start) { start_ = start; }
 
@@ -399,7 +405,14 @@
   int length_;
 };
 
+#ifdef NACL
+template<typename A, typename B>
+inline bool operator==(const Vector<A>& a, const Vector<B>& b) {
+  return a.length()==b.length() && (void*)a.start()==(void*)b.start();
+}
+#endif
 
+
 // A temporary assignment sets a (non-local) variable to a value on
 // construction and resets it the value on destruction.
 template <typename T>
@@ -578,6 +591,7 @@
 static inline void MemCopy(void* dest, const void* src, size_t size) {
   static MemCopyFunction memcopy = CreateMemCopyFunction();
   (*memcopy)(dest, src, size);
+
 #ifdef DEBUG
   CHECK_EQ(0, memcmp(dest, src, size));
 #endif
@@ -587,7 +601,7 @@
 // Limit below which the extra overhead of the MemCopy function is likely
 // to outweigh the benefits of faster copying.
 // TODO(lrn): Try to find a more precise value.
-static const int kMinComplexMemCopy = 256;
+static const int kMinComplexMemCopy = 64;
 
 #else  // V8_TARGET_ARCH_IA32
 
@@ -662,7 +676,8 @@
 #define STOS "stosq"
 #endif
 
-#if defined(__GNUC__) && defined(STOS)
+// NACL_CHANGE(pmarch)
+#if defined(__GNUC__) && defined(STOS) && !defined(NACL)
   asm volatile(
       "cld;"
       "rep ; " STOS
Index: src/ast-inl.h
===================================================================
--- src/ast-inl.h	(revision 4925)
+++ src/ast-inl.h	(working copy)
@@ -45,7 +45,9 @@
 
 
 IterationStatement::IterationStatement(ZoneStringList* labels)
-    : BreakableStatement(labels, TARGET_FOR_ANONYMOUS), body_(NULL) {
+    : BreakableStatement(labels, TARGET_FOR_ANONYMOUS),
+      body_(NULL),
+      continue_target_(JumpTarget::BIDIRECTIONAL) {
 }
 
 
Index: src/spaces.cc
===================================================================
--- src/spaces.cc	(revision 4925)
+++ src/spaces.cc	(working copy)
@@ -227,7 +227,11 @@
     GetNextAllocationBlock(requested);
   }
   // Commit the requested memory at the start of the current allocation block.
+#ifdef NACL
+  *allocated = RoundUp(requested, NACL_MAP_PAGE);
+#else
   *allocated = RoundUp(requested, Page::kPageSize);
+#endif
   FreeBlock current = allocation_list_[current_allocation_block_index_];
   if (*allocated >= current.size - Page::kPageSize) {
     // Don't leave a small free block, useless for a large object or chunk.
Index: src/serialize.cc
===================================================================
--- src/serialize.cc	(revision 4925)
+++ src/serialize.cc	(working copy)
@@ -364,6 +364,7 @@
       UNCLASSIFIED,
       6,
       "RegExpStack::limit_address()");
+#ifndef V8_INTERPRETED_REGEXP
   Add(ExternalReference::address_of_regexp_stack_memory_address().address(),
       UNCLASSIFIED,
       7,
@@ -376,6 +377,7 @@
       UNCLASSIFIED,
       9,
       "OffsetsVector::static_offsets_vector");
+#endif
   Add(ExternalReference::new_space_start().address(),
       UNCLASSIFIED,
       10,
@@ -1002,7 +1004,7 @@
 
 void StartupSerializer::SerializeStrongReferences() {
   // No active threads.
-  CHECK_EQ(NULL, ThreadState::FirstInUse());
+  CHECK_EQ((void*)0, ThreadState::FirstInUse());
   // No active or weak handles.
   CHECK(HandleScopeImplementer::instance()->blocks()->is_empty());
   CHECK_EQ(0, GlobalHandles::NumberOfWeakHandles());
Index: src/builtins.cc
===================================================================
--- src/builtins.cc	(revision 4925)
+++ src/builtins.cc	(working copy)
@@ -195,6 +195,7 @@
   }
 
   // 'array' now contains the JSArray we should initialize.
+  ASSERT(array->HasFastElements());
 
   // Optimize the case where there is one argument and the argument is a
   // small smi.
@@ -958,7 +959,7 @@
         holder,
         callee,
         is_construct,
-        reinterpret_cast<void**>(&args[0] - 1),
+        reinterpret_cast<void**>(&args[0]-1 - 1),
         args.length() - 1);
 
     v8::Handle<v8::Value> value;
@@ -1039,7 +1040,7 @@
       holder,
       callee,
       is_construct,
-      reinterpret_cast<void**>(&args[0] - 1),
+      reinterpret_cast<void**>(&args[0]-1 - 1),
       args_length - 1);
 
   HandleScope scope;
@@ -1108,7 +1109,7 @@
         self,
         callee,
         is_construct_call,
-        reinterpret_cast<void**>(&args[0] - 1),
+        reinterpret_cast<void**>(&args[0]-1 - 1),
         args.length() - 1);
     v8::Handle<v8::Value> value;
     {
@@ -1269,7 +1270,7 @@
 
 static void Generate_StoreIC_ArrayLength(MacroAssembler* masm) {
   StoreIC::GenerateArrayLength(masm);
-}
+}	
 
 
 static void Generate_KeyedStoreIC_Generic(MacroAssembler* masm) {
@@ -1452,6 +1453,9 @@
   // separate code object for each one.
   for (int i = 0; i < builtin_count; i++) {
     if (create_heap_objects) {
+#if defined(DEBUG) || defined(NACL)
+      memset(buffer,0x90, sizeof buffer); // fill buffer with halts
+#endif
       MacroAssembler masm(buffer, sizeof buffer);
       // Generate the code/adaptor.
       typedef void (*Generator)(MacroAssembler*, int, BuiltinExtraArguments);
Index: src/v8-counters.h
===================================================================
--- src/v8-counters.h	(revision 4925)
+++ src/v8-counters.h	(working copy)
@@ -153,6 +153,10 @@
   SC(keyed_store_inline_miss, V8.KeyedStoreInlineMiss)                \
   SC(named_store_global_inline, V8.NamedStoreGlobalInline)            \
   SC(named_store_global_inline_miss, V8.NamedStoreGlobalInlineMiss)   \
+  SC(call_miss, V8.CallMiss)                                          \
+  SC(keyed_call_miss, V8.KeyedCallMiss)                               \
+  SC(load_miss, V8.LoadMiss)                                          \
+  SC(keyed_load_miss, V8.KeyedLoadMiss)                               \
   SC(call_const, V8.CallConst)                                        \
   SC(call_const_fast_api, V8.CallConstFastApi)                        \
   SC(call_const_interceptor, V8.CallConstInterceptor)                 \
Index: src/full-codegen.cc
===================================================================
--- src/full-codegen.cc	(revision 4925)
+++ src/full-codegen.cc	(working copy)
@@ -690,11 +690,11 @@
 int FullCodeGenerator::SlotOffset(Slot* slot) {
   ASSERT(slot != NULL);
   // Offset is negative because higher indexes are at lower addresses.
-  int offset = -slot->index() * kPointerSize;
+  int offset = -slot->index() * kStackPointerSize;
   // Adjust by a (parameter or local) base offset.
   switch (slot->type()) {
     case Slot::PARAMETER:
-      offset += (scope()->num_parameters() + 1) * kPointerSize;
+      offset += (scope()->num_parameters() + 1) * kStackPointerSize;
       break;
     case Slot::LOCAL:
       offset += JavaScriptFrameConstants::kLocal0Offset;
@@ -754,6 +754,7 @@
         }
       }
     }
+
     // Invoke the platform-dependent code generator to do the actual
     // declaration the global variables and functions.
     DeclareGlobals(array);
Index: src/jsregexp.cc
===================================================================
--- src/jsregexp.cc	(revision 4925)
+++ src/jsregexp.cc	(working copy)
@@ -796,7 +796,11 @@
   macro_assembler_->PushBacktrack(&fail);
   Trace new_trace;
   start->Emit(this, &new_trace);
+#ifdef NACL
+  macro_assembler_->Bind_Aligned(&fail);
+#else
   macro_assembler_->Bind(&fail);
+#endif
   macro_assembler_->Fail();
   while (!work_list.is_empty()) {
     work_list.RemoveLast()->Emit(this, &new_trace);
@@ -1067,7 +1071,11 @@
   successor->Emit(compiler, &new_state);
 
   // On backtrack we need to restore state.
+#ifdef NACL
+  assembler->Bind_Aligned(&undo);
+#else
   assembler->Bind(&undo);
+#endif
   RestoreAffectedRegisters(assembler,
                            max_register,
                            registers_to_pop,
Index: src/checks.h
===================================================================
--- src/checks.h	(revision 4925)
+++ src/checks.h	(working copy)
@@ -29,6 +29,7 @@
 #define V8_CHECKS_H_
 
 #include <string.h>
+#include <sstream>
 
 #include "flags.h"
 
@@ -67,7 +68,6 @@
 // prints a message to stderr and aborts.
 #define CHECK(condition) CheckHelper(__FILE__, __LINE__, #condition, condition)
 
-
 // Helper function used by the CHECK_EQ function when given int
 // arguments.  Should not be called directly.
 static inline void CheckEqualsHelper(const char* file, int line,
@@ -81,6 +81,7 @@
 }
 
 
+
 // Helper function used by the CHECK_EQ function when given int64_t
 // arguments.  Should not be called directly.
 static inline void CheckEqualsHelper(const char* file, int line,
@@ -102,7 +103,6 @@
   }
 }
 
-
 // Helper function used by the CHECK_NE function when given int
 // arguments.  Should not be called directly.
 static inline void CheckNonEqualsHelper(const char* file,
@@ -245,15 +245,52 @@
                        const char* value_source,
                        v8::Handle<v8::Value> value);
 
+#ifdef NACL
 
+template <typename A, typename B>
+inline bool check_cmp(A a, B b) {
+  return a==b;
+}
+template <>
+inline bool check_cmp(int a, unsigned int b) {
+  return (unsigned int)a==b;
+}
+template <>
+inline bool check_cmp(int a, unsigned long b) {
+  return (unsigned long)a==b;
+}
+template <>
+inline bool check_cmp(unsigned int b, int a) {
+  return (unsigned int)a==b;
+}
+template <>
+inline bool check_cmp(unsigned long b, int a) {
+  return (unsigned long)a==b;
+}
+template <>
+inline bool check_cmp(const char* a , const char* b) { return strcmp(a,b)==0; }
+template <typename T>
+inline bool check_cmp(int a , T* b) {
+  CHECK(a==0);
+  return b==0;
+}
+template <typename T>
+inline bool check_cmp(T* b, int a) {
+  CHECK(a==0);
+  return b==0;
+}
+
+#define CHECK_EQ(a, b) CHECK( check_cmp((a), (b)))
+#define CHECK_NE(a, b) CHECK(!check_cmp((a), (b)))
+#else
 #define CHECK_EQ(expected, value) CheckEqualsHelper(__FILE__, __LINE__, \
   #expected, expected, #value, value)
 
 
 #define CHECK_NE(unexpected, value) CheckNonEqualsHelper(__FILE__, __LINE__, \
   #unexpected, unexpected, #value, value)
+#endif
 
-
 #define CHECK_GT(a, b) CHECK((a) > (b))
 #define CHECK_GE(a, b) CHECK((a) >= (b))
 
Index: src/execution.h
===================================================================
--- src/execution.h	(revision 4925)
+++ src/execution.h	(working copy)
@@ -232,9 +232,9 @@
   static void EnableInterrupts();
   static void DisableInterrupts();
 
-  static const uintptr_t kLimitSize = kPointerSize * 128 * KB;
+  static const uintptr_t kLimitSize = kStackPointerSize * 128 * KB;
 
-#ifdef V8_TARGET_ARCH_X64
+#if defined(V8_TARGET_ARCH_X64) & !defined(NACL)
   static const uintptr_t kInterruptLimit = V8_UINT64_C(0xfffffffffffffffe);
   static const uintptr_t kIllegalLimit = V8_UINT64_C(0xfffffffffffffff8);
 #else
Index: src/factory.cc
===================================================================
--- src/factory.cc	(revision 4925)
+++ src/factory.cc	(working copy)
@@ -274,11 +274,22 @@
   return copy;
 }
 
+
 Handle<Map> Factory::CopyMapDropTransitions(Handle<Map> src) {
   CALL_HEAP_FUNCTION(src->CopyDropTransitions(), Map);
 }
 
 
+Handle<Map> Factory::GetFastElementsMap(Handle<Map> src) {
+  CALL_HEAP_FUNCTION(src->GetFastElementsMap(), Map);
+}
+
+
+Handle<Map> Factory::GetSlowElementsMap(Handle<Map> src) {
+  CALL_HEAP_FUNCTION(src->GetSlowElementsMap(), Map);
+}
+
+
 Handle<FixedArray> Factory::CopyFixedArray(Handle<FixedArray> array) {
   CALL_HEAP_FUNCTION(array->Copy(), FixedArray);
 }
Index: src/globals.h
===================================================================
--- src/globals.h	(revision 4925)
+++ src/globals.h	(working copy)
@@ -132,6 +132,11 @@
 // than defining __STDC_CONSTANT_MACROS before including <stdint.h>, and it
 // works on compilers that don't have it (like MSVC).
 #if V8_HOST_ARCH_64_BIT
+#ifdef NACL
+#define V8_UINT64_C(x)  (x ## UL)
+#define V8_INT64_C(x)   (x ## L)
+#define V8_PTR_PREFIX ""
+#else  // NACL
 #ifdef _MSC_VER
 #define V8_UINT64_C(x)  (x ## UI64)
 #define V8_INT64_C(x)   (x ## I64)
@@ -141,6 +146,7 @@
 #define V8_INT64_C(x)   (x ## L)
 #define V8_PTR_PREFIX "l"
 #endif  // _MSC_VER
+#endif  // NACL
 #else  // V8_HOST_ARCH_64_BIT
 #define V8_PTR_PREFIX ""
 #endif  // V8_HOST_ARCH_64_BIT
@@ -186,10 +192,16 @@
 const int kPointerSize  = sizeof(void*);     // NOLINT
 const int kIntptrSize   = sizeof(intptr_t);  // NOLINT
 
-#if V8_HOST_ARCH_64_BIT
+#if defined(V8_HOST_ARCH_64_BIT) && !defined(NACL)
+// NACL_CHANGE(pmarch) NaCl x64 uses 32-bit pointers in heap and 64-bit pointers
+// on stack
+const int kStackPointerSize = 8;
+const int kStackPointerSizeLog2 = 3;
 const int kPointerSizeLog2 = 3;
 const intptr_t kIntptrSignBit = V8_INT64_C(0x8000000000000000);
 #else
+const int kStackPointerSize = 8;
+const int kStackPointerSizeLog2 = 3;
 const int kPointerSizeLog2 = 2;
 const intptr_t kIntptrSignBit = 0x80000000;
 #endif
@@ -206,7 +218,7 @@
 const intptr_t kPointerAlignmentMask = kPointerAlignment - 1;
 
 // Desired alignment for maps.
-#if V8_HOST_ARCH_64_BIT
+#if V8_HOST_ARCH_64_BIT && !defined(NACL)
 const intptr_t kMapAlignmentBits = kObjectAlignmentBits;
 #else
 const intptr_t kMapAlignmentBits = kObjectAlignmentBits + 3;
@@ -237,7 +249,7 @@
 
 // Zap-value: The value used for zapping dead objects.
 // Should be a recognizable hex value tagged as a heap object pointer.
-#ifdef V8_HOST_ARCH_64_BIT
+#if defined(V8_HOST_ARCH_64_BIT) && !defined(NACL)
 const Address kZapValue =
     reinterpret_cast<Address>(V8_UINT64_C(0xdeadbeedbeadbeed));
 const Address kHandleZapValue =
@@ -245,6 +257,7 @@
 const Address kFromSpaceZapValue =
     reinterpret_cast<Address>(V8_UINT64_C(0x1beefdad0beefdad));
 #else
+// NACL_CHANGE(pmarch) NaCl x64 uses 32-bit pointers
 const Address kZapValue = reinterpret_cast<Address>(0xdeadbeed);
 const Address kHandleZapValue = reinterpret_cast<Address>(0xbaddead);
 const Address kFromSpaceZapValue = reinterpret_cast<Address>(0xbeefdad);
Index: src/prettyprinter.cc
===================================================================
--- src/prettyprinter.cc	(revision 4925)
+++ src/prettyprinter.cc	(working copy)
@@ -455,8 +455,8 @@
     va_list arguments;
     va_start(arguments, format);
     int n = OS::VSNPrintF(Vector<char>(output_, size_) + pos_,
-                          format,
-                          arguments);
+                            format,
+                            arguments);
     va_end(arguments);
 
     if (n >= 0) {
Index: src/disassembler.cc
===================================================================
--- src/disassembler.cc	(revision 4925)
+++ src/disassembler.cc	(working copy)
@@ -231,8 +231,11 @@
           out.AddFormatted(" constructor,");
         }
         Code* code = Code::GetCodeFromTargetAddress(relocinfo.target_address());
-        Code::Kind kind = code->kind();
-        if (code->is_inline_cache_stub()) {
+        Code::Kind kind = (code!=NULL ? code->kind() : Code::FIRST_IC_KIND);
+        
+        if (code == NULL ) {
+          out.AddFormatted("    ;; (error-decoding)");
+        } else if (code->is_inline_cache_stub()) {
           if (rmode == RelocInfo::CODE_TARGET_CONTEXT) {
             out.AddFormatted(" contextual,");
           }
Index: src/utils.cc
===================================================================
--- src/utils.cc	(revision 4925)
+++ src/utils.cc	(working copy)
@@ -248,7 +248,7 @@
   int n = OS::VSNPrintF(buffer_ + position_, format, args);
   va_end(args);
   if (n < 0 || n >= (buffer_.length() - position_)) {
-    position_ = buffer_.length();
+    position_ = buffer_.length()-1;
   } else {
     position_ += n;
   }
Index: src/arguments.h
===================================================================
--- src/arguments.h	(revision 4925)
+++ src/arguments.h	(working copy)
@@ -50,7 +50,11 @@
 
   Object*& operator[] (int index) {
     ASSERT(0 <= index && index < length_);
-    return arguments_[-index];
+#if defined(NACL) && defined(V8_HOST_ARCH_64_BIT)
+    return arguments_[-index*2];
+#else
+    return arguments_[-index*1];
+#endif
   }
 
   template <class S> Handle<S> at(int index) {
@@ -67,8 +71,13 @@
   Object** arguments() { return arguments_; }
 
  private:
+#if defined(V8_HOST_ARCH_64_BIT) && defined(NACL)
+  int length_ __attribute__ ((aligned (8)));
+  Object** arguments_ __attribute__ ((aligned (8)));
+#else
   int length_;
   Object** arguments_;
+#endif
 };
 
 
Index: src/ia32/assembler-ia32-inl.h
===================================================================
--- src/ia32/assembler-ia32-inl.h	(revision 4925)
+++ src/ia32/assembler-ia32-inl.h	(working copy)
@@ -39,6 +39,9 @@
 
 #include "cpu.h"
 #include "debug.h"
+#ifdef NACL
+#include "../naclcode.h"
+#endif
 
 namespace v8 {
 namespace internal {
@@ -84,9 +87,9 @@
 }
 
 
-void RelocInfo::set_target_address(Address target) {
+void RelocInfo::set_target_address(Address target, intptr_t extraoffset) {
   ASSERT(IsCodeTarget(rmode_) || rmode_ == RUNTIME_ENTRY);
-  Assembler::set_target_address_at(pc_, target);
+  Assembler::set_target_address_at(pc_, target, extraoffset);
 }
 
 
@@ -261,7 +264,9 @@
 void Assembler::emit_code_relative_offset(Label* label) {
   if (label->is_bound()) {
     int32_t pos;
-    pos = label->pos() + Code::kHeaderSize - kHeapObjectTag;
+    //NACL: do instructions deref elsewhere
+    //pos = label->pos() + Code::kHeaderSize - kHeapObjectTag;
+    pos = label->pos();
     emit(pos);
   } else {
     emit_disp(label, Displacement::CODE_RELATIVE);
@@ -282,9 +287,19 @@
 }
 
 
-void Assembler::set_target_address_at(Address pc, Address target) {
+void Assembler::set_target_address_at(Address pc, Address target, intptr_t extraoffset) {
+  int32_t val = target - (pc + sizeof(int32_t) + extraoffset);
   int32_t* p = reinterpret_cast<int32_t*>(pc);
-  *p = target - (pc + sizeof(int32_t));
+#ifdef NACL
+  ASSERT((reinterpret_cast<uintptr_t>(target) & (NACL_CHUNK-1)) == 0);
+  if(NaClCode::IsProtectedCode(pc)) {
+    NaClCode::PatchWord(p, val);
+  }else{
+    *p = val;
+  }
+#else
+  *p = val;
+#endif
   CPU::FlushICache(p, sizeof(int32_t));
 }
 
Index: src/ia32/codegen-ia32.cc
===================================================================
--- src/ia32/codegen-ia32.cc	(revision 4925)
+++ src/ia32/codegen-ia32.cc	(working copy)
@@ -42,6 +42,9 @@
 #include "runtime.h"
 #include "scopes.h"
 #include "virtual-frame-inl.h"
+#ifdef NACL
+#include "naclcode.h"
+#endif
 
 namespace v8 {
 namespace internal {
@@ -8784,6 +8787,9 @@
     // which allows the assert below to succeed and patching to work.
     deferred->Branch(not_equal);
 
+#if defined(NACL) && NACL_CHUNK==16
+    masm()->Align(NACL_CHUNK);
+#endif
     // The delta from the patch label to the load offset must be statically
     // known.
     ASSERT(masm()->SizeOfCodeGeneratedSince(deferred->patch_site()) ==
@@ -8853,7 +8859,7 @@
     // Use masm-> here instead of the double underscore macro since extra
     // coverage code can interfere with the patching.
     masm_->cmp(FieldOperand(receiver.reg(), HeapObject::kMapOffset),
-              Immediate(Factory::null_value()));
+               Immediate(Factory::null_value()));
     deferred->Branch(not_equal);
 
     // Check that the key is a smi.
@@ -8868,9 +8874,11 @@
     // is not a dictionary.
     __ mov(elements.reg(),
            FieldOperand(receiver.reg(), JSObject::kElementsOffset));
-    __ cmp(FieldOperand(elements.reg(), HeapObject::kMapOffset),
-           Immediate(Factory::fixed_array_map()));
-    deferred->Branch(not_equal);
+    if (FLAG_debug_code) {
+      __ cmp(FieldOperand(elements.reg(), HeapObject::kMapOffset),
+             Immediate(Factory::fixed_array_map()));
+      __ Assert(equal, "JSObject with fast elements map has slow elements");
+    }
 
     // Check that the key is within bounds.
     __ cmp(key.reg(),
@@ -11440,6 +11448,7 @@
 
   // Locate the code entry and call it.
   __ add(Operand(edx), Immediate(Code::kHeaderSize - kHeapObjectTag));
+  NACL_PATCH_INSTRUCTION_START(edx);
   __ CallCFunction(edx, kRegExpExecuteArguments);
 
   // Check the result.
@@ -12400,6 +12409,7 @@
   }
   __ mov(edx, Operand(edx, 0));  // deref address
   __ lea(edx, FieldOperand(edx, Code::kHeaderSize));
+  NACL_PATCH_INSTRUCTION_START(edx);
   __ call(Operand(edx));
 
   // Unlink this frame from the handler chain.
@@ -13293,6 +13303,9 @@
   __ test(edx, Immediate(kSmiTagMask));
   __ j(not_zero, &runtime);
   __ sub(ecx, Operand(edx));
+  __ cmp(ecx, FieldOperand(eax, String::kLengthOffset));
+  Label return_eax;
+  __ j(equal, &return_eax);
   // Special handling of sub-strings of length 1 and 2. One character strings
   // are handled in the runtime system (looked up in the single character
   // cache). Two character strings are looked for in the symbol cache.
@@ -13397,6 +13410,8 @@
   // esi: character of sub string start
   StringHelper::GenerateCopyCharactersREP(masm, edi, esi, ecx, ebx, false);
   __ mov(esi, edx);  // Restore esi.
+
+  __ bind(&return_eax);
   __ IncrementCounter(&Counters::sub_string_native, 1);
   __ ret(3 * kPointerSize);
 
@@ -13529,11 +13544,16 @@
 #define __ masm.
 
 MemCopyFunction CreateMemCopyFunction() {
-  size_t actual_size;
+  size_t actual_size = 0;
+#ifdef NACL
+  byte* buffer = 0;//let masm handle buffers
+#else
   byte* buffer = static_cast<byte*>(OS::Allocate(Assembler::kMinimalBufferSize,
                                                  &actual_size,
                                                  true));
   CHECK(buffer);
+#endif
+
   HandleScope handles;
   MacroAssembler masm(buffer, static_cast<int>(actual_size));
 
@@ -13725,6 +13745,14 @@
 
   CodeDesc desc;
   masm.GetCode(&desc);
+
+#ifdef NACL
+  //must validate and copy to executable space
+  buffer = NaClCode::Allocate(RoundUp(desc.instr_size, 32));
+  memset(desc.buffer+desc.instr_size, 0xf4, RoundUp(desc.instr_size, 32)-desc.instr_size);
+  NaClCode::Install(buffer, desc.buffer, RoundUp(desc.instr_size, 32));
+#endif
+
   // Call the function from C++.
   return FUNCTION_CAST<MemCopyFunction>(buffer);
 }
Index: src/ia32/regexp-macro-assembler-ia32.cc
===================================================================
--- src/ia32/regexp-macro-assembler-ia32.cc	(revision 4925)
+++ src/ia32/regexp-macro-assembler-ia32.cc	(working copy)
@@ -154,7 +154,9 @@
   CheckPreemption();
   // Pop Code* offset from backtrack stack, add Code* and jump to location.
   Pop(ebx);
-  __ add(Operand(ebx), Immediate(masm_->CodeObject()));
+  //NACL_CHANGE: dont use code object directly
+  //__ add(Operand(ebx), Immediate(masm_->CodeObject()));
+  AddInstructionStartToRegister(ebx);
   __ jmp(Operand(ebx));
 }
 
@@ -1154,7 +1156,9 @@
 
 void RegExpMacroAssemblerIA32::SafeReturn() {
   __ pop(ebx);
-  __ add(Operand(ebx), Immediate(masm_->CodeObject()));
+  //NACL_CHANGE: dont use code object directly
+  //__ add(Operand(ebx), Immediate(masm_->CodeObject()));
+  AddInstructionStartToRegister(ebx);
   __ jmp(Operand(ebx));
 }
 
@@ -1162,8 +1166,30 @@
 void RegExpMacroAssemblerIA32::SafeCallTarget(Label* name) {
   __ bind(name);
 }
+  
+inline void RegExpMacroAssemblerIA32::AddInstructionStartToRegister(Register reg) {
+  //NACL_CHANGE: added this function
 
+  //find a tmp that is different than reg
+  Register tmp = (reg.code()==eax.code() ? ebx : eax);
 
+  //spill current value of tmp
+  //TODO(janse): see if I really need spill or if I can trash tmp
+  //__ push(tmp);
+
+  // load code object and deref external code point
+  __ mov(tmp, Immediate(masm_->CodeObject()));
+  __ add(Operand(tmp), Immediate(Code::kHeaderSize - kHeapObjectTag));
+  NACL_PATCH_INSTRUCTION_START(tmp);
+
+  // add the computed value
+  __ add(reg, Operand(tmp));
+
+  //restore value of tmp
+  //__ pop(tmp);
+}
+
+
 void RegExpMacroAssemblerIA32::Push(Register source) {
   ASSERT(!source.is(backtrack_stackpointer()));
   // Notice: This updates flags, unlike normal Push.
Index: src/ia32/full-codegen-ia32.cc
===================================================================
--- src/ia32/full-codegen-ia32.cc	(revision 4925)
+++ src/ia32/full-codegen-ia32.cc	(working copy)
@@ -202,6 +202,11 @@
       __ push(eax);
       __ CallRuntime(Runtime::kTraceExit, 1);
     }
+#ifdef NACL
+    //make entire return sequence packed in one block
+    __ Align(32);
+#endif
+
 #ifdef DEBUG
     // Add a label for checking the size of the code used for returning.
     Label check_exit_codesize;
@@ -2175,7 +2180,7 @@
   // LAST_JS_OBJECT_TYPE.
   ASSERT(LAST_TYPE == JS_FUNCTION_TYPE);
   ASSERT(JS_FUNCTION_TYPE == LAST_JS_OBJECT_TYPE + 1);
-  __ cmp(ebx, JS_FUNCTION_TYPE);
+  __ CmpInstanceType(eax, JS_FUNCTION_TYPE);
   __ j(equal, &function);
 
   // Check if the constructor in the map is a function.
Index: src/ia32/codegen-ia32.h
===================================================================
--- src/ia32/codegen-ia32.h	(revision 4925)
+++ src/ia32/codegen-ia32.h	(working copy)
@@ -797,7 +797,7 @@
         args_in_registers_(false),
         args_reversed_(false),
         static_operands_type_(operands_type),
-        runtime_operands_type_(BinaryOpIC::DEFAULT),
+        runtime_operands_type_(FLAG_use_ic ? BinaryOpIC::DEFAULT : BinaryOpIC::GENERIC),
         name_(NULL) {
     if (static_operands_type_.IsSmi()) {
       mode_ = NO_OVERWRITE;
Index: src/ia32/regexp-macro-assembler-ia32.h
===================================================================
--- src/ia32/regexp-macro-assembler-ia32.h	(revision 4925)
+++ src/ia32/regexp-macro-assembler-ia32.h	(working copy)
@@ -174,6 +174,11 @@
   inline void SafeReturn();
   inline void SafeCallTarget(Label* name);
 
+  // NACL_CHANGE:
+  // Add the pointer to the start of the instrcution block for this code to a 
+  // given register
+  inline void AddInstructionStartToRegister(Register reg);
+
   // Pushes the value of a register on the backtrack stack. Decrements the
   // stack pointer (ecx) by a word size and stores the register's value there.
   inline void Push(Register source);
Index: src/ia32/macro-assembler-ia32.cc
===================================================================
--- src/ia32/macro-assembler-ia32.cc	(revision 4925)
+++ src/ia32/macro-assembler-ia32.cc	(working copy)
@@ -35,6 +35,8 @@
 #include "runtime.h"
 #include "serialize.h"
 
+#define __ 
+
 namespace v8 {
 namespace internal {
 
@@ -70,7 +72,45 @@
   shr(addr, Page::kRegionSizeLog2);
 
   // Set dirty mark for region.
+#if defined(NACL) && 0 /* nacl now supports bts, in dev */
+  //this special case can be removed once the following bug is fixed:
+  //http://code.google.com/p/nativeclient/issues/detail?id=688
+  //compute address of target word into object
+  mov(scratch, addr);
+  shr(scratch, 0x5);
+  shl(scratch, 0x2);
+  lea(object, Operand(object, Page::kDirtyFlagOffset));
+  add(object, Operand(scratch));
+  //compute mask into scratch
+  and_(addr,   0x1f);
+  mov(scratch, 0x1);
+  if(addr.is(ecx)){
+    //shift uses lower bits of addr/ecx
+    shl_cl(scratch);
+  }else if(!scratch.is(ecx)){
+    //temporarily put addr in ecx for shift, since shl_cl requires ecx
+    //ecx is unused, so just swap with it
+    xchg(ecx, addr);
+    shl_cl(scratch);
+    xchg(ecx, addr);
+  }else{
+    // scratch is ecx
+    // need 3 way swap to get valid register assignment
+    // addr    => scratch/ecx
+    // scratch => object
+    // object  => addr
+    xchg(object, scratch);
+    xchg(ecx, addr);
+    shl_cl(object);
+    //undo swap
+    xchg(ecx, addr);
+    xchg(object, scratch);
+  }
+  //mask target word
+  or_(Operand(object,0), scratch);
+#else
   bts(Operand(object, Page::kDirtyFlagOffset), addr);
+#endif
 }
 
 
@@ -1286,6 +1326,7 @@
     if (!code_constant.is_null()) {
       mov(edx, Immediate(code_constant));
       add(Operand(edx), Immediate(Code::kHeaderSize - kHeapObjectTag));
+      NACL_PATCH_INSTRUCTION_START(edx);
     } else if (!code_operand.is_reg(edx)) {
       mov(edx, code_operand);
     }
@@ -1345,6 +1386,7 @@
   SmiUntag(ebx);
   mov(edx, FieldOperand(edx, SharedFunctionInfo::kCodeOffset));
   lea(edx, FieldOperand(edx, Code::kHeaderSize));
+  NACL_PATCH_INSTRUCTION_START(edx);
 
   ParameterCount expected(ebx);
   InvokeCode(Operand(edx), expected, actual, flag);
@@ -1402,6 +1444,7 @@
     pop(target);
   }
   lea(target, FieldOperand(target, Code::kHeaderSize));
+  NACL_PATCH_INSTRUCTION_START(target);
 }
 
 
Index: src/ia32/assembler-ia32.cc
===================================================================
--- src/ia32/assembler-ia32.cc	(revision 4925)
+++ src/ia32/assembler-ia32.cc	(working copy)
@@ -42,9 +42,17 @@
 #include "macro-assembler.h"
 #include "serialize.h"
 
+//length of an Operator
+#define L(op) op.len_
+#define I(imm32) Immediate(imm32).is_int8() ? 1 : 4
+//min offset in the bundle we must be to use a call
+#define NACL_USE_CALL_THRESH 20
+
+
 namespace v8 {
 namespace internal {
 
+
 // -----------------------------------------------------------------------------
 // Implementation of CpuFeatures
 
@@ -67,8 +75,38 @@
   Assembler assm(NULL, 0);
   Label cpuid, done;
 #define __ assm.
+#ifdef NACL
   // Save old esp, since we are going to modify the stack.
   __ push(ebp);
+  __ push(ecx);
+  __ push(ebx);
+  __ mov(ebp, Operand(esp));
+
+  // Invoke CPUID with 1 in eax to get feature information in
+  // ecx:edx. Temporarily enable CPUID support because we know it's
+  // safe here.
+  __ mov(eax, 1);
+  supported_ = (1 << CPUID);
+  { Scope fscope(CPUID);
+    __ cpuid();
+  }
+  supported_ = 0;
+
+  // Move the result from ecx:edx to edx:eax and make sure to mark the
+  // CPUID feature as supported.
+  __ mov(eax, Operand(edx));
+  __ or_(eax, 1 << CPUID);
+  __ mov(edx, Operand(ecx));
+
+  // Done.
+  __ mov(esp, Operand(ebp));
+  __ pop(ebx);
+  __ pop(ecx);
+  __ pop(ebp);
+  __ ret(0);
+#else
+  // Save old esp, since we are going to modify the stack.
+  __ push(ebp);
   __ pushfd();
   __ push(ecx);
   __ push(ebx);
@@ -116,6 +154,7 @@
   __ popfd();
   __ pop(ebp);
   __ ret(0);
+#endif
 #undef __
 
   CodeDesc desc;
@@ -322,7 +361,8 @@
   // Clear the buffer in debug mode unless it was provided by the
   // caller in which case we can't be sure it's okay to overwrite
   // existing code in it; see CodePatcher::CodePatcher(...).
-#ifdef DEBUG
+#if defined(DEBUG) || defined(NACL)
+  //NACL_CHANGE: this is required for validation.... should figure out where it is using uninitialized memory
   if (own_buffer_) {
     memset(buffer_, 0xCC, buffer_size);  // int3
   }
@@ -372,8 +412,9 @@
 
 void Assembler::Align(int m) {
   ASSERT(IsPowerOf2(m));
-  while ((pc_offset() & (m - 1)) != 0) {
-    nop();
+  int left = m - (pc_offset() & (m-1));
+  if(left != m) {
+    nops(left);
   }
 }
 
@@ -385,7 +426,7 @@
 
 void Assembler::cpuid() {
   ASSERT(CpuFeatures::IsEnabled(CPUID));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0x0F);
   EMIT(0xA2);
@@ -393,40 +434,42 @@
 
 
 void Assembler::pushad() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x60);
 }
 
 
 void Assembler::popad() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x61);
 }
 
 
 void Assembler::pushfd() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x9C);
 }
 
 
 void Assembler::popfd() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x9D);
 }
 
 
 void Assembler::push(const Immediate& x) {
-  EnsureSpace ensure_space(this);
-  last_pc_ = pc_;
   if (x.is_int8()) {
+    EnsureSpace ensure_space(this, 2);
+    last_pc_ = pc_;
     EMIT(0x6a);
     EMIT(x.x_);
   } else {
+    EnsureSpace ensure_space(this, 5);
+    last_pc_ = pc_;
     EMIT(0x68);
     emit(x);
   }
@@ -434,19 +477,25 @@
 
 
 void Assembler::push(Register src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x50 | src.code());
 }
 
 
 void Assembler::push(const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(src));
   last_pc_ = pc_;
   EMIT(0xFF);
   emit_operand(esi, src);
 }
 
+void Assembler::push(byte* x, RelocInfo::Mode rmode) {
+  EnsureSpace ensure_space(this, 5);
+  last_pc_ = pc_;
+  EMIT(0x68);
+  emit(reinterpret_cast<uint32_t>(x), rmode);
+}
 
 void Assembler::pop(Register dst) {
   ASSERT(reloc_info_writer.last_pc() != NULL);
@@ -456,7 +505,12 @@
     // relocation information generated between the last instruction and this
     // pop instruction.
     byte instr = last_pc_[0];
-    if ((instr & ~0x7) == 0x50) {
+    if ((instr & ~0x7) == 0x50
+#ifdef NACL
+      //make sure we aren't at the end of a bundle
+      && (pc_offset()&31)<31
+#endif
+        ) {
       int push_reg_code = instr & 0x7;
       if (push_reg_code == dst.code()) {
         pc_ = last_pc_;
@@ -467,7 +521,7 @@
         // Convert 'push src; pop dst' to 'mov dst, src'.
         last_pc_[0] = 0x8b;
         Register src = { push_reg_code };
-        EnsureSpace ensure_space(this);
+        EnsureSpace ensure_space(this, 1);
         emit_operand(dst, Operand(src));
         if (FLAG_print_peephole_optimization) {
           PrintF("%d push/pop (reg->reg) eliminated\n", pc_offset());
@@ -505,7 +559,12 @@
         }
         return;
       }
-    } else if (instr == 0x6a && dst.is(eax)) {  // push of immediate 8 bit
+    } else if (instr == 0x6a && dst.is(eax)
+#ifdef NACL
+      //make sure we aren't at the end of a bundle
+      && (pc_offset()&31)<28
+#endif
+        ) {  // push of immediate 8 bit
       byte imm8 = last_pc_[1];
       if (imm8 == 0) {
         // 6a00         push 0x0
@@ -523,7 +582,7 @@
         // 6a00         push 0xXX
         // 58           pop eax
         last_pc_[0] = 0xb8;
-        EnsureSpace ensure_space(this);
+        EnsureSpace ensure_space(this, 3);
         if ((imm8 & 0x80) != 0) {
           EMIT(0xff);
           EMIT(0xff);
@@ -560,14 +619,14 @@
     // 0x712716   102  890424         mov [esp], eax
     // 0x712719   105  8b1424         mov edx, [esp]
   }
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x58 | dst.code());
 }
 
 
 void Assembler::pop(const Operand& dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(dst));
   last_pc_ = pc_;
   EMIT(0x8F);
   emit_operand(eax, dst);
@@ -575,6 +634,9 @@
 
 
 void Assembler::enter(const Immediate& size) {
+#ifdef NACL
+  UNIMPLEMENTED();
+#endif
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   EMIT(0xC8);
@@ -584,7 +646,7 @@
 
 
 void Assembler::leave() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0xC9);
 }
@@ -592,7 +654,7 @@
 
 void Assembler::mov_b(Register dst, const Operand& src) {
   ASSERT(dst.code() < 4);
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(src));
   last_pc_ = pc_;
   EMIT(0x8A);
   emit_operand(dst, src);
@@ -600,7 +662,7 @@
 
 
 void Assembler::mov_b(const Operand& dst, int8_t imm8) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(dst));
   last_pc_ = pc_;
   EMIT(0xC6);
   emit_operand(eax, dst);
@@ -610,7 +672,7 @@
 
 void Assembler::mov_b(const Operand& dst, Register src) {
   ASSERT(src.code() < 4);
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(dst));
   last_pc_ = pc_;
   EMIT(0x88);
   emit_operand(src, dst);
@@ -618,7 +680,7 @@
 
 
 void Assembler::mov_w(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(src));
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x8B);
@@ -627,7 +689,7 @@
 
 
 void Assembler::mov_w(const Operand& dst, Register src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(dst));
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x89);
@@ -636,7 +698,7 @@
 
 
 void Assembler::mov(Register dst, int32_t imm32) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5);
   last_pc_ = pc_;
   EMIT(0xB8 | dst.code());
   emit(imm32);
@@ -644,7 +706,7 @@
 
 
 void Assembler::mov(Register dst, const Immediate& x) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5);
   last_pc_ = pc_;
   EMIT(0xB8 | dst.code());
   emit(x);
@@ -652,7 +714,7 @@
 
 
 void Assembler::mov(Register dst, Handle<Object> handle) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5);
   last_pc_ = pc_;
   EMIT(0xB8 | dst.code());
   emit(handle);
@@ -660,7 +722,7 @@
 
 
 void Assembler::mov(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(src));
   last_pc_ = pc_;
   EMIT(0x8B);
   emit_operand(dst, src);
@@ -668,7 +730,7 @@
 
 
 void Assembler::mov(Register dst, Register src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0x89);
   EMIT(0xC0 | src.code() << 3 | dst.code());
@@ -676,7 +738,7 @@
 
 
 void Assembler::mov(const Operand& dst, const Immediate& x) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5+L(dst));
   last_pc_ = pc_;
   EMIT(0xC7);
   emit_operand(eax, dst);
@@ -685,7 +747,7 @@
 
 
 void Assembler::mov(const Operand& dst, Handle<Object> handle) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5+L(dst));
   last_pc_ = pc_;
   EMIT(0xC7);
   emit_operand(eax, dst);
@@ -694,7 +756,7 @@
 
 
 void Assembler::mov(const Operand& dst, Register src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(dst));
   last_pc_ = pc_;
   EMIT(0x89);
   emit_operand(src, dst);
@@ -702,7 +764,7 @@
 
 
 void Assembler::movsx_b(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(src));
   last_pc_ = pc_;
   EMIT(0x0F);
   EMIT(0xBE);
@@ -711,7 +773,7 @@
 
 
 void Assembler::movsx_w(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(src));
   last_pc_ = pc_;
   EMIT(0x0F);
   EMIT(0xBF);
@@ -720,7 +782,7 @@
 
 
 void Assembler::movzx_b(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(src));
   last_pc_ = pc_;
   EMIT(0x0F);
   EMIT(0xB6);
@@ -729,7 +791,7 @@
 
 
 void Assembler::movzx_w(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(src));
   last_pc_ = pc_;
   EMIT(0x0F);
   EMIT(0xB7);
@@ -761,7 +823,7 @@
 
 void Assembler::cmov(Condition cc, Register dst, const Operand& src) {
   ASSERT(CpuFeatures::IsEnabled(CMOV));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(src));
   last_pc_ = pc_;
   // Opcode: 0f 40 + cc /r.
   EMIT(0x0F);
@@ -771,14 +833,14 @@
 
 
 void Assembler::cld() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0xFC);
 }
 
 
 void Assembler::rep_movs() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xF3);
   EMIT(0xA5);
@@ -786,7 +848,7 @@
 
 
 void Assembler::rep_stos() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xF3);
   EMIT(0xAB);
@@ -794,18 +856,19 @@
 
 
 void Assembler::stos() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0xAB);
 }
 
 
 void Assembler::xchg(Register dst, Register src) {
-  EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   if (src.is(eax) || dst.is(eax)) {  // Single-byte encoding.
+    EnsureSpace ensure_space(this, 1);
     EMIT(0x90 | (src.is(eax) ? dst.code() : src.code()));
   } else {
+    EnsureSpace ensure_space(this, 2);
     EMIT(0x87);
     EMIT(0xC0 | src.code() << 3 | dst.code());
   }
@@ -813,14 +876,14 @@
 
 
 void Assembler::adc(Register dst, int32_t imm32) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 6);
   last_pc_ = pc_;
   emit_arith(2, Operand(dst), Immediate(imm32));
 }
 
 
 void Assembler::adc(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this,1+L(src));
   last_pc_ = pc_;
   EMIT(0x13);
   emit_operand(dst, src);
@@ -828,7 +891,7 @@
 
 
 void Assembler::add(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this,1+L(src));
   last_pc_ = pc_;
   EMIT(0x03);
   emit_operand(dst, src);
@@ -853,21 +916,21 @@
       }
     }
   }
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this,5+L(dst));
   last_pc_ = pc_;
   emit_arith(0, dst, x);
 }
 
 
 void Assembler::and_(Register dst, int32_t imm32) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, Immediate(imm32).is_int8() ? 3 : 6);
   last_pc_ = pc_;
   emit_arith(4, Operand(dst), Immediate(imm32));
 }
 
 
 void Assembler::and_(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(src));
   last_pc_ = pc_;
   EMIT(0x23);
   emit_operand(dst, src);
@@ -875,14 +938,14 @@
 
 
 void Assembler::and_(const Operand& dst, const Immediate& x) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5+L(dst));
   last_pc_ = pc_;
   emit_arith(4, dst, x);
 }
 
 
 void Assembler::and_(const Operand& dst, Register src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(dst));
   last_pc_ = pc_;
   EMIT(0x21);
   emit_operand(src, dst);
@@ -890,7 +953,7 @@
 
 
 void Assembler::cmpb(const Operand& op, int8_t imm8) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(op));
   last_pc_ = pc_;
   EMIT(0x80);
   emit_operand(edi, op);  // edi == 7
@@ -900,7 +963,7 @@
 
 void Assembler::cmpb(const Operand& dst, Register src) {
   ASSERT(src.is_byte_register());
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(dst));
   last_pc_ = pc_;
   EMIT(0x38);
   emit_operand(src, dst);
@@ -909,7 +972,7 @@
 
 void Assembler::cmpb(Register dst, const Operand& src) {
   ASSERT(dst.is_byte_register());
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(src));
   last_pc_ = pc_;
   EMIT(0x3A);
   emit_operand(dst, src);
@@ -928,21 +991,21 @@
 
 
 void Assembler::cmp(Register reg, int32_t imm32) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 6);
   last_pc_ = pc_;
   emit_arith(7, Operand(reg), Immediate(imm32));
 }
 
 
 void Assembler::cmp(Register reg, Handle<Object> handle) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 6);
   last_pc_ = pc_;
   emit_arith(7, Operand(reg), Immediate(handle));
 }
 
 
 void Assembler::cmp(Register reg, const Operand& op) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(op));
   last_pc_ = pc_;
   EMIT(0x3B);
   emit_operand(reg, op);
@@ -950,21 +1013,21 @@
 
 
 void Assembler::cmp(const Operand& op, const Immediate& imm) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5+L(op));
   last_pc_ = pc_;
   emit_arith(7, op, imm);
 }
 
 
 void Assembler::cmp(const Operand& op, Handle<Object> handle) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5+L(op));
   last_pc_ = pc_;
   emit_arith(7, op, Immediate(handle));
 }
 
 
 void Assembler::cmpb_al(const Operand& op) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(op));
   last_pc_ = pc_;
   EMIT(0x38);  // CMP r/m8, r8
   emit_operand(eax, op);  // eax has same code as register al.
@@ -972,7 +1035,7 @@
 
 
 void Assembler::cmpw_ax(const Operand& op) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(op));
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x39);  // CMP r/m16, r16
@@ -981,7 +1044,7 @@
 
 
 void Assembler::dec_b(Register dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xFE);
   EMIT(0xC8 | dst.code());
@@ -989,14 +1052,14 @@
 
 
 void Assembler::dec(Register dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x48 | dst.code());
 }
 
 
 void Assembler::dec(const Operand& dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(dst));
   last_pc_ = pc_;
   EMIT(0xFF);
   emit_operand(ecx, dst);
@@ -1004,14 +1067,14 @@
 
 
 void Assembler::cdq() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x99);
 }
 
 
 void Assembler::idiv(Register src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xF7);
   EMIT(0xF8 | src.code());
@@ -1019,7 +1082,7 @@
 
 
 void Assembler::imul(Register reg) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xF7);
   EMIT(0xE8 | reg.code());
@@ -1027,7 +1090,7 @@
 
 
 void Assembler::imul(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(src));
   last_pc_ = pc_;
   EMIT(0x0F);
   EMIT(0xAF);
@@ -1036,7 +1099,7 @@
 
 
 void Assembler::imul(Register dst, Register src, int32_t imm32) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 6);
   last_pc_ = pc_;
   if (is_int8(imm32)) {
     EMIT(0x6B);
@@ -1051,14 +1114,14 @@
 
 
 void Assembler::inc(Register dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x40 | dst.code());
 }
 
 
 void Assembler::inc(const Operand& dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(dst));
   last_pc_ = pc_;
   EMIT(0xFF);
   emit_operand(eax, dst);
@@ -1066,7 +1129,7 @@
 
 
 void Assembler::lea(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(src));
   last_pc_ = pc_;
   EMIT(0x8D);
   emit_operand(dst, src);
@@ -1074,7 +1137,7 @@
 
 
 void Assembler::mul(Register src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xF7);
   EMIT(0xE0 | src.code());
@@ -1082,7 +1145,7 @@
 
 
 void Assembler::neg(Register dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xF7);
   EMIT(0xD8 | dst.code());
@@ -1090,7 +1153,7 @@
 
 
 void Assembler::not_(Register dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xF7);
   EMIT(0xD0 | dst.code());
@@ -1098,14 +1161,14 @@
 
 
 void Assembler::or_(Register dst, int32_t imm32) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 6);
   last_pc_ = pc_;
   emit_arith(1, Operand(dst), Immediate(imm32));
 }
 
 
 void Assembler::or_(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(src));
   last_pc_ = pc_;
   EMIT(0x0B);
   emit_operand(dst, src);
@@ -1113,14 +1176,14 @@
 
 
 void Assembler::or_(const Operand& dst, const Immediate& x) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5+L(dst));
   last_pc_ = pc_;
   emit_arith(1, dst, x);
 }
 
 
 void Assembler::or_(const Operand& dst, Register src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(dst));
   last_pc_ = pc_;
   EMIT(0x09);
   emit_operand(src, dst);
@@ -1128,7 +1191,7 @@
 
 
 void Assembler::rcl(Register dst, uint8_t imm8) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3);
   last_pc_ = pc_;
   ASSERT(is_uint5(imm8));  // illegal shift count
   if (imm8 == 1) {
@@ -1143,7 +1206,7 @@
 
 
 void Assembler::sar(Register dst, uint8_t imm8) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3);
   last_pc_ = pc_;
   ASSERT(is_uint5(imm8));  // illegal shift count
   if (imm8 == 1) {
@@ -1158,7 +1221,7 @@
 
 
 void Assembler::sar_cl(Register dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD3);
   EMIT(0xF8 | dst.code());
@@ -1174,7 +1237,7 @@
 
 
 void Assembler::shld(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(src));
   last_pc_ = pc_;
   EMIT(0x0F);
   EMIT(0xA5);
@@ -1183,7 +1246,7 @@
 
 
 void Assembler::shl(Register dst, uint8_t imm8) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this,3);
   last_pc_ = pc_;
   ASSERT(is_uint5(imm8));  // illegal shift count
   if (imm8 == 1) {
@@ -1197,8 +1260,9 @@
 }
 
 
+
 void Assembler::shl_cl(Register dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD3);
   EMIT(0xE0 | dst.code());
@@ -1215,7 +1279,7 @@
 
 
 void Assembler::shr(Register dst, uint8_t imm8) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3);
   last_pc_ = pc_;
   ASSERT(is_uint5(imm8));  // illegal shift count
   if (imm8 == 1) {
@@ -1230,7 +1294,7 @@
 
 
 void Assembler::shr_cl(Register dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD3);
   EMIT(0xE8 | dst.code());
@@ -1251,14 +1315,14 @@
 
 
 void Assembler::sub(const Operand& dst, const Immediate& x) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this,5+L(dst));
   last_pc_ = pc_;
   emit_arith(5, dst, x);
 }
 
 
 void Assembler::sub(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(src));
   last_pc_ = pc_;
   EMIT(0x2B);
   emit_operand(dst, src);
@@ -1267,7 +1331,7 @@
 
 void Assembler::subb(Register dst, const Operand& src) {
   ASSERT(dst.code() < 4);
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(src));
   last_pc_ = pc_;
   EMIT(0x2A);
   emit_operand(dst, src);
@@ -1275,7 +1339,7 @@
 
 
 void Assembler::sub(const Operand& dst, Register src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(dst));
   last_pc_ = pc_;
   EMIT(0x29);
   emit_operand(src, dst);
@@ -1283,19 +1347,23 @@
 
 
 void Assembler::test(Register reg, const Immediate& imm) {
-  EnsureSpace ensure_space(this);
-  last_pc_ = pc_;
   // Only use test against byte for registers that have a byte
   // variant: eax, ebx, ecx, and edx.
   if (imm.rmode_ == RelocInfo::NONE && is_uint8(imm.x_) && reg.code() < 4) {
     uint8_t imm8 = imm.x_;
     if (reg.is(eax)) {
+      EnsureSpace ensure_space(this, 2);
+      last_pc_ = pc_;
       EMIT(0xA8);
       EMIT(imm8);
     } else {
+      EnsureSpace ensure_space(this, 3);
+      last_pc_ = pc_;
       emit_arith_b(0xF6, 0xC0, reg, imm8);
     }
   } else {
+    EnsureSpace ensure_space(this, reg.is(eax) ? 5 : 6);
+    last_pc_ = pc_;
     // This is not using emit_arith because test doesn't support
     // sign-extension of 8-bit operands.
     if (reg.is(eax)) {
@@ -1310,7 +1378,7 @@
 
 
 void Assembler::test(Register reg, const Operand& op) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(op));
   last_pc_ = pc_;
   EMIT(0x85);
   emit_operand(reg, op);
@@ -1318,7 +1386,7 @@
 
 
 void Assembler::test_b(Register reg, const Operand& op) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(op));
   last_pc_ = pc_;
   EMIT(0x84);
   emit_operand(reg, op);
@@ -1326,7 +1394,7 @@
 
 
 void Assembler::test(const Operand& op, const Immediate& imm) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5+L(op));
   last_pc_ = pc_;
   EMIT(0xF7);
   emit_operand(eax, op);
@@ -1335,7 +1403,7 @@
 
 
 void Assembler::test_b(const Operand& op, uint8_t imm8) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(op));
   last_pc_ = pc_;
   EMIT(0xF6);
   emit_operand(eax, op);
@@ -1344,14 +1412,14 @@
 
 
 void Assembler::xor_(Register dst, int32_t imm32) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 6);
   last_pc_ = pc_;
   emit_arith(6, Operand(dst), Immediate(imm32));
 }
 
 
 void Assembler::xor_(Register dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(src));
   last_pc_ = pc_;
   EMIT(0x33);
   emit_operand(dst, src);
@@ -1359,7 +1427,7 @@
 
 
 void Assembler::xor_(const Operand& src, Register dst) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(src));
   last_pc_ = pc_;
   EMIT(0x31);
   emit_operand(dst, src);
@@ -1367,7 +1435,7 @@
 
 
 void Assembler::xor_(const Operand& dst, const Immediate& x) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5+L(dst));
   last_pc_ = pc_;
   emit_arith(6, dst, x);
 }
@@ -1392,21 +1460,25 @@
 
 
 void Assembler::hlt() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0xF4);
 }
 
 
 void Assembler::int3() {
-  EnsureSpace ensure_space(this);
+#ifdef NACL
+  hlt();
+#else
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0xCC);
+#endif
 }
 
 
 void Assembler::nop() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x90);
 }
@@ -1422,9 +1494,16 @@
 
 
 void Assembler::ret(int imm16) {
-  EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   ASSERT(is_uint16(imm16));
+#if defined(NACL)
+  pop(rcx);
+  if(imm16 != 0) {
+    add(Operand(esp), Immediate(imm16));
+  }
+  jmp(Operand(ecx));
+#else
+  EnsureSpace ensure_space(this);
   if (imm16 == 0) {
     EMIT(0xC3);
   } else {
@@ -1432,6 +1511,7 @@
     EMIT(imm16 & 0xFF);
     EMIT((imm16 >> 8) & 0xFF);
   }
+#endif
 }
 
 
@@ -1468,7 +1548,7 @@
 
 
 void Assembler::bind_to(Label* L, int pos) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 0);
   last_pc_ = NULL;
   ASSERT(0 <= pos && pos <= pc_offset());  // must have a valid binding position
   while (L->is_linked()) {
@@ -1476,7 +1556,9 @@
     int fixup_pos = L->pos();
     if (disp.type() == Displacement::CODE_RELATIVE) {
       // Relative to Code* heap object pointer.
-      long_at_put(fixup_pos, pos + Code::kHeaderSize - kHeapObjectTag);
+      //NACL_CHANGE: don't include code offset in CODE_RELATIVE displacements
+      //long_at_put(fixup_pos, pos + Code::kHeaderSize - kHeapObjectTag);
+      long_at_put(fixup_pos, pos);
     } else {
       if (disp.type() == Displacement::UNCONDITIONAL_JUMP) {
         ASSERT(byte_at(fixup_pos - 1) == 0xE9);  // jmp expected
@@ -1492,7 +1574,7 @@
 
 
 void Assembler::link_to(Label* L, Label* appendix) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 0);
   last_pc_ = NULL;
   if (appendix->is_linked()) {
     if (L->is_linked()) {
@@ -1518,51 +1600,90 @@
 
 
 void Assembler::bind(Label* L) {
-  EnsureSpace ensure_space(this);
   last_pc_ = NULL;
   ASSERT(!L->is_bound());  // label can only be bound once
+  before_bind(L);
+  EnsureSpace ensure_space(this, 0);
   bind_to(L, pc_offset());
 }
 
 
 void Assembler::call(Label* L) {
-  EnsureSpace ensure_space(this);
-  last_pc_ = pc_;
-  if (L->is_bound()) {
-    const int long_size = 5;
-    int offs = L->pos() - pc_offset();
-    ASSERT(offs <= 0);
-    // 1110 1000 #32-bit disp.
-    EMIT(0xE8);
-    emit(offs - long_size);
-  } else {
-    // 1110 1000 #32-bit disp.
-    EMIT(0xE8);
-    emit_disp(L, Displacement::OTHER);
+#if defined(NACL)
+  int offset = pc_offset() &  (NACL_CHUNK-1);
+  if(NACL_USE_CALL_THRESH <= offset && offset<NACL_CHUNK-5) {
+#endif
+    before_call(5);
+    EnsureSpace ensure_space(this,5);
+    last_pc_ = pc_;
+    if (L->is_bound()) {
+      const int long_size = 5;
+      int offs = L->pos() - pc_offset();
+      ASSERT(offs <= 0);
+      // 1110 1000 #32-bit disp.
+      EMIT(0xE8);
+      emit(offs - long_size);
+    } else {
+      // 1110 1000 #32-bit disp.
+      EMIT(0xE8);
+      emit_disp(L, Displacement::OTHER);
+    }
+#if defined(NACL)
+  }else{
+    int retaddr = push_aligned_rv(5);
+    jmp(L);
+    nops(retaddr - pc_offset());
   }
+#endif
 }
 
 
 void Assembler::call(byte* entry, RelocInfo::Mode rmode) {
-  EnsureSpace ensure_space(this);
-  last_pc_ = pc_;
   ASSERT(!RelocInfo::IsCodeTarget(rmode));
-  EMIT(0xE8);
-  emit(entry - (pc_ + sizeof(int32_t)), rmode);
+#if defined(NACL)
+  int offset = pc_offset() &  (NACL_CHUNK-1);
+  if(NACL_USE_CALL_THRESH <= offset && offset<NACL_CHUNK-5) {
+#endif
+    before_call(5);
+    EnsureSpace ensure_space(this,5);
+    last_pc_ = pc_;
+    EMIT(0xE8);
+    emit(entry - (pc_ + sizeof(int32_t)), rmode);
+#if defined(NACL)
+  }else{
+    int retaddr = push_aligned_rv(5);
+    jmp(entry, rmode);
+    nops(retaddr - pc_offset());
+  }
+#endif
 }
 
 
 void Assembler::call(const Operand& adr) {
-  EnsureSpace ensure_space(this);
-  last_pc_ = pc_;
-  EMIT(0xFF);
-  emit_operand(edx, adr);
+#if defined(NACL)
+  int offset = pc_offset() &  (NACL_CHUNK-1);
+  if(NACL_USE_CALL_THRESH <= offset && offset<NACL_CHUNK-5) {
+#endif
+    ASSERT(L(adr) == 1);
+    before_branch(adr, 5);
+    EnsureSpace ensure_space(this, 1+L(adr));
+    last_pc_ = pc_;
+    EMIT(0xFF);
+    emit_operand(edx, adr);
+#if defined(NACL)
+  }else{
+    int retaddr = push_aligned_rv(5);
+    jmp(adr);
+    nops(retaddr - pc_offset());
+  }
+#endif
 }
 
 
 void Assembler::call(Handle<Code> code, RelocInfo::Mode rmode) {
   WriteRecordedPositions();
-  EnsureSpace ensure_space(this);
+  before_call(5);
+  EnsureSpace ensure_space(this,5);
   last_pc_ = pc_;
   ASSERT(RelocInfo::IsCodeTarget(rmode));
   EMIT(0xE8);
@@ -1571,7 +1692,7 @@
 
 
 void Assembler::jmp(Label* L) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5);
   last_pc_ = pc_;
   if (L->is_bound()) {
     const int short_size = 2;
@@ -1596,7 +1717,7 @@
 
 
 void Assembler::jmp(byte* entry, RelocInfo::Mode rmode) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5);
   last_pc_ = pc_;
   ASSERT(!RelocInfo::IsCodeTarget(rmode));
   EMIT(0xE9);
@@ -1605,7 +1726,8 @@
 
 
 void Assembler::jmp(const Operand& adr) {
-  EnsureSpace ensure_space(this);
+  before_branch(adr,4+L(adr));
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xFF);
   emit_operand(esp, adr);
@@ -1613,7 +1735,7 @@
 
 
 void Assembler::jmp(Handle<Code> code, RelocInfo::Mode rmode) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5);
   last_pc_ = pc_;
   ASSERT(RelocInfo::IsCodeTarget(rmode));
   EMIT(0xE9);
@@ -1623,7 +1745,9 @@
 
 
 void Assembler::j(Condition cc, Label* L, Hint hint) {
-  EnsureSpace ensure_space(this);
+  int instsize= L->is_bound()&&is_int8(L->pos()-pc_offset()-2) ? 2 : 6;
+  instsize += FLAG_emit_branch_hints && hint != no_hint;
+  EnsureSpace ensure_space(this, instsize);
   last_pc_ = pc_;
   ASSERT(0 <= cc && cc < 16);
   if (FLAG_emit_branch_hints && hint != no_hint) EMIT(hint);
@@ -1654,7 +1778,7 @@
 
 
 void Assembler::j(Condition cc, byte* entry, RelocInfo::Mode rmode, Hint hint) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 6);
   last_pc_ = pc_;
   ASSERT((0 <= cc) && (cc < 16));
   if (FLAG_emit_branch_hints && hint != no_hint) EMIT(hint);
@@ -1666,7 +1790,7 @@
 
 
 void Assembler::j(Condition cc, Handle<Code> code, Hint hint) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 6);
   last_pc_ = pc_;
   if (FLAG_emit_branch_hints && hint != no_hint) EMIT(hint);
   // 0000 1111 1000 tttn #32-bit disp
@@ -1693,7 +1817,7 @@
 
 
 void Assembler::fld1() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD9);
   EMIT(0xE8);
@@ -1701,7 +1825,7 @@
 
 
 void Assembler::fldpi() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD9);
   EMIT(0xEB);
@@ -1709,7 +1833,7 @@
 
 
 void Assembler::fldz() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD9);
   EMIT(0xEE);
@@ -1717,7 +1841,7 @@
 
 
 void Assembler::fld_s(const Operand& adr) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xD9);
   emit_operand(eax, adr);
@@ -1725,7 +1849,7 @@
 
 
 void Assembler::fld_d(const Operand& adr) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xDD);
   emit_operand(eax, adr);
@@ -1733,7 +1857,7 @@
 
 
 void Assembler::fstp_s(const Operand& adr) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xD9);
   emit_operand(ebx, adr);
@@ -1741,7 +1865,7 @@
 
 
 void Assembler::fstp_d(const Operand& adr) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xDD);
   emit_operand(ebx, adr);
@@ -1749,7 +1873,7 @@
 
 
 void Assembler::fst_d(const Operand& adr) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xDD);
   emit_operand(edx, adr);
@@ -1757,7 +1881,7 @@
 
 
 void Assembler::fild_s(const Operand& adr) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xDB);
   emit_operand(eax, adr);
@@ -1765,7 +1889,7 @@
 
 
 void Assembler::fild_d(const Operand& adr) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xDF);
   emit_operand(ebp, adr);
@@ -1773,7 +1897,7 @@
 
 
 void Assembler::fistp_s(const Operand& adr) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xDB);
   emit_operand(ebx, adr);
@@ -1782,7 +1906,7 @@
 
 void Assembler::fisttp_s(const Operand& adr) {
   ASSERT(CpuFeatures::IsEnabled(SSE3));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xDB);
   emit_operand(ecx, adr);
@@ -1791,7 +1915,7 @@
 
 void Assembler::fisttp_d(const Operand& adr) {
   ASSERT(CpuFeatures::IsEnabled(SSE3));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xDD);
   emit_operand(ecx, adr);
@@ -1799,7 +1923,7 @@
 
 
 void Assembler::fist_s(const Operand& adr) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xDB);
   emit_operand(edx, adr);
@@ -1807,7 +1931,7 @@
 
 
 void Assembler::fistp_d(const Operand& adr) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1+L(adr));
   last_pc_ = pc_;
   EMIT(0xDF);
   emit_operand(edi, adr);
@@ -1815,7 +1939,7 @@
 
 
 void Assembler::fabs() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD9);
   EMIT(0xE1);
@@ -1823,7 +1947,7 @@
 
 
 void Assembler::fchs() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD9);
   EMIT(0xE0);
@@ -1831,7 +1955,7 @@
 
 
 void Assembler::fcos() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD9);
   EMIT(0xFF);
@@ -1839,7 +1963,7 @@
 
 
 void Assembler::fsin() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD9);
   EMIT(0xFE);
@@ -1941,7 +2065,7 @@
 
 
 void Assembler::fincstp() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD9);
   EMIT(0xF7);
@@ -1949,14 +2073,14 @@
 
 
 void Assembler::ffree(int i) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   emit_farith(0xDD, 0xC0, i);
 }
 
 
 void Assembler::ftst() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD9);
   EMIT(0xE4);
@@ -1971,7 +2095,7 @@
 
 
 void Assembler::fucompp() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xDA);
   EMIT(0xE9);
@@ -1979,7 +2103,7 @@
 
 
 void Assembler::fucomi(int i) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xDB);
   EMIT(0xE8 + i);
@@ -1987,7 +2111,7 @@
 
 
 void Assembler::fucomip() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xDF);
   EMIT(0xE9);
@@ -1995,7 +2119,7 @@
 
 
 void Assembler::fcompp() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xDE);
   EMIT(0xD9);
@@ -2003,7 +2127,7 @@
 
 
 void Assembler::fnstsw_ax() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xDF);
   EMIT(0xE0);
@@ -2011,14 +2135,14 @@
 
 
 void Assembler::fwait() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x9B);
 }
 
 
 void Assembler::frndint() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xD9);
   EMIT(0xFC);
@@ -2026,7 +2150,7 @@
 
 
 void Assembler::fnclex() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2);
   last_pc_ = pc_;
   EMIT(0xDB);
   EMIT(0xE2);
@@ -2034,7 +2158,7 @@
 
 
 void Assembler::sahf() {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 1);
   last_pc_ = pc_;
   EMIT(0x9E);
 }
@@ -2042,7 +2166,7 @@
 
 void Assembler::setcc(Condition cc, Register reg) {
   ASSERT(reg.is_byte_register());
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3);
   last_pc_ = pc_;
   EMIT(0x0F);
   EMIT(0x90 | cc);
@@ -2052,7 +2176,7 @@
 
 void Assembler::cvttss2si(Register dst, const Operand& src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3+L(src));
   last_pc_ = pc_;
   EMIT(0xF3);
   EMIT(0x0F);
@@ -2063,7 +2187,7 @@
 
 void Assembler::cvttsd2si(Register dst, const Operand& src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3+L(src));
   last_pc_ = pc_;
   EMIT(0xF2);
   EMIT(0x0F);
@@ -2074,7 +2198,7 @@
 
 void Assembler::cvtsi2sd(XMMRegister dst, const Operand& src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3+L(src));
   last_pc_ = pc_;
   EMIT(0xF2);
   EMIT(0x0F);
@@ -2085,7 +2209,7 @@
 
 void Assembler::cvtss2sd(XMMRegister dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4);
   last_pc_ = pc_;
   EMIT(0xF3);
   EMIT(0x0F);
@@ -2096,7 +2220,7 @@
 
 void Assembler::addsd(XMMRegister dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4);
   last_pc_ = pc_;
   EMIT(0xF2);
   EMIT(0x0F);
@@ -2107,7 +2231,7 @@
 
 void Assembler::mulsd(XMMRegister dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4);
   last_pc_ = pc_;
   EMIT(0xF2);
   EMIT(0x0F);
@@ -2118,7 +2242,7 @@
 
 void Assembler::subsd(XMMRegister dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4);
   last_pc_ = pc_;
   EMIT(0xF2);
   EMIT(0x0F);
@@ -2129,7 +2253,7 @@
 
 void Assembler::divsd(XMMRegister dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4);
   last_pc_ = pc_;
   EMIT(0xF2);
   EMIT(0x0F);
@@ -2140,7 +2264,7 @@
 
 void Assembler::xorpd(XMMRegister dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4);
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x0F);
@@ -2150,7 +2274,7 @@
 
 
 void Assembler::sqrtsd(XMMRegister dst, XMMRegister src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4);
   last_pc_ = pc_;
   EMIT(0xF2);
   EMIT(0x0F);
@@ -2161,7 +2285,7 @@
 
 void Assembler::ucomisd(XMMRegister dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4);
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x0F);
@@ -2172,7 +2296,7 @@
 
 void Assembler::movmskpd(Register dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4);
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x0F);
@@ -2183,7 +2307,7 @@
 
 void Assembler::movdqa(const Operand& dst, XMMRegister src ) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3+L(dst));
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x0F);
@@ -2194,7 +2318,7 @@
 
 void Assembler::movdqa(XMMRegister dst, const Operand& src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3+L(src));
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x0F);
@@ -2205,7 +2329,7 @@
 
 void Assembler::movdqu(const Operand& dst, XMMRegister src ) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3+L(dst));
   last_pc_ = pc_;
   EMIT(0xF3);
   EMIT(0x0F);
@@ -2216,7 +2340,7 @@
 
 void Assembler::movdqu(XMMRegister dst, const Operand& src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3+L(src));
   last_pc_ = pc_;
   EMIT(0xF3);
   EMIT(0x0F);
@@ -2227,7 +2351,7 @@
 
 void Assembler::movntdqa(XMMRegister dst, const Operand& src) {
   ASSERT(CpuFeatures::IsEnabled(SSE4_1));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4+L(src));
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x0F);
@@ -2239,7 +2363,7 @@
 
 void Assembler::movntdq(const Operand& dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3+L(dst));
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x0F);
@@ -2250,7 +2374,7 @@
 
 void Assembler::prefetch(const Operand& src, int level) {
   ASSERT(is_uint2(level));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 2+L(src));
   last_pc_ = pc_;
   EMIT(0x0F);
   EMIT(0x18);
@@ -2260,14 +2384,14 @@
 
 
 void Assembler::movdbl(XMMRegister dst, const Operand& src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this,3+L(src));
   last_pc_ = pc_;
   movsd(dst, src);
 }
 
 
 void Assembler::movdbl(const Operand& dst, XMMRegister src) {
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this,3+L(dst));
   last_pc_ = pc_;
   movsd(dst, src);
 }
@@ -2275,7 +2399,7 @@
 
 void Assembler::movsd(const Operand& dst, XMMRegister src ) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3+L(dst));
   last_pc_ = pc_;
   EMIT(0xF2);  // double
   EMIT(0x0F);
@@ -2286,7 +2410,7 @@
 
 void Assembler::movsd(XMMRegister dst, const Operand& src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3+L(src));
   last_pc_ = pc_;
   EMIT(0xF2);  // double
   EMIT(0x0F);
@@ -2296,7 +2420,7 @@
 
 void Assembler::movsd(XMMRegister dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4);
   last_pc_ = pc_;
   EMIT(0xF2);
   EMIT(0x0F);
@@ -2307,7 +2431,7 @@
 
 void Assembler::movd(XMMRegister dst, const Operand& src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 3+L(src));
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x0F);
@@ -2318,7 +2442,7 @@
 
 void Assembler::pxor(XMMRegister dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 4);
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x0F);
@@ -2329,7 +2453,7 @@
 
 void Assembler::ptest(XMMRegister dst, XMMRegister src) {
   ASSERT(CpuFeatures::IsEnabled(SSE2));
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 5);
   last_pc_ = pc_;
   EMIT(0x66);
   EMIT(0x0F);
@@ -2361,21 +2485,21 @@
 
 void Assembler::RecordJSReturn() {
   WriteRecordedPositions();
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 0);
   RecordRelocInfo(RelocInfo::JS_RETURN);
 }
 
 
 void Assembler::RecordDebugBreakSlot() {
   WriteRecordedPositions();
-  EnsureSpace ensure_space(this);
+  EnsureSpace ensure_space(this, 0);
   RecordRelocInfo(RelocInfo::DEBUG_BREAK_SLOT);
 }
 
 
 void Assembler::RecordComment(const char* msg) {
   if (FLAG_debug_code) {
-    EnsureSpace ensure_space(this);
+    EnsureSpace ensure_space(this, 0);
     RecordRelocInfo(RelocInfo::COMMENT, reinterpret_cast<intptr_t>(msg));
   }
 }
@@ -2401,7 +2525,7 @@
   // Write the statement position if it is different from what was written last
   // time.
   if (current_statement_position_ != written_statement_position_) {
-    EnsureSpace ensure_space(this);
+    EnsureSpace ensure_space(this, 0);
     RecordRelocInfo(RelocInfo::STATEMENT_POSITION, current_statement_position_);
     written_statement_position_ = current_statement_position_;
     written = true;
@@ -2411,7 +2535,7 @@
   // also different from the written statement position.
   if (current_position_ != written_position_ &&
       current_position_ != written_statement_position_) {
-    EnsureSpace ensure_space(this);
+    EnsureSpace ensure_space(this, 0);
     RecordRelocInfo(RelocInfo::POSITION, current_position_);
     written_position_ = current_position_;
     written = true;
@@ -2447,7 +2571,8 @@
 
   // Clear the buffer in debug mode. Use 'int3' instructions to make
   // sure to get into problems if we ever run uninitialized code.
-#ifdef DEBUG
+#if defined(DEBUG) || defined(NACL)
+  //TODO(jansel): why does nacl need this?
   memset(desc.buffer, 0xCC, desc.buffer_size);
 #endif
 
@@ -2552,7 +2677,154 @@
   emit(data, reloc_info);
 }
 
+void Assembler::before(int maxsize) {
+#ifdef NACL
+  int left = NACL_CHUNK - (pc_offset()&(NACL_CHUNK-1));
+  if(left < maxsize) {
+    nops(left);
+  }
+#endif
+}
 
+void Assembler::before_call(int n) {
+#if defined(NACL)
+  int left = NACL_CHUNK - (pc_offset()&(NACL_CHUNK-1));
+  if(left >= n) {
+    nops(left-n);
+  }else{
+    //scroll to next chunk and start over
+    nops(left);
+    before_call(n);
+  }
+#endif
+}
+
+void Assembler::before_branch(const Operand& op, int maxsize) {
+#if defined(NACL)
+  //hack since cant convert op to reg
+  Register r;
+  for(int i=0; i<8; ++i){
+    r.code_ = i;
+    if(op.is_reg(r)){
+#if defined(DEBUG) && 0
+      //verify target is aligned 
+      Label ok;
+      push(r);
+      and_(r, NACL_CHUNK-1);
+      j(zero, &ok);
+      int3();
+      bind(&ok);
+      pop(r);
+#endif
+      if(maxsize>0){
+        before_call(maxsize);
+      }
+      //required by nacl:
+      and_(r, ~(NACL_CHUNK-1));
+      return;
+    }
+  }
+#endif
+}
+
+void Assembler::before_bind(Label* L) {
+#ifdef NACL
+#if 0 //turns out this is slower
+
+  if(L->is_linked()) {
+    if (disp_at(L).type() == Displacement::CODE_RELATIVE) {
+      //code relative 
+      Align(NACL_CHUNK);
+      return;
+    } else {
+      //pc relative
+      if(pc_offset()>>5 != L->pos()>>5) {
+        //diff bundles
+        Align(NACL_CHUNK);
+        return;
+      }
+    }
+  } else { 
+    Align(NACL_CHUNK);
+    return;
+  }
+
+#else
+  Align(NACL_CHUNK);
+#endif
+#endif
+}
+
+#ifdef NACL
+int Assembler::push_aligned_rv(int space) {
+  int bundle = pc_offset() & ~(NACL_CHUNK-1);
+  int offset = pc_offset() &  (NACL_CHUNK-1);
+  int retaddr;
+  if(offset <= NACL_CHUNK-5-space) {
+    //will fit in this bundle
+    retaddr = bundle + NACL_CHUNK;
+  }else{
+    //will spill to next bundle
+    retaddr = bundle + NACL_CHUNK + NACL_CHUNK;
+  }
+  { EnsureSpace ensure_space(this,5);
+    push(buffer_+retaddr, RelocInfo::INTERNAL_REFERENCE);
+  }
+  return retaddr;
+}
+#endif
+  
+void Assembler::nops(int n) {
+  ASSERT(n>=0 && n<32);
+  EnsureSpace ensure_space(this, n);
+  last_pc_ = NULL;
+
+  if(n>20) {
+    n -= 2;
+    //2 byte jmp
+    EMIT(0xEB);
+    EMIT(n);
+  }
+  static const uint8_t opt_nop01[] = {0x90, 0};
+  static const uint8_t opt_nop02[] = {0x66, 0x90, 0};
+  static const uint8_t opt_nop03[] = {0x66, 0x87, 0xc9, 0};
+  static const uint8_t opt_nop04[] = {0x66, 0x90, 0x66, 0x90, 0};
+  static const uint8_t opt_nop05[] = {0x90, 0x8d, 0x7c, 0x27, 0x00, 0};
+  static const uint8_t opt_nop06[] = {0x66, 0x90, 0x8d, 0x76, 0x00, 0x90, 0};
+  static const uint8_t opt_nop07[] = {0x8d, 0x76, 0x00, 0x8d, 0x7c, 0x27, 0x00, 0};
+  static const uint8_t opt_nop08[] = {0x66, 0x90, 0x89, 0xf6, 0x8d, 0x7c, 0x27, 0x00, 0};
+  static const uint8_t opt_nop09[] = {0x8d, 0x76, 0x00, 0x89, 0xf6, 0x8d, 0x74, 0x26, 0x00, 0};
+  static const uint8_t opt_nop10[] = {0x8d, 0x74, 0x26, 0x00, 0x66, 0x90, 0x8d, 0x74, 0x26, 0x00, 0};
+  static const uint8_t opt_nop11[] = {0x89, 0xf6, 0x90, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0x7c, 0x27, 0x00, 0};
+  static const uint8_t opt_nop12[] = {0x66, 0x90, 0x8d, 0x76, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop13[] = {0x90, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0x74, 0x26, 0x00, 0};
+  static const uint8_t opt_nop14[] = {0x66, 0x90, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0x74, 0x26, 0x00, 0};
+  static const uint8_t opt_nop15[] = {0x89, 0xf6, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop16[] = {0x89, 0xf6, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbf, 0x00, 0x00, 0x00, 0x00, 0x66, 0x90, 0};
+  static const uint8_t opt_nop17[] = {0x89, 0xf6, 0x8d, 0x7c, 0x27, 0x00, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop18[] = {0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0x74, 0x26, 0x00, 0};
+  static const uint8_t opt_nop19[] = {0x8d, 0x7c, 0x27, 0x00, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x66, 0x90, 0};
+  static const uint8_t opt_nop20[] = {0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop21[] = {0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x76, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x7c, 0x27, 0x00, 0};
+  static const uint8_t opt_nop22[] = {0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x66, 0x90, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop23[] = {0x0f, 0x1f, 0x00, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbf, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop24[] = {0x90, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbf, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop25[] = {0x87, 0xc9, 0x8d, 0x76, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop26[] = {0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop27[] = {0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x66, 0x87, 0xd2, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop28[] = {0x8d, 0x76, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x76, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x0f, 0x1f, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop29[] = {0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop30[] = {0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x76, 0x00, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop31[] = {0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+
+  if(n>0) {
+   static const uint8_t* opt_nops[] = {opt_nop01, opt_nop02, opt_nop03, opt_nop04, opt_nop05, opt_nop06, opt_nop07, opt_nop08, opt_nop09, opt_nop10, opt_nop11, opt_nop12, opt_nop13, opt_nop14, opt_nop15, opt_nop16, opt_nop17, opt_nop18, opt_nop19, opt_nop20, opt_nop21, opt_nop22, opt_nop23, opt_nop24, opt_nop25, opt_nop26, opt_nop27, opt_nop28, opt_nop29, opt_nop30, opt_nop31, 0};
+    memcpy(pc_, opt_nops[n-1], n);
+    pc_+=n;
+  }
+
+}
+
 void Assembler::RecordRelocInfo(RelocInfo::Mode rmode, intptr_t data) {
   ASSERT(rmode != RelocInfo::NONE);
   // Don't record external references unless the heap will be serialized.
Index: src/ia32/stub-cache-ia32.cc
===================================================================
--- src/ia32/stub-cache-ia32.cc	(revision 4925)
+++ src/ia32/stub-cache-ia32.cc	(working copy)
@@ -66,6 +66,7 @@
 
     // Jump to the first instruction in the code stub.
     __ add(Operand(extra), Immediate(Code::kHeaderSize - kHeapObjectTag));
+    NACL_PATCH_INSTRUCTION_START(extra);
     __ jmp(Operand(extra));
 
     __ bind(&miss);
@@ -92,6 +93,7 @@
 
     // Jump to the first instruction in the code stub.
     __ add(Operand(offset), Immediate(Code::kHeaderSize - kHeapObjectTag));
+    NACL_PATCH_INSTRUCTION_START(offset);
     __ jmp(Operand(offset));
 
     // Pop at miss.
@@ -1039,6 +1041,7 @@
 
   // Do a tail-call of the compiled function.
   __ lea(ecx, FieldOperand(eax, Code::kHeaderSize));
+  NACL_PATCH_INSTRUCTION_START(ecx);
   __ jmp(Operand(ecx));
 
   return GetCodeWithFlags(flags, "LazyCompileStub");
Index: src/ia32/assembler-ia32.h
===================================================================
--- src/ia32/assembler-ia32.h	(revision 4925)
+++ src/ia32/assembler-ia32.h	(working copy)
@@ -37,6 +37,14 @@
 #ifndef V8_IA32_ASSEMBLER_IA32_H_
 #define V8_IA32_ASSEMBLER_IA32_H_
 
+
+//NACL_CHANGE: patch output code to use separate code location
+#ifdef NACL
+#define NACL_CHUNK 32
+#endif
+#define NACL_PATCH_INSTRUCTION_START(reg) \
+  __ mov(reg, Operand(reg, Code::kExternalInstructionsOffset-Code::kHeaderSize))
+
 #include "serialize.h"
 
 namespace v8 {
@@ -421,7 +429,11 @@
   // (There is a 15 byte limit on ia32 instruction length that rules out some
   // otherwise valid instructions.)
   // This allows for a single, fast space check per instruction.
+#ifdef NACL
+  static const int kGap = 64;
+#else
   static const int kGap = 32;
+#endif
 
  public:
   // Create an assembler. Instructions and relocation information are emitted
@@ -447,7 +459,7 @@
 
   // Read/Modify the code target in the branch/call instruction at pc.
   inline static Address target_address_at(Address pc);
-  inline static void set_target_address_at(Address pc, Address target);
+  inline static void set_target_address_at(Address pc, Address target, intptr_t extraoffset = 0);
 
   // This sets the branch destination (which is in the instruction on x86).
   // This is for calls and branches within generated code.
@@ -478,7 +490,11 @@
   static const int kPatchDebugBreakSlotAddressOffset = 1;  // JMP imm32.
 
   static const int kCallInstructionLength = 5;
+#ifdef NACL
+  static const int kJSReturnSequenceLength = NACL_CHUNK;
+#else
   static const int kJSReturnSequenceLength = 6;
+#endif
 
   // The debug break slot must be able to contain a call instruction.
   static const int kDebugBreakSlotLength = kCallInstructionLength;
@@ -521,6 +537,7 @@
   void push(Register src);
   void push(const Operand& src);
   void push(Label* label, RelocInfo::Mode relocation_mode);
+  void push(byte* x, RelocInfo::Mode rmode);
 
   void pop(Register dst);
   void pop(const Operand& dst);
@@ -851,6 +868,22 @@
 
   static bool IsNop(Address addr) { return *addr == 0x90; }
 
+  //NACL_CHANGE: call before each instruction
+  void before(int maxsize);
+  
+  //NACL_CHANGE: call before each branch instruction
+  void before_branch(const Operand& op, int maxsize=-1);
+  
+  //NACL_CHANGE: call before each call instruction
+  void before_call(int size);
+  
+  //NACL_CHANGE: call before bind
+  void before_bind(Label* l);
+
+  int push_aligned_rv(int space);
+  
+  void nops(int n);
+
   // Avoid overflows for displacements etc.
   static const int kMaximalBufferSize = 512*MB;
   static const int kMinimalBufferSize = 4*KB;
@@ -943,10 +976,19 @@
 // checks that we did not generate too much.
 class EnsureSpace BASE_EMBEDDED {
  public:
-  explicit EnsureSpace(Assembler* assembler) : assembler_(assembler) {
+  explicit EnsureSpace(Assembler* assembler, int maxinst = 15) : assembler_(assembler) {
+#ifndef NACL
     if (assembler_->overflow()) assembler_->GrowBuffer();
+    USE(maxinst);
+#else
+    if(maxinst > 1) {
+      assembler_->before(maxinst); 
+    }
+    if (assembler_->overflow()) assembler_->GrowBuffer();
+#endif
 #ifdef DEBUG
     space_before_ = assembler_->available_space();
+    max_pc_offset_after_ = assembler_->pc_offset() + maxinst;
 #endif
   }
 
@@ -954,6 +996,9 @@
   ~EnsureSpace() {
     int bytes_generated = space_before_ - assembler_->available_space();
     ASSERT(bytes_generated < assembler_->kGap);
+    ASSERT(assembler_->pc_offset() <= max_pc_offset_after_);
+    //asserts the quality of our size predictions by bounding waste:
+    //ASSERT(max_pc_offset_after_ - assembler_->pc_offset() <= 3);
   }
 #endif
 
@@ -961,6 +1006,7 @@
   Assembler* assembler_;
 #ifdef DEBUG
   int space_before_;
+  int max_pc_offset_after_;
 #endif
 };
 
Index: src/ia32/debug-ia32.cc
===================================================================
--- src/ia32/debug-ia32.cc	(revision 4925)
+++ src/ia32/debug-ia32.cc	(working copy)
@@ -141,7 +141,11 @@
   // overwritten by the address of DebugBreakXXX.
   ExternalReference after_break_target =
       ExternalReference(Debug_Address::AfterBreakTarget());
+#ifdef NACL
+  __ hlt(); // cant support this :(
+#else
   __ jmp(Operand::StaticVariable(after_break_target));
+#endif
 }
 
 
@@ -280,7 +284,9 @@
   __ mov(edx, FieldOperand(edi, JSFunction::kSharedFunctionInfoOffset));
   __ mov(edx, FieldOperand(edx, SharedFunctionInfo::kCodeOffset));
   __ lea(edx, FieldOperand(edx, Code::kHeaderSize));
+  NACL_PATCH_INSTRUCTION_START(edx);
 
+
   // Re-run JSFunction, edi is function, esi is context.
   __ jmp(Operand(edx));
 }
Index: src/ia32/cpu-ia32.cc
===================================================================
--- src/ia32/cpu-ia32.cc	(revision 4925)
+++ src/ia32/cpu-ia32.cc	(working copy)
@@ -73,6 +73,8 @@
   // instead
   // __asm { int 3 }
   __debugbreak();
+#elif defined(NACL)
+  asm("hlt");
 #else
   asm("int $3");
 #endif
Index: src/ia32/builtins-ia32.cc
===================================================================
--- src/ia32/builtins-ia32.cc	(revision 4925)
+++ src/ia32/builtins-ia32.cc	(working copy)
@@ -90,6 +90,7 @@
   __ mov(ebx, FieldOperand(edi, JSFunction::kSharedFunctionInfoOffset));
   __ mov(ebx, FieldOperand(ebx, SharedFunctionInfo::kConstructStubOffset));
   __ lea(ebx, FieldOperand(ebx, Code::kHeaderSize));
+  NACL_PATCH_INSTRUCTION_START(ebx);
   __ jmp(Operand(ebx));
 
   // edi: called object
@@ -550,6 +551,7 @@
   __ SmiUntag(ebx);
   __ mov(edx, FieldOperand(edx, SharedFunctionInfo::kCodeOffset));
   __ lea(edx, FieldOperand(edx, Code::kHeaderSize));
+  NACL_PATCH_INSTRUCTION_START(edx);
   __ cmp(eax, Operand(ebx));
   __ j(not_equal, Handle<Code>(builtin(ArgumentsAdaptorTrampoline)));
 
Index: src/ia32/ic-ia32.cc
===================================================================
--- src/ia32/ic-ia32.cc	(revision 4925)
+++ src/ia32/ic-ia32.cc	(working copy)
@@ -34,6 +34,9 @@
 #include "runtime.h"
 #include "stub-cache.h"
 #include "utils.h"
+#ifdef NACL
+#include "naclcode.h"
+#endif
 
 namespace v8 {
 namespace internal {
@@ -45,72 +48,96 @@
 #define __ ACCESS_MASM(masm)
 
 
+static void GenerateGlobalInstanceTypeCheck(MacroAssembler* masm,
+                                            Register type,
+                                            Label* global_object) {
+  // Register usage:
+  //   type: holds the receiver instance type on entry.
+  __ cmp(type, JS_GLOBAL_OBJECT_TYPE);
+  __ j(equal, global_object, not_taken);
+  __ cmp(type, JS_BUILTINS_OBJECT_TYPE);
+  __ j(equal, global_object, not_taken);
+  __ cmp(type, JS_GLOBAL_PROXY_TYPE);
+  __ j(equal, global_object, not_taken);
+}
+
+
+// Generated code falls through if the receiver is a regular non-global
+// JS object with slow properties and no interceptors.
+static void GenerateDictionaryLoadReceiverCheck(MacroAssembler* masm,
+                                                Register receiver,
+                                                Register r0,
+                                                Register r1,
+                                                Label* miss) {
+  // Register usage:
+  //   receiver: holds the receiver on entry and is unchanged.
+  //   r0: used to hold receiver instance type.
+  //       Holds the property dictionary on fall through.
+  //   r1: used to hold receivers map.
+
+  // Check that the receiver isn't a smi.
+  __ test(receiver, Immediate(kSmiTagMask));
+  __ j(zero, miss, not_taken);
+
+  // Check that the receiver is a valid JS object.
+  __ mov(r1, FieldOperand(receiver, HeapObject::kMapOffset));
+  __ movzx_b(r0, FieldOperand(r1, Map::kInstanceTypeOffset));
+  __ cmp(r0, FIRST_JS_OBJECT_TYPE);
+  __ j(below, miss, not_taken);
+
+  // If this assert fails, we have to check upper bound too.
+  ASSERT(LAST_TYPE == JS_FUNCTION_TYPE);
+
+  GenerateGlobalInstanceTypeCheck(masm, r0, miss);
+
+  // Check for non-global object that requires access check.
+  __ test_b(FieldOperand(r1, Map::kBitFieldOffset),
+            (1 << Map::kIsAccessCheckNeeded) |
+            (1 << Map::kHasNamedInterceptor));
+  __ j(not_zero, miss, not_taken);
+
+  __ mov(r0, FieldOperand(receiver, JSObject::kPropertiesOffset));
+  __ CheckMap(r0, Factory::hash_table_map(), miss, true);
+}
+
+
 // Helper function used to load a property from a dictionary backing storage.
 // This function may return false negatives, so miss_label
 // must always call a backup property load that is complete.
-// This function is safe to call if the receiver has fast properties,
-// or if name is not a symbol, and will jump to the miss_label in that case.
+// This function is safe to call if name is not a symbol, and will jump to
+// the miss_label in that case.
+// The generated code assumes that the receiver has slow properties,
+// is not a global object and does not have interceptors.
 static void GenerateDictionaryLoad(MacroAssembler* masm,
                                    Label* miss_label,
-                                   Register receiver,
+                                   Register elements,
                                    Register name,
                                    Register r0,
                                    Register r1,
-                                   Register r2,
-                                   Register result,
-                                   DictionaryCheck check_dictionary) {
+                                   Register result) {
   // Register use:
   //
-  // name - holds the name of the property and is unchanged.
-  // receiver - holds the receiver and is unchanged.
+  // elements - holds the property dictionary on entry and is unchanged.
+  //
+  // name - holds the name of the property on entry and is unchanged.
+  //
   // Scratch registers:
-  // r0   - used to hold the property dictionary.
   //
-  // r1   - used for the index into the property dictionary
+  // r0   - used for the index into the property dictionary
   //
-  // r2   - used to hold the capacity of the property dictionary.
+  // r1   - used to hold the capacity of the property dictionary.
   //
   // result - holds the result on exit.
 
   Label done;
 
-  // Check for the absence of an interceptor.
-  // Load the map into r0.
-  __ mov(r0, FieldOperand(receiver, JSObject::kMapOffset));
-
-  // Bail out if the receiver has a named interceptor.
-  __ test(FieldOperand(r0, Map::kBitFieldOffset),
-          Immediate(1 << Map::kHasNamedInterceptor));
-  __ j(not_zero, miss_label, not_taken);
-
-  // Bail out if we have a JS global proxy object.
-  __ movzx_b(r0, FieldOperand(r0, Map::kInstanceTypeOffset));
-  __ cmp(r0, JS_GLOBAL_PROXY_TYPE);
-  __ j(equal, miss_label, not_taken);
-
-  // Possible work-around for http://crbug.com/16276.
-  __ cmp(r0, JS_GLOBAL_OBJECT_TYPE);
-  __ j(equal, miss_label, not_taken);
-  __ cmp(r0, JS_BUILTINS_OBJECT_TYPE);
-  __ j(equal, miss_label, not_taken);
-
-  // Load properties array.
-  __ mov(r0, FieldOperand(receiver, JSObject::kPropertiesOffset));
-
-  // Check that the properties array is a dictionary.
-  if (check_dictionary == CHECK_DICTIONARY) {
-    __ cmp(FieldOperand(r0, HeapObject::kMapOffset),
-           Immediate(Factory::hash_table_map()));
-    __ j(not_equal, miss_label);
-  }
-
   // Compute the capacity mask.
   const int kCapacityOffset =
       StringDictionary::kHeaderSize +
       StringDictionary::kCapacityIndex * kPointerSize;
-  __ mov(r2, FieldOperand(r0, kCapacityOffset));
-  __ shr(r2, kSmiTagSize);  // convert smi to int
-  __ dec(r2);
+  __ mov(r1, FieldOperand(elements, kCapacityOffset));
+  __ shr(r1, kSmiTagSize);  // convert smi to int
+  __ dec(r1);
 
   // Generate an unrolled loop that performs a few probes before
   // giving up. Measurements done on Gmail indicate that 2 probes
@@ -121,20 +148,20 @@
       StringDictionary::kElementsStartIndex * kPointerSize;
   for (int i = 0; i < kProbes; i++) {
     // Compute the masked index: (hash + i + i * i) & mask.
-    __ mov(r1, FieldOperand(name, String::kHashFieldOffset));
-    __ shr(r1, String::kHashShift);
+    __ mov(r0, FieldOperand(name, String::kHashFieldOffset));
+    __ shr(r0, String::kHashShift);
     if (i > 0) {
-      __ add(Operand(r1), Immediate(StringDictionary::GetProbeOffset(i)));
+      __ add(Operand(r0), Immediate(StringDictionary::GetProbeOffset(i)));
     }
-    __ and_(r1, Operand(r2));
+    __ and_(r0, Operand(r1));
 
     // Scale the index by multiplying by the entry size.
     ASSERT(StringDictionary::kEntrySize == 3);
-    __ lea(r1, Operand(r1, r1, times_2, 0));  // r1 = r1 * 3
+    __ lea(r0, Operand(r0, r0, times_2, 0));  // r0 = r0 * 3
 
     // Check if the key is identical to the name.
-    __ cmp(name,
-           Operand(r0, r1, times_4, kElementsStartOffset - kHeapObjectTag));
+    __ cmp(name, Operand(elements, r0, times_4,
+                         kElementsStartOffset - kHeapObjectTag));
     if (i != kProbes - 1) {
       __ j(equal, &done, taken);
     } else {
@@ -145,13 +172,13 @@
   // Check that the value is a normal property.
   __ bind(&done);
   const int kDetailsOffset = kElementsStartOffset + 2 * kPointerSize;
-  __ test(Operand(r0, r1, times_4, kDetailsOffset - kHeapObjectTag),
+  __ test(Operand(elements, r0, times_4, kDetailsOffset - kHeapObjectTag),
           Immediate(PropertyDetails::TypeField::mask() << kSmiTagSize));
   __ j(not_zero, miss_label, not_taken);
 
   // Get the value at the masked, scaled index.
   const int kValueOffset = kElementsStartOffset + kPointerSize;
-  __ mov(result, Operand(r0, r1, times_4, kValueOffset - kHeapObjectTag));
+  __ mov(result, Operand(elements, r0, times_4, kValueOffset - kHeapObjectTag));
 }
 
 
@@ -257,7 +284,11 @@
 // The offset from the inlined patch site to the start of the
 // inlined load instruction.  It is 7 bytes (test eax, imm) plus
 // 6 bytes (jne slow_label).
+#if defined(NACL) && NACL_CHUNK == 16
+const int LoadIC::kOffsetToLoadInstruction = 16;
+#else
 const int LoadIC::kOffsetToLoadInstruction = 13;
+#endif
 
 
 void LoadIC::GenerateArrayLength(MacroAssembler* masm) {
@@ -307,6 +338,7 @@
 static void GenerateKeyedLoadReceiverCheck(MacroAssembler* masm,
                                            Register receiver,
                                            Register map,
+                                           int interceptor_bit,
                                            Label* slow) {
   // Register use:
   //   receiver - holds the receiver and is unchanged.
@@ -322,7 +354,7 @@
 
   // Check bit field.
   __ test_b(FieldOperand(map, Map::kBitFieldOffset),
-            KeyedLoadIC::kSlowCaseBitFieldMask);
+            (1 << Map::kIsAccessCheckNeeded) | (1 << interceptor_bit));
   __ j(not_zero, slow, not_taken);
   // Check that the object is some kind of JS object EXCEPT JS Value type.
   // In the case that the object is a value-wrapper object,
@@ -432,8 +464,6 @@
   Label slow, check_string, index_smi, index_string;
   Label check_pixel_array, probe_dictionary, check_number_dictionary;
 
-  GenerateKeyedLoadReceiverCheck(masm, edx, ecx, &slow);
-
   // Check that the key is a smi.
   __ test(eax, Immediate(kSmiTagMask));
   __ j(not_zero, &check_string, not_taken);
@@ -441,6 +471,9 @@
   // Now the key is known to be a smi. This place is also jumped to from
   // where a numeric string is converted to a smi.
 
+  GenerateKeyedLoadReceiverCheck(
+      masm, edx, ecx, Map::kHasIndexedInterceptor, &slow);
+
   GenerateFastArrayLoad(masm,
                         edx,
                         eax,
@@ -503,6 +536,9 @@
   __ bind(&check_string);
   GenerateKeyStringCheck(masm, eax, ecx, ebx, &index_string, &slow);
 
+  GenerateKeyedLoadReceiverCheck(
+      masm, edx, ecx, Map::kHasNamedInterceptor, &slow);
+
   // If the receiver is a fast-case object, check the keyed lookup
   // cache. Otherwise probe the dictionary.
   __ mov(ebx, FieldOperand(edx, JSObject::kPropertiesOffset));
@@ -555,15 +591,12 @@
   // Do a quick inline probe of the receiver's dictionary, if it
   // exists.
   __ bind(&probe_dictionary);
-  GenerateDictionaryLoad(masm,
-                         &slow,
-                         edx,
-                         eax,
-                         ebx,
-                         ecx,
-                         edi,
-                         eax,
-                         DICTIONARY_CHECK_DONE);
+
+  __ mov(ecx, FieldOperand(edx, JSObject::kMapOffset));
+  __ movzx_b(ecx, FieldOperand(ecx, Map::kInstanceTypeOffset));
+  GenerateGlobalInstanceTypeCheck(masm, ecx, &slow);
+
+  GenerateDictionaryLoad(masm, &slow, ebx, eax, ecx, edi, eax);
   __ IncrementCounter(&Counters::keyed_load_generic_symbol, 1);
   __ ret(0);
 
@@ -1173,24 +1206,18 @@
 }
 
 
-static void GenerateNormalHelper(MacroAssembler* masm,
-                                 int argc,
-                                 bool is_global_object,
-                                 Label* miss) {
+static void GenerateFunctionTailCall(MacroAssembler* masm,
+                                     int argc,
+                                     Label* miss) {
   // ----------- S t a t e -------------
   //  -- ecx                 : name
-  //  -- edx                 : receiver
+  //  -- edi                 : function
   //  -- esp[0]              : return address
   //  -- esp[(argc - n) * 4] : arg[n] (zero-based)
   //  -- ...
   //  -- esp[(argc + 1) * 4] : receiver
   // -----------------------------------
 
-  // Search dictionary - put result in register edi.
-  __ mov(edi, edx);
-  GenerateDictionaryLoad(
-      masm, miss, edx, ecx, eax, edi, ebx, edi, CHECK_DICTIONARY);
-
   // Check that the result is not a smi.
   __ test(edi, Immediate(kSmiTagMask));
   __ j(zero, miss, not_taken);
@@ -1199,12 +1226,6 @@
   __ CmpObjectType(edi, JS_FUNCTION_TYPE, eax);
   __ j(not_equal, miss, not_taken);
 
-  // Patch the receiver on stack with the global proxy if necessary.
-  if (is_global_object) {
-    __ mov(edx, FieldOperand(edx, GlobalObject::kGlobalReceiverOffset));
-    __ mov(Operand(esp, (argc + 1) * kPointerSize), edx);
-  }
-
   // Invoke the function.
   ParameterCount actual(argc);
   __ InvokeFunction(edi, actual, JUMP_FUNCTION);
@@ -1219,56 +1240,18 @@
   //  -- ...
   //  -- esp[(argc + 1) * 4] : receiver
   // -----------------------------------
-  Label miss, global_object, non_global_object;
+  Label miss;
 
   // Get the receiver of the function from the stack; 1 ~ return address.
   __ mov(edx, Operand(esp, (argc + 1) * kPointerSize));
 
-  // Check that the receiver isn't a smi.
-  __ test(edx, Immediate(kSmiTagMask));
-  __ j(zero, &miss, not_taken);
+  GenerateDictionaryLoadReceiverCheck(masm, edx, eax, ebx, &miss);
 
-  // Check that the receiver is a valid JS object.
-  __ mov(ebx, FieldOperand(edx, HeapObject::kMapOffset));
-  __ movzx_b(eax, FieldOperand(ebx, Map::kInstanceTypeOffset));
-  __ cmp(eax, FIRST_JS_OBJECT_TYPE);
-  __ j(below, &miss, not_taken);
+  // eax: elements
+  // Search the dictionary placing the result in edi.
+  GenerateDictionaryLoad(masm, &miss, eax, ecx, edi, ebx, edi);
+  GenerateFunctionTailCall(masm, argc, &miss);
 
-  // If this assert fails, we have to check upper bound too.
-  ASSERT(LAST_TYPE == JS_FUNCTION_TYPE);
-
-  // Check for access to global object.
-  __ cmp(eax, JS_GLOBAL_OBJECT_TYPE);
-  __ j(equal, &global_object);
-  __ cmp(eax, JS_BUILTINS_OBJECT_TYPE);
-  __ j(not_equal, &non_global_object);
-
-  // Accessing global object: Load and invoke.
-  __ bind(&global_object);
-  // Check that the global object does not require access checks.
-  __ test_b(FieldOperand(ebx, Map::kBitFieldOffset),
-            1 << Map::kIsAccessCheckNeeded);
-  __ j(not_equal, &miss, not_taken);
-  GenerateNormalHelper(masm, argc, true, &miss);
-
-  // Accessing non-global object: Check for access to global proxy.
-  Label global_proxy, invoke;
-  __ bind(&non_global_object);
-  __ cmp(eax, JS_GLOBAL_PROXY_TYPE);
-  __ j(equal, &global_proxy, not_taken);
-  // Check that the non-global, non-global-proxy object does not
-  // require access checks.
-  __ test_b(FieldOperand(ebx, Map::kBitFieldOffset),
-            1 << Map::kIsAccessCheckNeeded);
-  __ j(not_equal, &miss, not_taken);
-  __ bind(&invoke);
-  GenerateNormalHelper(masm, argc, false, &miss);
-
-  // Global object proxy access: Check access rights.
-  __ bind(&global_proxy);
-  __ CheckAccessGlobalProxy(edx, eax, &miss);
-  __ jmp(&invoke);
-
   __ bind(&miss);
 }
 
@@ -1282,6 +1265,12 @@
   //  -- esp[(argc + 1) * 4] : receiver
   // -----------------------------------
 
+  if (id == IC::kCallIC_Miss) {
+    __ IncrementCounter(&Counters::call_miss, 1);
+  } else {
+    __ IncrementCounter(&Counters::keyed_call_miss, 1);
+  }
+
   // Get the receiver of the function from the stack; 1 ~ return address.
   __ mov(edx, Operand(esp, (argc + 1) * kPointerSize));
 
@@ -1303,25 +1292,28 @@
   __ LeaveInternalFrame();
 
   // Check if the receiver is a global object of some sort.
-  Label invoke, global;
-  __ mov(edx, Operand(esp, (argc + 1) * kPointerSize));  // receiver
-  __ test(edx, Immediate(kSmiTagMask));
-  __ j(zero, &invoke, not_taken);
-  __ mov(ebx, FieldOperand(edx, HeapObject::kMapOffset));
-  __ movzx_b(ebx, FieldOperand(ebx, Map::kInstanceTypeOffset));
-  __ cmp(ebx, JS_GLOBAL_OBJECT_TYPE);
-  __ j(equal, &global);
-  __ cmp(ebx, JS_BUILTINS_OBJECT_TYPE);
-  __ j(not_equal, &invoke);
+  // This can happen only for regular CallIC but not KeyedCallIC.
+  if (id == IC::kCallIC_Miss) {
+    Label invoke, global;
+    __ mov(edx, Operand(esp, (argc + 1) * kPointerSize));  // receiver
+    __ test(edx, Immediate(kSmiTagMask));
+    __ j(zero, &invoke, not_taken);
+    __ mov(ebx, FieldOperand(edx, HeapObject::kMapOffset));
+    __ movzx_b(ebx, FieldOperand(ebx, Map::kInstanceTypeOffset));
+    __ cmp(ebx, JS_GLOBAL_OBJECT_TYPE);
+    __ j(equal, &global);
+    __ cmp(ebx, JS_BUILTINS_OBJECT_TYPE);
+    __ j(not_equal, &invoke);
 
-  // Patch the receiver on the stack.
-  __ bind(&global);
-  __ mov(edx, FieldOperand(edx, GlobalObject::kGlobalReceiverOffset));
-  __ mov(Operand(esp, (argc + 1) * kPointerSize), edx);
+    // Patch the receiver on the stack.
+    __ bind(&global);
+    __ mov(edx, FieldOperand(edx, GlobalObject::kGlobalReceiverOffset));
+    __ mov(Operand(esp, (argc + 1) * kPointerSize), edx);
+    __ bind(&invoke);
+  }
 
   // Invoke the function.
   ParameterCount actual(argc);
-  __ bind(&invoke);
   __ InvokeFunction(edi, actual, JUMP_FUNCTION);
 }
 
@@ -1393,7 +1385,8 @@
   // Now the key is known to be a smi. This place is also jumped to from
   // where a numeric string is converted to a smi.
 
-  GenerateKeyedLoadReceiverCheck(masm, edx, eax, &slow_call);
+  GenerateKeyedLoadReceiverCheck(
+      masm, edx, eax, Map::kHasIndexedInterceptor, &slow_call);
 
   GenerateFastArrayLoad(
       masm, edx, ecx, eax, edi, &check_number_dictionary, &slow_load);
@@ -1403,16 +1396,8 @@
   // receiver in edx is not used after this point.
   // ecx: key
   // edi: function
+  GenerateFunctionTailCall(masm, argc, &slow_call);
 
-  // Check that the value in edi is a JavaScript function.
-  __ test(edi, Immediate(kSmiTagMask));
-  __ j(zero, &slow_call, not_taken);
-  __ CmpObjectType(edi, JS_FUNCTION_TYPE, eax);
-  __ j(not_equal, &slow_call, not_taken);
-  // Invoke the function.
-  ParameterCount actual(argc);
-  __ InvokeFunction(edi, actual, JUMP_FUNCTION);
-
   __ bind(&check_number_dictionary);
   // eax: elements
   // ecx: smi key
@@ -1451,15 +1436,13 @@
   // If the receiver is a regular JS object with slow properties then do
   // a quick inline probe of the receiver's dictionary.
   // Otherwise do the monomorphic cache probe.
-  GenerateKeyedLoadReceiverCheck(masm, edx, eax, &lookup_monomorphic_cache);
+  GenerateKeyedLoadReceiverCheck(
+      masm, edx, eax, Map::kHasNamedInterceptor, &lookup_monomorphic_cache);
 
   __ mov(ebx, FieldOperand(edx, JSObject::kPropertiesOffset));
-  __ cmp(FieldOperand(ebx, HeapObject::kMapOffset),
-         Immediate(Factory::hash_table_map()));
-  __ j(not_equal, &lookup_monomorphic_cache, not_taken);
+  __ CheckMap(ebx, Factory::hash_table_map(), &lookup_monomorphic_cache, true);
 
-  GenerateDictionaryLoad(
-      masm, &slow_load, edx, ecx, ebx, eax, edi, edi, DICTIONARY_CHECK_DONE);
+  GenerateDictionaryLoad(masm, &slow_load, ebx, ecx, eax, edi, edi);
   __ IncrementCounter(&Counters::keyed_call_generic_lookup_dict, 1);
   __ jmp(&do_call);
 
@@ -1539,49 +1522,15 @@
   //  -- ecx    : name
   //  -- esp[0] : return address
   // -----------------------------------
-  Label miss, probe, global;
+  Label miss;
 
-  // Check that the receiver isn't a smi.
-  __ test(eax, Immediate(kSmiTagMask));
-  __ j(zero, &miss, not_taken);
+  GenerateDictionaryLoadReceiverCheck(masm, eax, edx, ebx, &miss);
 
-  // Check that the receiver is a valid JS object.
-  __ mov(ebx, FieldOperand(eax, HeapObject::kMapOffset));
-  __ movzx_b(edx, FieldOperand(ebx, Map::kInstanceTypeOffset));
-  __ cmp(edx, FIRST_JS_OBJECT_TYPE);
-  __ j(less, &miss, not_taken);
-
-  // If this assert fails, we have to check upper bound too.
-  ASSERT(LAST_TYPE == JS_FUNCTION_TYPE);
-
-  // Check for access to global object (unlikely).
-  __ cmp(edx, JS_GLOBAL_PROXY_TYPE);
-  __ j(equal, &global, not_taken);
-
-  // Check for non-global object that requires access check.
-  __ test_b(FieldOperand(ebx, Map::kBitFieldOffset),
-            1 << Map::kIsAccessCheckNeeded);
-  __ j(not_zero, &miss, not_taken);
-
+  // edx: elements
   // Search the dictionary placing the result in eax.
-  __ bind(&probe);
-  GenerateDictionaryLoad(masm,
-                         &miss,
-                         eax,
-                         ecx,
-                         edx,
-                         edi,
-                         ebx,
-                         edi,
-                         CHECK_DICTIONARY);
-  __ mov(eax, edi);
+  GenerateDictionaryLoad(masm, &miss, edx, ecx, edi, ebx, eax);
   __ ret(0);
 
-  // Global object access: Check access rights.
-  __ bind(&global);
-  __ CheckAccessGlobalProxy(eax, edx, &miss);
-  __ jmp(&probe);
-
   // Cache miss: Jump to runtime.
   __ bind(&miss);
   GenerateMiss(masm);
@@ -1595,6 +1544,8 @@
   //  -- esp[0] : return address
   // -----------------------------------
 
+  __ IncrementCounter(&Counters::load_miss, 1);
+
   __ pop(ebx);
   __ push(eax);  // receiver
   __ push(ecx);  // name
@@ -1656,14 +1607,22 @@
   // operand-immediate compare instruction, so we add 3 to get the
   // offset to the last 4 bytes.
   Address map_address = test_instruction_address + delta + 3;
+#ifdef NACL
+  NaClCode::PatchWord(reinterpret_cast<Object**>(map_address), map);
+#else
   *(reinterpret_cast<Object**>(map_address)) = map;
+#endif
 
   // The offset is in the last 4 bytes of a six byte
   // memory-to-register move instruction, so we add 2 to get the
   // offset to the last 4 bytes.
   Address offset_address =
       test_instruction_address + delta + kOffsetToLoadInstruction + 2;
+#ifdef NACL
+  NaClCode::PatchWord(reinterpret_cast<int*>(offset_address), offset - kHeapObjectTag);
+#else
   *reinterpret_cast<int*>(offset_address) = offset - kHeapObjectTag;
+#endif
   return true;
 }
 
@@ -1685,7 +1644,11 @@
   // to the offset to get the map address.
   Address map_address = test_instruction_address + delta + 3;
   // Patch the map check.
+#ifdef NACL
+  NaClCode::PatchWord(reinterpret_cast<Object**>(map_address), map);
+#else
   *(reinterpret_cast<Object**>(map_address)) = map;
+#endif
   return true;
 }
 
@@ -1711,6 +1674,8 @@
   //  -- esp[0] : return address
   // -----------------------------------
 
+  __ IncrementCounter(&Counters::keyed_load_miss, 1);
+
   __ pop(ebx);
   __ push(edx);  // receiver
   __ push(eax);  // name
Index: src/flag-definitions.h
===================================================================
--- src/flag-definitions.h	(revision 4925)
+++ src/flag-definitions.h	(working copy)
@@ -304,19 +304,23 @@
 DEFINE_bool(enable_slow_asserts, false,
             "enable asserts that are slow to execute")
 
+#define __CODE  false
+#define __TRACE false
+
+
 // codegen-ia32.cc / codegen-arm.cc
-DEFINE_bool(trace_codegen, false,
+DEFINE_bool(trace_codegen, __CODE,
             "print name of functions for which code is generated")
-DEFINE_bool(print_source, false, "pretty print source code")
-DEFINE_bool(print_builtin_source, false,
+DEFINE_bool(print_source, __CODE, "pretty print source code")
+DEFINE_bool(print_builtin_source, __CODE,
             "pretty print source code for builtins")
 DEFINE_bool(print_ast, false, "print source AST")
 DEFINE_bool(print_builtin_ast, false, "print source AST for builtins")
 DEFINE_bool(print_json_ast, false, "print source AST as JSON")
 DEFINE_bool(print_builtin_json_ast, false,
             "print source AST for builtins as JSON")
-DEFINE_bool(trace_calls, false, "trace calls")
-DEFINE_bool(trace_builtin_calls, false, "trace builtins calls")
+DEFINE_bool(trace_calls, __TRACE, "trace calls")
+DEFINE_bool(trace_builtin_calls, __TRACE, "trace builtins calls")
 DEFINE_string(stop_at, "", "function name where to insert a breakpoint")
 
 // compiler.cc
@@ -347,7 +351,7 @@
             "prints when objects are turned into dictionaries.")
 
 // runtime.cc
-DEFINE_bool(trace_lazy, false, "trace lazy compilation")
+DEFINE_bool(trace_lazy, __TRACE, "trace lazy compilation")
 
 // serialize.cc
 DEFINE_bool(debug_serialization, false,
@@ -433,12 +437,13 @@
 #define FLAG FLAG_READONLY
 #endif
 
+
 // code-stubs.cc
-DEFINE_bool(print_code_stubs, false, "print code stubs")
+DEFINE_bool(print_code_stubs, __CODE, "print code stubs")
 
 // codegen-ia32.cc / codegen-arm.cc
-DEFINE_bool(print_code, false, "print generated code")
-DEFINE_bool(print_builtin_code, false, "print generated code for builtins")
+DEFINE_bool(print_code, __CODE, "print generated code")
+DEFINE_bool(print_builtin_code, __CODE, "print generated code for builtins")
 
 // Cleanup...
 #undef FLAG_FULL
Index: src/ic.cc
===================================================================
--- src/ic.cc	(revision 4925)
+++ src/ic.cc	(working copy)
@@ -38,6 +38,14 @@
 namespace v8 {
 namespace internal {
 
+#define _DD(a, b, c) \
+  printf("____________START___________\n"); \
+  printf(a); \
+  b->Print(); \
+  printf("\n"); \
+  c->Print(); \
+  printf("\n____________END_____________\n");
+
 #ifdef DEBUG
 static char TransitionMarkFromState(IC::State state) {
   switch (state) {
@@ -136,7 +144,7 @@
 
 IC::State IC::StateFrom(Code* target, Object* receiver, Object* name) {
   IC::State state = target->ic_state();
-
+ 
   if (state != MONOMORPHIC) return state;
   if (receiver->IsUndefined() || receiver->IsNull()) return state;
 
@@ -400,7 +408,6 @@
   frame->SetExpression(index, *Factory::ToObject(object));
 }
 
-
 Object* CallICBase::LoadFunction(State state,
                                  Handle<Object> object,
                                  Handle<String> name) {
@@ -432,6 +439,7 @@
   LookupResult lookup;
   LookupForRead(*object, *name, &lookup);
 
+
   if (!lookup.IsProperty()) {
     // If the object does not have the requested property, check which
     // exception we need to throw.
@@ -992,12 +1000,14 @@
       }
     }
     set_target(stub);
-    // For JSObjects that are not value wrappers and that do not have
-    // indexed interceptors, we initialize the inlined fast case (if
-    // present) by patching the inlined map check.
+    // For JSObjects with fast elements that are not value wrappers
+    // and that do not have indexed interceptors, we initialize the
+    // inlined fast case (if present) by patching the inlined map
+    // check.
     if (object->IsJSObject() &&
         !object->IsJSValue() &&
-        !JSObject::cast(*object)->HasIndexedInterceptor()) {
+        !JSObject::cast(*object)->HasIndexedInterceptor() &&
+        JSObject::cast(*object)->HasFastElements()) {
       Map* map = JSObject::cast(*object)->map();
       PatchInlinedLoad(address(), map);
     }
Index: src/ic.h
===================================================================
--- src/ic.h	(revision 4925)
+++ src/ic.h	(working copy)
@@ -33,11 +33,7 @@
 namespace v8 {
 namespace internal {
 
-// Flag indicating whether an IC stub needs to check that a backing
-// store is in dictionary case.
-enum DictionaryCheck { CHECK_DICTIONARY, DICTIONARY_CHECK_DONE };
 
-
 // IC_UTIL_LIST defines all utility functions called from generated
 // inline caching code. The argument for the macro, ICU, is the function name.
 #define IC_UTIL_LIST(ICU)                             \
Index: src/platform-nullos.cc
===================================================================
--- src/platform-nullos.cc	(revision 4925)
+++ src/platform-nullos.cc	(working copy)
@@ -137,14 +137,14 @@
   vfprintf(stderr, format, args);
 }
 
-
-int OS::SNPrintF(char* str, size_t size, const char* format, ...) {
+int OS::SNPrintF(Vector<char> str, const char* format, ...) {
   UNIMPLEMENTED();
   return 0;
 }
 
-
-int OS::VSNPrintF(char* str, size_t size, const char* format, va_list args) {
+int OS::VSNPrintF(Vector<char> str,
+                  const char* format,
+                  va_list args) {
   UNIMPLEMENTED();
   return 0;
 }
@@ -163,6 +163,7 @@
 
 bool OS::ArmCpuHasFeature(CpuFeature feature) {
   UNIMPLEMENTED();
+  return false;
 }
 
 
@@ -240,7 +241,7 @@
 }
 
 
-VirtualMemory::VirtualMemory(size_t size, void* address_hint) {
+VirtualMemory::VirtualMemory(size_t size) {
   UNIMPLEMENTED();
 }
 
@@ -401,6 +402,11 @@
     UNIMPLEMENTED();
   }
 
+  virtual bool Wait(int timeout) {
+    UNIMPLEMENTED();
+    return false;
+  }
+
   virtual void Signal() {
     UNIMPLEMENTED();
   }
@@ -416,6 +422,9 @@
 
 #ifdef ENABLE_LOGGING_AND_PROFILING
 
+/*
+//ProfileSampler no longer seems to exist...
+
 class ProfileSampler::PlatformData  : public Malloced {
  public:
   PlatformData() {
@@ -449,6 +458,8 @@
   UNIMPLEMENTED();
 }
 
+*/
+
 #endif  // ENABLE_LOGGING_AND_PROFILING
 
 } }  // namespace v8::internal
Index: src/top.h
===================================================================
--- src/top.h	(revision 4925)
+++ src/top.h	(working copy)
@@ -235,6 +235,7 @@
   static Address c_entry_fp(ThreadLocalTop* thread) {
     return thread->c_entry_fp_;
   }
+  
   static Address handler(ThreadLocalTop* thread) { return thread->handler_; }
 
   static inline Address* c_entry_fp_address() {
Index: src/regexp-macro-assembler.cc
===================================================================
--- src/regexp-macro-assembler.cc	(revision 4925)
+++ src/regexp-macro-assembler.cc	(working copy)
@@ -111,6 +111,12 @@
   ASSERT(previous_index >= 0);
   ASSERT(previous_index <= subject->length());
 
+
+#if 0
+  // pmarch dump regexp code
+  regexp_code->Print();
+#endif
+
   // No allocations before calling the regexp, but we can't use
   // AssertNoAllocation, since regexps might be preempted, and another thread
   // might do allocation anyway.
Index: src/jsregexp.h
===================================================================
--- src/jsregexp.h	(revision 4925)
+++ src/jsregexp.h	(working copy)
@@ -114,7 +114,7 @@
   static IrregexpResult IrregexpExecOnce(Handle<JSRegExp> regexp,
                                          Handle<String> subject,
                                          int index,
-                                         Vector<int32_t> registers);
+                                         Vector<int> registers);
 
   // Execute an Irregexp bytecode pattern.
   // On a successful match, the result is a JSArray containing
Index: src/platform-nacl.cc
===================================================================
--- src/platform-nacl.cc	(revision 0)
+++ src/platform-nacl.cc	(revision 0)
@@ -0,0 +1,510 @@
+// Copyright 2006-2008 the V8 project authors. All rights reserved.
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+//     * Redistributions of source code must retain the above copyright
+//       notice, this list of conditions and the following disclaimer.
+//     * Redistributions in binary form must reproduce the above
+//       copyright notice, this list of conditions and the following
+//       disclaimer in the documentation and/or other materials provided
+//       with the distribution.
+//     * Neither the name of Google Inc. nor the names of its
+//       contributors may be used to endorse or promote products derived
+//       from this software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+// Platform specific code for Native Client goes here
+
+#include <errno.h>
+#include <pthread.h>
+#include <sched.h>
+#include <semaphore.h>
+#include <stddef.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <time.h>
+#include <sys/time.h>
+#include <sys/mman.h>
+#include <stdarg.h>
+
+#include "v8.h"
+#include "platform.h"
+
+namespace v8 {
+namespace internal {
+
+// NACL doesn't yet support _SC_PAGESIZE
+static const size_t kNaclPageSize = 0x10000;
+
+// 0 is never a valid thread id on Linux since tids and pids share a
+// name space and pid 0 is reserved (see man 2 kill).
+static const pthread_t kNoThread = (pthread_t) 0;
+
+// Give V8 the opportunity to override the default ceil behaviour.
+double ceiling(double x) {
+  return ceil(x);
+}
+
+// Initialize OS class early in the V8 startup.
+void OS::Setup() {
+  //NACL does yet support srandom() so use srand() instead
+  uint64_t seed = static_cast<uint64_t>(TimeCurrentMillis());
+  srand(static_cast<unsigned int>(seed));
+}
+
+// Returns the accumulated user time for thread.
+int OS::GetUserTime(uint32_t* secs,  uint32_t* usecs) {
+  UNIMPLEMENTED();
+  *secs = 0;
+  *usecs = 0;
+  return 0;
+}
+
+// Returns a string identifying the current timezone taking into
+// account daylight saving.
+const char* OS::LocalTimezone(double time) {
+  return "<none>";
+}
+
+// Returns the local time offset in milliseconds east of UTC without
+// taking daylight savings time into account.
+double OS::LocalTimeOffset() {
+  return 0;
+}
+
+int OS::SNPrintF(Vector<char> str, const char* format, ...) {
+  va_list args;
+  va_start(args, format);
+  int n = vsprintf(str.start(), format, args);
+  va_end(args);
+  return n;
+}
+
+int OS::VSNPrintF(Vector<char> str,
+                  const char* format,
+                  va_list args) {
+  //*THIS IS BAD*
+  // NACL doesn't support vsnprintf, so we ignore size of buffer
+  int len = 0x500;
+  static char* buf = NULL;
+  if (buf == NULL)
+    buf = (char*) malloc(len);
+
+  int n = vsprintf(buf, format, args);
+  if (n > len) {
+    fprintf(stderr, "Too much %d\n", n);
+    ASSERT(0);
+  }
+
+  if (n < 0 || n >= str.length()) {
+    if ( n >= str.length())
+      memcpy(str.start(), buf, str.length());
+    str[str.length() - 1] = '\0';
+    return -1;
+  }
+
+  memcpy(str.start(), buf, n);
+  str[n] = '\0';
+
+  return n; 
+}
+
+
+uint64_t OS::CpuFeaturesImpliedByPlatform() {
+  return 0;
+}
+
+bool OS::ArmCpuHasFeature(CpuFeature feature) {
+  UNIMPLEMENTED();
+  return false;
+}
+
+// We keep the lowest and highest addresses mapped as a quick way of
+// determining that pointers are outside the heap (used mostly in assertions
+// and verification).  The estimate is conservative, ie, not all addresses in
+// 'allocated' space are actually allocated to our heap.  The range is
+// [lowest, highest), inclusive on the low and and exclusive on the high end.
+static void* lowest_ever_allocated = reinterpret_cast<void*>(-1);
+static void* highest_ever_allocated = reinterpret_cast<void*>(0);
+
+static void UpdateAllocatedSpaceLimits(void* address, int size) {
+  lowest_ever_allocated = Min(lowest_ever_allocated, address);
+  highest_ever_allocated =
+      Max(highest_ever_allocated,
+          reinterpret_cast<void*>(reinterpret_cast<char*>(address) + size));
+}
+
+bool OS::IsOutsideAllocatedSpace(void* address) {
+  return address < lowest_ever_allocated || address >= highest_ever_allocated;
+}
+
+size_t OS::AllocateAlignment() {
+  return kNaclPageSize;
+}
+
+void* OS::Allocate(const size_t requested,
+                   size_t* allocated,
+                   bool is_executable) {
+  const size_t msize = RoundUp(requested, kNaclPageSize);
+  int prot = PROT_READ | PROT_WRITE;
+  void* mbase = mmap(NULL, msize, prot, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
+  if (mbase == MAP_FAILED) {
+    LOG(StringEvent("OS::Allocate", "mmap failed"));
+    return NULL;
+  }
+  *allocated = msize;
+  UpdateAllocatedSpaceLimits(mbase, msize);
+  return mbase;
+}
+
+
+void OS::Free(void* buf, const size_t length) {
+  const size_t msize = RoundUp(length, kNaclPageSize);
+  int rv = munmap(buf, msize);
+  if (rv != 0) {
+    LOG(StringEvent("OS::Free", "munmap failed"));
+  }
+}
+
+
+#ifdef ENABLE_HEAP_PROTECTION
+
+void OS::Protect(void* address, size_t size) {
+  UNIMPLEMENTED();
+}
+
+
+void OS::Unprotect(void* address, size_t size, bool is_executable) {
+  UNIMPLEMENTED();
+}
+
+#endif
+
+
+void OS::Sleep(int milliseconds) {
+  UNIMPLEMENTED();
+}
+
+void OS::Abort() {
+#ifdef DEBUG
+  static int* invalid = reinterpret_cast<int*>(0);
+  *invalid=1;//force debugger to stop without int3
+#endif
+  asm ( "hlt" );
+}
+
+void OS::DebugBreak() {
+  Abort();
+}
+
+OS::MemoryMappedFile* OS::MemoryMappedFile::create(const char* name, int size,
+    void* initial) {
+  UNIMPLEMENTED();
+  return NULL;
+}
+
+
+void OS::LogSharedLibraryAddresses() {
+  UNIMPLEMENTED();
+}
+
+
+int OS::StackWalk(Vector<OS::StackFrame> frames) {
+  UNIMPLEMENTED();
+  return 0;
+}
+
+// Constants used for mmap.
+static const int kMmapFd = -1;
+static const int kMmapFdOffset = 0;
+
+VirtualMemory::VirtualMemory(size_t size) {
+  address_ = mmap(NULL, size, PROT_NONE,
+                  MAP_PRIVATE | MAP_ANONYMOUS,
+                  kMmapFd, kMmapFdOffset);
+  size_ = size;
+}
+
+VirtualMemory::~VirtualMemory() {
+  if (IsReserved()) {
+    if (0 == munmap(address(), size())) address_ = MAP_FAILED;
+  }
+}
+
+bool VirtualMemory::IsReserved() {
+  return address_ != MAP_FAILED;
+}
+
+bool VirtualMemory::Commit(void* address, size_t size, bool is_executable) {
+  int prot = PROT_READ | PROT_WRITE | (is_executable ? PROT_EXEC : 0);
+  
+  if (MAP_FAILED == mmap(address, size, prot,
+                         MAP_PRIVATE | MAP_ANONYMOUS | MAP_FIXED,
+                         kMmapFd, kMmapFdOffset)) {
+    return false;
+  }
+
+  UpdateAllocatedSpaceLimits(address, size);
+  return true;
+}
+
+
+bool VirtualMemory::Uncommit(void* address, size_t size) {
+  return mmap(address, size, PROT_NONE,
+              MAP_PRIVATE | MAP_ANONYMOUS | MAP_FIXED,
+              kMmapFd, kMmapFdOffset) != MAP_FAILED;
+}
+
+class ThreadHandle::PlatformData : public Malloced {
+ public:
+  explicit PlatformData(ThreadHandle::Kind kind) {
+    Initialize(kind);
+  }
+
+  void Initialize(ThreadHandle::Kind kind) {
+    switch (kind) {
+      case ThreadHandle::SELF: thread_ = pthread_self(); break;
+      case ThreadHandle::INVALID: thread_ = kNoThread; break;
+    }
+  }
+
+  pthread_t thread_;  // Thread handle for pthread.
+};
+
+
+ThreadHandle::ThreadHandle(Kind kind) {
+  data_ = new PlatformData(kind);
+}
+
+
+void ThreadHandle::Initialize(ThreadHandle::Kind kind) {
+  data_->Initialize(kind);
+}
+
+
+ThreadHandle::~ThreadHandle() {
+  delete data_;
+}
+
+
+bool ThreadHandle::IsSelf() const {
+  return pthread_equal(data_->thread_, pthread_self());
+}
+
+
+bool ThreadHandle::IsValid() const {
+  return data_->thread_ != kNoThread;
+}
+
+
+Thread::Thread() : ThreadHandle(ThreadHandle::INVALID) {
+}
+
+
+Thread::~Thread() {
+}
+
+
+static void* ThreadEntry(void* arg) {
+  Thread* thread = reinterpret_cast<Thread*>(arg);
+  // This is also initialized by the first argument to pthread_create() but we
+  // don't know which thread will run first (the original thread or the new
+  // one) so we initialize it here too.
+  thread->thread_handle_data()->thread_ = pthread_self();
+  ASSERT(thread->IsValid());
+  thread->Run();
+  return NULL;
+}
+
+
+void Thread::Start() {
+  pthread_create(&thread_handle_data()->thread_, NULL, ThreadEntry, this);
+  ASSERT(IsValid());
+}
+
+
+void Thread::Join() {
+  pthread_join(thread_handle_data()->thread_, NULL);
+}
+
+
+Thread::LocalStorageKey Thread::CreateThreadLocalKey() {
+  pthread_key_t key;
+  int result = pthread_key_create(&key, NULL);
+  USE(result);
+  ASSERT(result == 0);
+  return static_cast<LocalStorageKey>(key);
+}
+
+
+void Thread::DeleteThreadLocalKey(LocalStorageKey key) {
+  pthread_key_t pthread_key = static_cast<pthread_key_t>(key);
+  int result = pthread_key_delete(pthread_key);
+  USE(result);
+  ASSERT(result == 0);
+}
+
+
+void* Thread::GetThreadLocal(LocalStorageKey key) {
+  pthread_key_t pthread_key = static_cast<pthread_key_t>(key);
+  return pthread_getspecific(pthread_key);
+}
+
+
+void Thread::SetThreadLocal(LocalStorageKey key, void* value) {
+  pthread_key_t pthread_key = static_cast<pthread_key_t>(key);
+  pthread_setspecific(pthread_key, value);
+}
+
+
+void Thread::YieldCPU() {
+  sched_yield();
+}
+
+class LinuxMutex : public Mutex {
+ public:
+
+  LinuxMutex() {
+    pthread_mutexattr_t attrs;
+    int result = pthread_mutexattr_init(&attrs);
+    ASSERT(result == 0);
+    result = pthread_mutexattr_settype(&attrs, PTHREAD_MUTEX_RECURSIVE);
+    ASSERT(result == 0);
+    result = pthread_mutex_init(&mutex_, &attrs);
+    ASSERT(result == 0);
+  }
+
+  virtual ~LinuxMutex() { pthread_mutex_destroy(&mutex_); }
+
+  virtual int Lock() {
+    int result = pthread_mutex_lock(&mutex_);
+    return result;
+  }
+
+  virtual int Unlock() {
+    int result = pthread_mutex_unlock(&mutex_);
+    return result;
+  }
+
+ private:
+  pthread_mutex_t mutex_;   // Pthread mutex for POSIX platforms.
+};
+
+
+Mutex* OS::CreateMutex() {
+  return new LinuxMutex();
+}
+
+
+class LinuxSemaphore : public Semaphore {
+ public:
+  explicit LinuxSemaphore(int count) {  sem_init(&sem_, 0, count); }
+  virtual ~LinuxSemaphore() { sem_destroy(&sem_); }
+
+  virtual void Wait();
+  virtual bool Wait(int timeout);
+  virtual void Signal() { sem_post(&sem_); }
+ private:
+  sem_t sem_;
+};
+
+
+void LinuxSemaphore::Wait() {
+  while (true) {
+    int result = sem_wait(&sem_);
+    if (result == 0) return;  // Successfully got semaphore.
+    CHECK(result == -1 && errno == EINTR);  // Signal caused spurious wakeup.
+  }
+}
+
+bool LinuxSemaphore::Wait(int timeout) {
+  //nacl is missing sem_timedwait
+  UNIMPLEMENTED();
+  return false;
+}
+
+Semaphore* OS::CreateSemaphore(int count) {
+  return new LinuxSemaphore(count);
+}
+Socket* OS::CreateSocket() {
+  //nacl is missing socket apis
+  UNIMPLEMENTED();
+  return 0;
+}
+
+bool Socket::Setup() {
+  return true;
+}
+
+int Socket::LastError() {
+  return errno;
+}
+
+int OS::ActivationFrameAlignment() {
+  return 32;
+}
+
+uint16_t Socket::HToN(uint16_t value) {
+  UNIMPLEMENTED();
+  return 0;
+}
+
+
+uint16_t Socket::NToH(uint16_t value) {
+  UNIMPLEMENTED();
+  return 0;
+}
+
+
+uint32_t Socket::HToN(uint32_t value) {
+  UNIMPLEMENTED();
+  return 0;
+}
+
+
+uint32_t Socket::NToH(uint32_t value) {
+  UNIMPLEMENTED();
+  return 0;
+}
+
+void OS::ReleaseStore(volatile AtomicWord* ptr, AtomicWord value) {
+  __asm__ __volatile__("" : : : "memory");
+  // An x86 store acts as a release barrier.
+  *ptr = value;
+}
+
+
+
+#ifdef ENABLE_LOGGING_AND_PROFILING
+
+void Sampler::Start() {
+  UNIMPLEMENTED();
+}
+void Sampler::Stop() {
+  UNIMPLEMENTED();
+}
+
+Sampler::Sampler(int interval, bool profiling)
+    : interval_(interval), profiling_(profiling), active_(false) {
+}
+
+Sampler::~Sampler() {
+}
+
+
+#endif  // ENABLE_LOGGING_AND_PROFILING
+
+} }  // namespace v8::internal
+
Index: src/factory.h
===================================================================
--- src/factory.h	(revision 4925)
+++ src/factory.h	(working copy)
@@ -180,6 +180,10 @@
 
   static Handle<Map> CopyMapDropTransitions(Handle<Map> map);
 
+  static Handle<Map> GetFastElementsMap(Handle<Map> map);
+
+  static Handle<Map> GetSlowElementsMap(Handle<Map> map);
+
   static Handle<FixedArray> CopyFixedArray(Handle<FixedArray> array);
 
   // Numbers (eg, literals) are pretenured by the parser.
Index: src/assembler.h
===================================================================
--- src/assembler.h	(revision 4925)
+++ src/assembler.h	(working copy)
@@ -198,7 +198,11 @@
   // this relocation applies to;
   // can only be called if IsCodeTarget(rmode_) || rmode_ == RUNTIME_ENTRY
   INLINE(Address target_address());
+#ifdef NACL  
+  INLINE(void set_target_address(Address target, uint32_t extraoffset = 0));
+#else
   INLINE(void set_target_address(Address target));
+#endif  
   INLINE(Object* target_object());
   INLINE(Handle<Object> target_object_handle(Assembler* origin));
   INLINE(Object** target_object_address());
Index: src/regexp-stack.cc
===================================================================
--- src/regexp-stack.cc	(revision 4925)
+++ src/regexp-stack.cc	(working copy)
@@ -92,7 +92,7 @@
     }
     thread_local_.memory_ = new_memory;
     thread_local_.memory_size_ = size;
-    thread_local_.limit_ = new_memory + kStackLimitSlack * kPointerSize;
+    thread_local_.limit_ = new_memory + kStackLimitSlack * kStackPointerSize;
   }
   return thread_local_.memory_ + thread_local_.memory_size_;
 }
Index: src/v8.cc
===================================================================
--- src/v8.cc	(revision 4925)
+++ src/v8.cc	(working copy)
@@ -35,6 +35,7 @@
 #include "heap-profiler.h"
 #include "oprofile-agent.h"
 #include "log.h"
+#include <stdlib.h>
 
 namespace v8 {
 namespace internal {
@@ -166,7 +167,11 @@
 
 static uint32_t random_seed() {
   if (FLAG_random_seed == 0) {
+#ifdef NACL
+    return rand();
+#else
     return random();
+#endif
   }
   return FLAG_random_seed;
 }
Index: src/jump-target-light.h
===================================================================
--- src/jump-target-light.h	(revision 4925)
+++ src/jump-target-light.h	(working copy)
@@ -120,6 +120,9 @@
   // Has an entry frame been found?
   bool entry_frame_set_;
 
+  // Can we branch backwards to this label?
+  Directionality direction_;
+
   // The frame used on entry to the block and expected at backward
   // jumps to the block.  Set the first time something branches to this
   // jump target.
@@ -150,6 +153,7 @@
  public:
   // Construct a break target.
   inline BreakTarget();
+  inline BreakTarget(JumpTarget::Directionality direction);
 
   virtual ~BreakTarget() {}
 
Index: src/jump-target-heavy.h
===================================================================
--- src/jump-target-heavy.h	(revision 4925)
+++ src/jump-target-heavy.h	(working copy)
@@ -196,6 +196,8 @@
  public:
   // Construct a break target.
   BreakTarget() {}
+  explicit BreakTarget(JumpTarget::Directionality direction)
+    : JumpTarget(direction) { }
 
   virtual ~BreakTarget() {}
 
Index: src/ic-inl.h
===================================================================
--- src/ic-inl.h	(revision 4925)
+++ src/ic-inl.h	(working copy)
@@ -69,6 +69,7 @@
   // Convert target address to the code object. Code::GetCodeFromTargetAddress
   // is safe for use during GC where the map might be marked.
   Code* result = Code::GetCodeFromTargetAddress(target);
+  ASSERT(result);
   ASSERT(result->is_inline_cache_stub());
   return result;
 }
@@ -76,7 +77,11 @@
 
 void IC::SetTargetAtAddress(Address address, Code* target) {
   ASSERT(target->is_inline_cache_stub());
+#ifdef NACL  
   Assembler::set_target_address_at(address, target->instruction_start());
+#else
+  Assembler::set_target_address_at(address, target->instruction_start());
+#endif  
 }
 
 
Index: src/frames.cc
===================================================================
--- src/frames.cc	(revision 4925)
+++ src/frames.cc	(working copy)
@@ -432,18 +432,18 @@
 
 Address StandardFrame::GetExpressionAddress(int n) const {
   const int offset = StandardFrameConstants::kExpressionsOffset;
-  return fp() + offset - n * kPointerSize;
+  return fp() + offset - n * kStackPointerSize;
 }
 
 
 int StandardFrame::ComputeExpressionsCount() const {
   const int offset =
-      StandardFrameConstants::kExpressionsOffset + kPointerSize;
+      StandardFrameConstants::kExpressionsOffset + kStackPointerSize;
   Address base = fp() + offset;
   Address limit = sp();
   ASSERT(base >= limit);  // stack grows downwards
   // Include register-allocated locals in number of expressions.
-  return static_cast<int>((base - limit) / kPointerSize);
+  return static_cast<int>((base - limit) / kStackPointerSize);
 }
 
 
@@ -472,14 +472,14 @@
 Object* JavaScriptFrame::GetParameter(int index) const {
   ASSERT(index >= 0 && index < ComputeParametersCount());
   const int offset = JavaScriptFrameConstants::kParam0Offset;
-  return Memory::Object_at(caller_sp() + offset - (index * kPointerSize));
+  return Memory::Object_at(caller_sp() + offset - (index * kStackPointerSize));
 }
 
 
 int JavaScriptFrame::ComputeParametersCount() const {
   Address base  = caller_sp() + JavaScriptFrameConstants::kReceiverOffset;
   Address limit = fp() + JavaScriptFrameConstants::kSavedRegistersOffset;
-  return static_cast<int>((base - limit) / kPointerSize);
+  return static_cast<int>((base - limit) / kStackPointerSize);
 }
 
 
Index: src/x64/macro-assembler-x64.cc
===================================================================
--- src/x64/macro-assembler-x64.cc	(revision 4925)
+++ src/x64/macro-assembler-x64.cc	(working copy)
@@ -101,7 +101,45 @@
   andl(addr, Immediate(Page::kPageAlignmentMask >> Page::kRegionSizeLog2));
 
   // Set dirty mark for region.
+#if defined(NACL) && 1 /* nacl now supports bts, in dev */
+  //this special case can be removed once the following bug is fixed:
+  //http://code.google.com/p/nativeclient/issues/detail?id=688
+  //compute address of target word into object
+  movl(scratch, addr);
+  shrl(scratch, Immediate(0x5));
+  shll(scratch, Immediate(0x2));
+  leal(object, Operand(object, Page::kDirtyFlagOffset));
+  addl(object, scratch);
+  //compute mask into scratch
+  andl(addr,   Immediate(0x1f));
+  movl(scratch, Immediate(0x1));
+  if(addr.is(rcx)){
+    //shift uses lower bits of addr/ecx
+    shll_cl(scratch);
+  }else if(!scratch.is(rcx)){
+    //temporarily put addr in ecx for shift, since shl_cl requires ecx
+    //ecx is unused, so just swap with it
+    xchg(rcx, addr);
+    shl_cl(scratch);
+    xchg(rcx, addr);
+  }else{
+    // scratch is ecx
+    // need 3 way swap to get valid register assignment
+    // addr    => scratch/ecx
+    // scratch => object
+    // object  => addr
+    xchg(object, scratch);
+    xchg(rcx, addr);
+    shl_cl(object);
+    //undo swap
+    xchg(rcx, addr);
+    xchg(object, scratch);
+  }
+  //mask target word
+  or_(Operand(object, 0), scratch);
+#else
   bts(Operand(object, Page::kDirtyFlagOffset), addr);
+#endif
 }
 
 
@@ -134,9 +172,15 @@
   // avoid having the fast case for smis leave the registers
   // unchanged.
   if (FLAG_debug_code) {
+#ifdef NACL              
+    movq(object, BitCast<int32_t>(kZapValue), RelocInfo::NONE);
+    movq(value, BitCast<int32_t>(kZapValue), RelocInfo::NONE);
+    movq(index, BitCast<int32_t>(kZapValue), RelocInfo::NONE);
+#else
     movq(object, BitCast<int64_t>(kZapValue), RelocInfo::NONE);
     movq(value, BitCast<int64_t>(kZapValue), RelocInfo::NONE);
     movq(index, BitCast<int64_t>(kZapValue), RelocInfo::NONE);
+#endif    
   }
 }
 
@@ -182,7 +226,7 @@
     // KeyedStoreIC::GenerateGeneric.
     lea(dst, FieldOperand(object,
                           index,
-                          times_pointer_size,
+                          times_heap_pointer_size,
                           FixedArray::kHeaderSize));
   }
   RecordWriteHelper(object, dst, scratch);
@@ -192,9 +236,15 @@
   // Clobber all input registers when running with the debug-code flag
   // turned on to provoke errors.
   if (FLAG_debug_code) {
+#ifdef NACL          
+    movq(object, BitCast<int32_t>(kZapValue), RelocInfo::NONE);
+    movq(scratch, BitCast<int32_t>(kZapValue), RelocInfo::NONE);
+    movq(index, BitCast<int32_t>(kZapValue), RelocInfo::NONE);
+#else
     movq(object, BitCast<int64_t>(kZapValue), RelocInfo::NONE);
     movq(scratch, BitCast<int64_t>(kZapValue), RelocInfo::NONE);
     movq(index, BitCast<int64_t>(kZapValue), RelocInfo::NONE);
+#endif
   }
 }
 
@@ -251,7 +301,7 @@
 void MacroAssembler::CheckStackAlignment() {
   int frame_alignment = OS::ActivationFrameAlignment();
   int frame_alignment_mask = frame_alignment - 1;
-  if (frame_alignment > kPointerSize) {
+  if (frame_alignment > kStackPointerSize) {
     ASSERT(IsPowerOf2(frame_alignment));
     Label alignment_as_expected;
     testq(rsp, Immediate(frame_alignment_mask));
@@ -321,13 +371,13 @@
 
 void MacroAssembler::StubReturn(int argc) {
   ASSERT(argc >= 1 && generating_stub());
-  ret((argc - 1) * kPointerSize);
+  ret((argc - 1) * kStackPointerSize);
 }
 
 
 void MacroAssembler::IllegalOperation(int num_arguments) {
   if (num_arguments > 0) {
-    addq(rsp, Immediate(num_arguments * kPointerSize));
+    addq(rsp, Immediate(num_arguments * kStackPointerSize));
   }
   LoadRoot(rax, Heap::kUndefinedValueRootIndex);
 }
@@ -439,6 +489,11 @@
     pop(target);
   }
   lea(target, FieldOperand(target, Code::kHeaderSize));
+#ifdef NACL
+#define __
+  __ NACL_PATCH_INSTRUCTION_START(target);
+#undef __
+#endif
 }
 
 
@@ -484,9 +539,15 @@
   ASSERT_EQ(0, kSmiTag);
   // 32-bit integer always fits in a long smi.
   if (!dst.is(src)) {
+    // movl clears the most significant word in dst reg
     movl(dst, src);
   }
-  shl(dst, Immediate(kSmiShift));
+#ifdef NACL
+  JumpIfNotValidSmiValue(dst, on_overflow);
+  shll(dst, Immediate(kSmiShift));
+#else
+  shll(dst, Immediate(kSmiShift));
+#endif
 }
 
 
@@ -502,14 +563,24 @@
     }
     bind(&ok);
   }
+
+#ifdef NACL
+  Integer32ToSmi(src, src);
+  movl(dst, src);
+  SmiToInteger32(src, src);
+#else
   ASSERT(kSmiShift % kBitsPerByte == 0);
+  
   movl(Operand(dst, kSmiShift / kBitsPerByte), src);
+#endif
 }
 
 
 void MacroAssembler::Integer64PlusConstantToSmi(Register dst,
                                                 Register src,
                                                 int constant) {
+  // PMARCH
+  ASSERT(0);
   if (dst.is(src)) {
     addq(dst, Immediate(constant));
   } else {
@@ -524,26 +595,43 @@
   if (!dst.is(src)) {
     movq(dst, src);
   }
+#ifdef NACL
+  sarl(dst, Immediate(kSmiShift));
+#else
   shr(dst, Immediate(kSmiShift));
+#endif
 }
 
 
 void MacroAssembler::SmiToInteger32(Register dst, const Operand& src) {
+#ifdef NACL
+  movl(dst, src);
+  SmiToInteger32(dst, dst);
+#else
   movl(dst, Operand(src, kSmiShift / kBitsPerByte));
+#endif
 }
 
 
 void MacroAssembler::SmiToInteger64(Register dst, Register src) {
+#ifdef NACL
+  ASSERT(0);
+#else
   ASSERT_EQ(0, kSmiTag);
   if (!dst.is(src)) {
     movq(dst, src);
   }
   sar(dst, Immediate(kSmiShift));
+#endif
 }
 
 
 void MacroAssembler::SmiToInteger64(Register dst, const Operand& src) {
+#ifdef NACL
+  ASSERT(0);
+#else
   movsxlq(dst, Operand(src, kSmiShift / kBitsPerByte));
+#endif
 }
 
 
@@ -553,7 +641,11 @@
 
 
 void MacroAssembler::SmiCompare(Register dst, Register src) {
+#ifdef NACL
+  cmpl(dst, src);
+#else
   cmpq(dst, src);
+#endif
 }
 
 
@@ -563,34 +655,63 @@
     testq(dst, dst);
   } else {
     Move(kScratchRegister, src);
+#ifdef NACL
+    cmpl(dst, kScratchRegister);
+#else
     cmpq(dst, kScratchRegister);
+#endif
   }
 }
 
 
 void MacroAssembler::SmiCompare(Register dst, const Operand& src) {
+#ifdef NACL
+  cmpl(dst, src);
+#else
   cmpq(dst, src);
+#endif
 }
 
 
 void MacroAssembler::SmiCompare(const Operand& dst, Register src) {
+#ifdef NACL
+  cmpl(dst, src);
+#else
   cmpq(dst, src);
+#endif
 }
 
 
 void MacroAssembler::SmiCompare(const Operand& dst, Smi* src) {
+#ifdef NACL
+  cmpl(dst, Immediate(src->value() << kSmiShift));
+#else
   cmpl(Operand(dst, kSmiShift / kBitsPerByte), Immediate(src->value()));
+#endif
 }
 
 
 void MacroAssembler::SmiCompareInteger32(const Operand& dst, Register src) {
+#ifdef NACL
+  // scratch register is used here to preserve the value of src register
+  // if src register is used to store smi value and then converted back to
+  // integer, the later conversion will modify flags set by cmp instruction
+  ASSERT(dst.get_base().is(no_reg) || dst.get_index().is(no_reg));
+  Integer32ToSmi(scratch_reg, src);
+  cmpl(dst, scratch_reg);
+#else
   cmpl(Operand(dst, kSmiShift / kBitsPerByte), src);
+#endif
 }
 
 
 void MacroAssembler::PositiveSmiTimesPowerOfTwoToInteger64(Register dst,
                                                            Register src,
                                                            int power) {
+#ifdef NACL
+  ASSERT(0);
+#endif
+
   ASSERT(power >= 0);
   ASSERT(power < 64);
   if (power == 0) {
@@ -608,12 +729,35 @@
 }
 
 
+void MacroAssembler::PositiveSmiTimesPowerOfTwoToInteger32(Register dst,
+                                                           Register src,
+                                                           int power) {
+  ASSERT(power >= 0);
+  ASSERT(power < 32);
+  if (power == 0) {
+    SmiToInteger32(dst, src);
+    return;
+  }
+  if (!dst.is(src)) {
+    movl(dst, src);
+  }
+  if (power < kSmiShift) {
+    sarl(dst, Immediate(kSmiShift - power));
+  } else if (power > kSmiShift) {
+    shll(dst, Immediate(power - kSmiShift));
+  }
+}
+
+
 void MacroAssembler::PositiveSmiDivPowerOfTwoToInteger32(Register dst,
                                                          Register src,
                                                          int power) {
-  ASSERT((0 <= power) && (power < 32));
-  if (dst.is(src)) {
+  ASSERT((0 <= power) && (power < 32));  if (dst.is(src)) {
+#ifdef NACL
+    shrl(dst, Immediate(power + kSmiShift));
+#else
     shr(dst, Immediate(power + kSmiShift));
+#endif
   } else {
     UNIMPLEMENTED();  // Not used.
   }
@@ -629,8 +773,13 @@
 
 Condition MacroAssembler::CheckPositiveSmi(Register src) {
   ASSERT_EQ(0, kSmiTag);
+#ifdef NACL
+  movl(kScratchRegister, src);
+  roll(kScratchRegister, Immediate(1));
+#else
   movq(kScratchRegister, src);
   rol(kScratchRegister, Immediate(1));
+#endif
   testl(kScratchRegister, Immediate(0x03));
   return zero;
 }
@@ -654,7 +803,11 @@
   }
   movl(kScratchRegister, first);
   orl(kScratchRegister, second);
+#ifdef NACL
+  roll(kScratchRegister, Immediate(1));
+#else
   rol(kScratchRegister, Immediate(1));
+#endif
   testl(kScratchRegister, Immediate(0x03));
   return zero;
 }
@@ -675,22 +828,60 @@
 Condition MacroAssembler::CheckIsMinSmi(Register src) {
   ASSERT(kSmiTag == 0 && kSmiTagSize == 1);
   movq(kScratchRegister, src);
+#ifdef NACL
+  roll(kScratchRegister, Immediate(1));
+#else
   rol(kScratchRegister, Immediate(1));
+#endif
   cmpq(kScratchRegister, Immediate(1));
   return equal;
 }
 
 
 Condition MacroAssembler::CheckInteger32ValidSmiValue(Register src) {
+#ifdef NACL
+  // if integer is positive it should have top bits set to 00..
+  // if integer is negative then the top bits shoudl be 11..
+  // the top bit is the sign, and the second top bit must be 0 (when +) or 1
+  // (when -). This bit will be descarded when integer is converted to Sim
+// PMARCH alternative version without kScratchReg
+#if 0
+  ASSERT(!src.is(kScratchRegister));
+  movl(kScratchRegister, src);
+  shll(kScratchRegister, Immediate(1));
+  xorl(kScratchRegister, src);
+  testl(kScratchRegister, Immediate(0x80000000));
+  return zero;
+#else
+  cmpl(src, Immediate(0xc0000000));
+  return positive;
+
+  Label ok, fail;
+  testl(src, Immediate(0xc0000000));
+  j(zero, &ok);
+  testl(src, Immediate(0xc0000000));
+  j(above_equal, &ok);
+  jmp(&fail);
+  bind(&ok);
+  testl(src, Immediate(0x0));
+  bind(&fail);
+  return zero;
+#endif
+#else
   // A 32-bit integer value can always be converted to a smi.
   return always;
+#endif
 }
 
 
 Condition MacroAssembler::CheckUInteger32ValidSmiValue(Register src) {
   // An unsigned 32-bit integer value is valid as long as the high bit
   // is not set.
+#ifdef NACL
+  testq(src, Immediate(0xc0000000));
+#else
   testq(src, Immediate(0x80000000));
+#endif
   return zero;
 }
 
@@ -699,14 +890,22 @@
   if (dst.is(src)) {
     ASSERT(!dst.is(kScratchRegister));
     movq(kScratchRegister, src);
+#ifdef NACL
+    negl(dst);
+#else
     neg(dst);  // Low 32 bits are retained as zero by negation.
     // Test if result is zero or Smi::kMinValue.
+#endif
     cmpq(dst, kScratchRegister);
     j(not_equal, on_smi_result);
     movq(src, kScratchRegister);
   } else {
     movq(dst, src);
+#ifdef NACL
+    negl(dst);
+#else
     neg(dst);
+#endif
     cmpq(dst, src);
     // If the result is zero or Smi::kMinValue, negation failed to create a smi.
     j(not_equal, on_smi_result);
@@ -758,12 +957,20 @@
     }
     Assert(no_overflow, "Smi subtraction overflow");
   } else if (dst.is(src1)) {
+#ifdef NACL
+    cmpl(dst, src2);
+#else
     cmpq(dst, src2);
+#endif
     j(overflow, on_not_smi_result);
     subq(dst, src2);
   } else {
     movq(dst, src1);
+#ifdef NACL
+    subl(dst, src2);
+#else 
     subq(dst, src2);
+#endif
     j(overflow, on_not_smi_result);
   }
 }
@@ -780,17 +987,29 @@
       subq(dst, src2);
     } else {
       movq(dst, src1);
+#ifdef NACL
+      subl(dst, src2);
+#else
       subq(dst, src2);
+#endif
     }
     Assert(no_overflow, "Smi subtraction overflow");
   } else if (dst.is(src1)) {
     movq(kScratchRegister, src1);
+#ifdef NACL
+    subl(kScratchRegister, src2);
+#else
     subq(kScratchRegister, src2);
+#endif
     j(overflow, on_not_smi_result);
     movq(src1, kScratchRegister);
   } else {
     movq(dst, src1);
+#ifdef NACL
+    subl(dst, src2);
+#else
     subq(dst, src2);
+#endif
     j(overflow, on_not_smi_result);
   }
 }
@@ -807,8 +1026,13 @@
   if (dst.is(src1)) {
     Label failure, zero_correct_result;
     movq(kScratchRegister, src1);  // Create backup for later testing.
+#ifdef NACL
+    SmiToInteger32(dst, src1);
+    imull(dst, src2);
+#else
     SmiToInteger64(dst, src1);
     imul(dst, src2);
+#endif
     j(overflow, &failure);
 
     // Check for negative zero result.  If product is zero, and one
@@ -830,7 +1054,11 @@
 
     bind(&correct_result);
   } else {
+#ifdef NACL
+    SmiToInteger32(dst, src1);
+#else
     SmiToInteger64(dst, src1);
+#endif
     imul(dst, src2);
     j(overflow, on_not_smi_result);
     // Check for negative zero result.  If product is zero, and one
@@ -888,7 +1116,11 @@
 
 void MacroAssembler::SmiAddConstant(const Operand& dst, Smi* constant) {
   if (constant->value() != 0) {
+#ifdef NACL
+    addl(dst, Immediate(constant->value() << kSmiShift));
+#else
     addl(Operand(dst, kSmiShift / kBitsPerByte), Immediate(constant->value()));
+#endif
   }
 }
 
@@ -1014,7 +1246,12 @@
   // We overshoot a little and go to slow case if we divide min-value
   // by any negative value, not just -1.
   Label safe_div;
+#ifdef NACL
+  // kMinValue is 0xc0000000 in NACL
+  testl(rax, Immediate(0x3fffffff));
+#else
   testl(rax, Immediate(0x7fffffff));
+#endif
   j(not_zero, &safe_div);
   testq(src2, src2);
   if (src1.is(rax)) {
@@ -1105,6 +1342,11 @@
 
 
 void MacroAssembler::SmiNot(Register dst, Register src) {
+#ifdef NACL
+  if (!dst.is(src))
+    movl(dst, src);
+  leal(dst, Operand(dst, kSmiTagMask));
+#else
   ASSERT(!dst.is(kScratchRegister));
   ASSERT(!src.is(kScratchRegister));
   // Set tag and padding bits before negating, so that they are zero afterwards.
@@ -1114,6 +1356,7 @@
   } else {
     lea(dst, Operand(src, kScratchRegister, times_1, 0));
   }
+#endif
   not_(dst);
 }
 
@@ -1187,8 +1430,13 @@
   ASSERT(is_uint5(shift_value));
   if (shift_value > 0) {
     if (dst.is(src)) {
+#ifdef NACL
+      sarl(dst, Immediate(shift_value));
+      andl(dst, Immediate(~1));
+#else
       sar(dst, Immediate(shift_value + kSmiShift));
       shl(dst, Immediate(kSmiShift));
+#endif
     } else {
       UNIMPLEMENTED();  // Not used.
     }
@@ -1200,6 +1448,7 @@
                                                   Register src,
                                                   int shift_value,
                                                   Label* on_not_smi_result) {
+
   // Logic right shift interprets its result as an *unsigned* number.
   if (dst.is(src)) {
     UNIMPLEMENTED();  // Not used.
@@ -1209,37 +1458,56 @@
       testq(dst, dst);
       j(negative, on_not_smi_result);
     }
+#ifdef NACL
+    SmiToInteger32(dst, src);
+    shrl(dst, Immediate(shift_value));
+    JumpIfNotValidSmiValue(dst, on_not_smi_result);
+    Integer32ToSmi(dst, dst);
+#else
     shr(dst, Immediate(shift_value + kSmiShift));
     shl(dst, Immediate(kSmiShift));
+#endif
   }
 }
 
 
 void MacroAssembler::SmiShiftLeftConstant(Register dst,
                                           Register src,
-                                          int shift_value) {
-  if (!dst.is(src)) {
-    movq(dst, src);
-  }
-  if (shift_value > 0) {
-    shl(dst, Immediate(shift_value));
-  }
+                                          int shift_value,
+                                          Label* on_not_smi_result) {
+  SmiToInteger32(dst, src);
+  shl(dst, Immediate(shift_value));
+  JumpIfNotValidSmiValue(dst, on_not_smi_result); 
+  Integer32ToSmi(dst, dst); 
 }
 
 
 void MacroAssembler::SmiShiftLeft(Register dst,
                                   Register src1,
-                                  Register src2) {
+                                  Register src2,
+                                  Label* on_not_smi_result) {
+  ASSERT(!dst.is(kScratchRegister) &&
+         !src1.is(kScratchRegister) &&
+         !src2.is(kScratchRegister));
   ASSERT(!dst.is(rcx));
-  Label result_ok;
-  // Untag shift amount.
-  if (!dst.is(src1)) {
-    movq(dst, src1);
-  }
+  ASSERT(!src1.is(rcx));
+
+  if (src2.is(rcx))
+    movl(kScratchRegister, src2);
+
+  SmiToInteger32(dst, src1);
   SmiToInteger32(rcx, src2);
   // Shift amount specified by lower 5 bits, not six as the shl opcode.
   and_(rcx, Immediate(0x1f));
   shl_cl(dst);
+
+  if (src2.is(rcx))
+    movl(src2, kScratchRegister);
+
+  // if the value is not Smi, it dst register should contain the value
+  // wheich later will be converted to heap number
+  JumpIfNotValidSmiValue(dst, on_not_smi_result);
+  Integer32ToSmi(dst, dst);
 }
 
 
@@ -1247,6 +1515,30 @@
                                           Register src1,
                                           Register src2,
                                           Label* on_not_smi_result) {
+#ifdef NACL
+  ASSERT(!dst.is(kScratchRegister) &&
+         !src1.is(kScratchRegister) &&
+         !src2.is(kScratchRegister));
+  ASSERT(!dst.is(rcx));
+  ASSERT(!src1.is(rcx));
+
+  if (src2.is(rcx)) {
+    movq(kScratchRegister, src2);
+  }
+
+  SmiToInteger32(rcx, src2);
+  SmiToInteger32(dst, src1);
+  shr_cl(dst);
+
+  if (src2.is(rcx)) {
+    movq(src2, kScratchRegister);
+  }
+
+  JumpIfNotValidSmiValue(dst, on_not_smi_result);
+  Integer32ToSmi(dst, dst);
+  testq(dst, dst);
+  j(negative, on_not_smi_result);  // src2 was zero and src1 negative.
+#else
   ASSERT(!dst.is(kScratchRegister));
   ASSERT(!src1.is(kScratchRegister));
   ASSERT(!src2.is(kScratchRegister));
@@ -1276,12 +1568,14 @@
   } else {
     j(negative, on_not_smi_result);  // src2 was zero and src1 negative.
   }
+#endif
 }
 
 
 void MacroAssembler::SmiShiftArithmeticRight(Register dst,
                                              Register src1,
                                              Register src2) {
+#ifdef NACL
   ASSERT(!dst.is(kScratchRegister));
   ASSERT(!src1.is(kScratchRegister));
   ASSERT(!src2.is(kScratchRegister));
@@ -1294,15 +1588,41 @@
   if (!dst.is(src1)) {
     movq(dst, src1);
   }
+
   SmiToInteger32(rcx, src2);
+  sar_cl(dst);
+  andl(dst, Immediate(~1));
+
+  if (src1.is(rcx)) {
+    movq(src1, kScratchRegister);
+  } else if (src2.is(rcx)) {
+    movq(src2, kScratchRegister);
+  }
+#else
+  ASSERT(!dst.is(kScratchRegister));
+  ASSERT(!src1.is(kScratchRegister));
+  ASSERT(!src2.is(kScratchRegister));
+  ASSERT(!dst.is(rcx));
+
+  if (src1.is(rcx)) {
+    movq(kScratchRegister, src1);
+  } else if (src2.is(rcx)) {
+    movq(kScratchRegister, src2);
+  }
+  if (!dst.is(src1)) {
+    movq(dst, src1);
+  }
+  SmiToInteger32(rcx, src2);
   orl(rcx, Immediate(kSmiShift));
   sar_cl(dst);  // Shift 32 + original rcx & 0x1f.
   shl(dst, Immediate(kSmiShift));
+  
   if (src1.is(rcx)) {
     movq(src1, kScratchRegister);
   } else if (src2.is(rcx)) {
     movq(src2, kScratchRegister);
   }
+#endif
 }
 
 
@@ -1353,9 +1673,17 @@
     movq(dst, src);
   }
   if (shift < kSmiShift) {
+#ifdef NACL
+    sarl(dst, Immediate(kSmiShift - shift));
+#else
     sar(dst, Immediate(kSmiShift - shift));
+#endif
   } else {
+#ifdef NACL
+    shll(dst, Immediate(shift - kSmiShift));
+#else
     shl(dst, Immediate(shift - kSmiShift));
+#endif
   }
   return SmiIndex(dst, times_1);
 }
@@ -1576,13 +1904,17 @@
 
 void MacroAssembler::Drop(int stack_elements) {
   if (stack_elements > 0) {
-    addq(rsp, Immediate(stack_elements * kPointerSize));
+    addq(rsp, Immediate(stack_elements * kStackPointerSize));
   }
 }
 
 
 void MacroAssembler::Test(const Operand& src, Smi* source) {
+#ifdef NACL
+  testl(src, Immediate(source->value()));
+#else
   testl(Operand(src, kIntSize), Immediate(source->value()));
+#endif
 }
 
 
@@ -1626,17 +1958,17 @@
 void MacroAssembler::PushTryHandler(CodeLocation try_location,
                                     HandlerType type) {
   // Adjust this code if not the case.
-  ASSERT(StackHandlerConstants::kSize == 4 * kPointerSize);
+  ASSERT(StackHandlerConstants::kSize == 4 * kStackPointerSize);
 
   // The pc (return address) is already on TOS.  This code pushes state,
   // frame pointer and current handler.  Check that they are expected
   // next on the stack, in that order.
   ASSERT_EQ(StackHandlerConstants::kStateOffset,
-            StackHandlerConstants::kPCOffset - kPointerSize);
+            StackHandlerConstants::kPCOffset - kStackPointerSize);
   ASSERT_EQ(StackHandlerConstants::kFPOffset,
-            StackHandlerConstants::kStateOffset - kPointerSize);
+            StackHandlerConstants::kStateOffset - kStackPointerSize);
   ASSERT_EQ(StackHandlerConstants::kNextOffset,
-            StackHandlerConstants::kFPOffset - kPointerSize);
+            StackHandlerConstants::kFPOffset - kStackPointerSize);
 
   if (try_location == IN_JAVASCRIPT) {
     if (type == TRY_CATCH_HANDLER) {
@@ -1667,7 +1999,7 @@
   movq(kScratchRegister, ExternalReference(Top::k_handler_address));
   pop(Operand(kScratchRegister, 0));
   // Remove the remaining fields.
-  addq(rsp, Immediate(StackHandlerConstants::kSize - kPointerSize));
+  addq(rsp, Immediate(StackHandlerConstants::kSize - kStackPointerSize));
 }
 
 
@@ -1678,8 +2010,7 @@
 
 void MacroAssembler::FCmp() {
   fucomip();
-  ffree(0);
-  fincstp();
+  fstp(0);
 }
 
 
@@ -1910,7 +2241,7 @@
           ExternalReference(Debug_Address::Register(i));
       movq(kScratchRegister, reg_addr);
       movq(Operand(kScratchRegister, 0), scratch);
-      lea(base, Operand(base, kPointerSize));
+      lea(base, Operand(base, kStackPointerSize));
     }
   }
 }
@@ -1975,6 +2306,11 @@
     if (!code_constant.is_null()) {
       movq(rdx, code_constant, RelocInfo::EMBEDDED_OBJECT);
       addq(rdx, Immediate(Code::kHeaderSize - kHeapObjectTag));
+#ifdef NACL
+#define __
+      NACL_PATCH_INSTRUCTION_START(rdx);
+#undef __
+#endif
     } else if (!code_register.is(rdx)) {
       movq(rdx, code_register);
     }
@@ -2032,10 +2368,15 @@
   movq(rsi, FieldOperand(function, JSFunction::kContextOffset));
   movsxlq(rbx,
           FieldOperand(rdx, SharedFunctionInfo::kFormalParameterCountOffset));
-  movq(rdx, FieldOperand(rdx, SharedFunctionInfo::kCodeOffset));
   // Advances rdx to the end of the Code object header, to the start of
   // the executable code.
+  movq(rdx, FieldOperand(rdx, SharedFunctionInfo::kCodeOffset));
   lea(rdx, FieldOperand(rdx, Code::kHeaderSize));
+#ifdef NACL
+  #define __
+  NACL_PATCH_INSTRUCTION_START(rdx);
+  #undef __
+#endif
 
   ParameterCount expected(rbx);
   InvokeCode(rdx, expected, actual, flag);
@@ -2088,14 +2429,14 @@
 void MacroAssembler::EnterExitFrame(ExitFrame::Mode mode, int result_size) {
   // Setup the frame structure on the stack.
   // All constants are relative to the frame pointer of the exit frame.
-  ASSERT(ExitFrameConstants::kCallerSPDisplacement == +2 * kPointerSize);
-  ASSERT(ExitFrameConstants::kCallerPCOffset == +1 * kPointerSize);
-  ASSERT(ExitFrameConstants::kCallerFPOffset ==  0 * kPointerSize);
+  ASSERT(ExitFrameConstants::kCallerSPDisplacement == +2 * kStackPointerSize);
+  ASSERT(ExitFrameConstants::kCallerPCOffset == +1 * kStackPointerSize);
+  ASSERT(ExitFrameConstants::kCallerFPOffset ==  0 * kStackPointerSize);
   push(rbp);
   movq(rbp, rsp);
 
   // Reserve room for entry stack pointer and push the debug marker.
-  ASSERT(ExitFrameConstants::kSPOffset == -1 * kPointerSize);
+  ASSERT(ExitFrameConstants::kSPOffset == -1 * kStackPointerSize);
   push(Immediate(0));  // Saved entry sp, patched before call.
   movq(kScratchRegister, CodeObject(), RelocInfo::EMBEDDED_OBJECT);
   push(kScratchRegister);  // Accessed from EditFrame::code_slot.
@@ -2103,17 +2444,19 @@
   // Save the frame pointer and the context in top.
   ExternalReference c_entry_fp_address(Top::k_c_entry_fp_address);
   ExternalReference context_address(Top::k_context_address);
-  movq(r14, rax);  // Backup rax before we use it.
 
+  //NACL_CHANGE(pmarch) r15 ==> r14; r14 ==> r12; we must not use r15  
+  movq(r12, rax);  // Backup rax before we use it.
+
   movq(rax, rbp);
   store_rax(c_entry_fp_address);
   movq(rax, rsi);
   store_rax(context_address);
 
-  // Setup argv in callee-saved register r15. It is reused in LeaveExitFrame,
+  // Setup argv in callee-saved register r14. It is reused in LeaveExitFrame,
   // so it must be retained across the C-call.
-  int offset = StandardFrameConstants::kCallerSPOffset - kPointerSize;
-  lea(r15, Operand(rbp, r14, times_pointer_size, offset));
+  int offset = StandardFrameConstants::kCallerSPOffset - kStackPointerSize;
+  lea(r14, Operand(rbp, r12, times_stack_pointer_size, offset));
 
 #ifdef ENABLE_DEBUGGER_SUPPORT
   // Save the state of all registers to the stack from the memory
@@ -2131,14 +2474,14 @@
 
 #ifdef _WIN64
   // Reserve space on stack for result and argument structures, if necessary.
-  int result_stack_space = (result_size < 2) ? 0 : result_size * kPointerSize;
+  int result_stack_space = (result_size < 2) ? 0 : result_size * kStackPointerSize;
   // Reserve space for the Arguments object.  The Windows 64-bit ABI
   // requires us to pass this structure as a pointer to its location on
   // the stack.  The structure contains 2 values.
-  int argument_stack_space = 2 * kPointerSize;
+  int argument_stack_space = 2 * kStackPointerSize;
   // We also need backing space for 4 parameters, even though
   // we only pass one or two parameter, and it is in a register.
-  int argument_mirror_space = 4 * kPointerSize;
+  int argument_mirror_space = 4 * kStackPointerSize;
   int total_stack_space =
       argument_mirror_space + argument_stack_space + result_stack_space;
   subq(rsp, Immediate(total_stack_space));
@@ -2159,14 +2502,14 @@
 
 void MacroAssembler::LeaveExitFrame(ExitFrame::Mode mode, int result_size) {
   // Registers:
-  // r15 : argv
+  // r14 : argv
 #ifdef ENABLE_DEBUGGER_SUPPORT
   // Restore the memory copy of the registers by digging them out from
   // the stack. This is needed to allow nested break points.
   if (mode == ExitFrame::MODE_DEBUG) {
     // It's okay to clobber register rbx below because we don't need
     // the function pointer after this.
-    const int kCallerSavedSize = kNumJSCallerSaved * kPointerSize;
+    const int kCallerSavedSize = kNumJSCallerSaved * kStackPointerSize;
     int kOffset = ExitFrameConstants::kCodeOffset - kCallerSavedSize;
     lea(rbx, Operand(rbp, kOffset));
     CopyRegistersFromStackToMemory(rbx, rcx, kJSCallerSaved);
@@ -2174,17 +2517,14 @@
 #endif
 
   // Get the return address from the stack and restore the frame pointer.
-  movq(rcx, Operand(rbp, 1 * kPointerSize));
-  movq(rbp, Operand(rbp, 0 * kPointerSize));
-
-  // Pop everything up to and including the arguments and the receiver
-  // from the caller stack.
-  lea(rsp, Operand(r15, 1 * kPointerSize));
-
+  movq(rcx, Operand(rbp, 1 * kStackPointerSize));
+  movq(rbp, Operand(rbp, 0 * kStackPointerSize));
+  lea(rsp, Operand(r14, 1 * kStackPointerSize));
   // Restore current context from top and clear it in debug mode.
   ExternalReference context_address(Top::k_context_address);
   movq(kScratchRegister, context_address);
   movq(rsi, Operand(kScratchRegister, 0));
+
 #ifdef DEBUG
   movq(Operand(kScratchRegister, 0), Immediate(0));
 #endif
@@ -2217,7 +2557,7 @@
   int depth = 0;
 
   if (save_at_depth == depth) {
-    movq(Operand(rsp, kPointerSize), object_reg);
+    movq(Operand(rsp, kStackPointerSize), object_reg);
   }
 
   // Check the maps in the prototype chain.
@@ -2269,7 +2609,7 @@
     }
 
     if (save_at_depth == depth) {
-      movq(Operand(rsp, kPointerSize), reg);
+      movq(Operand(rsp, kStackPointerSize), reg);
     }
 
     // Go to the next object in the prototype chain.
@@ -2642,7 +2982,6 @@
                      gc_required,
                      TAG_OBJECT);
 
-  // Set the map. The other fields are left uninitialized.
   LoadRoot(kScratchRegister, Heap::kConsStringMapRootIndex);
   movq(FieldOperand(result, HeapObject::kMapOffset), kScratchRegister);
 }
@@ -2714,9 +3053,9 @@
   ASSERT(IsPowerOf2(frame_alignment));
   int argument_slots_on_stack =
       ArgumentStackSlotsForCFunctionCall(num_arguments);
-  subq(rsp, Immediate((argument_slots_on_stack + 1) * kPointerSize));
+  subq(rsp, Immediate((argument_slots_on_stack + 1) * kStackPointerSize));
   and_(rsp, Immediate(-frame_alignment));
-  movq(Operand(rsp, argument_slots_on_stack * kPointerSize), kScratchRegister);
+  movq(Operand(rsp, argument_slots_on_stack * kStackPointerSize), kScratchRegister);
 }
 
 
@@ -2738,7 +3077,7 @@
   ASSERT(num_arguments >= 0);
   int argument_slots_on_stack =
       ArgumentStackSlotsForCFunctionCall(num_arguments);
-  movq(rsp, Operand(rsp, argument_slots_on_stack * kPointerSize));
+  movq(rsp, Operand(rsp, argument_slots_on_stack * kStackPointerSize));
 }
 
 
Index: src/x64/assembler-x64.cc
===================================================================
--- src/x64/assembler-x64.cc	(revision 4925)
+++ src/x64/assembler-x64.cc	(working copy)
@@ -31,6 +31,9 @@
 
 #include "macro-assembler.h"
 #include "serialize.h"
+#ifdef NACL
+#include "nacl-sandbox-x64.h"
+#endif
 
 namespace v8 {
 namespace internal {
@@ -57,19 +60,26 @@
 #define __ assm.
   // Save old rsp, since we are going to modify the stack.
   __ push(rbp);
+#ifndef NACL
+  // illegal
   __ pushfq();
+#endif
   __ push(rcx);
   __ push(rbx);
   __ movq(rbp, rsp);
 
   // If we can modify bit 21 of the EFLAGS register, then CPUID is supported.
+#ifndef NACL
   __ pushfq();
+#endif
   __ pop(rax);
   __ movq(rdx, rax);
   __ xor_(rax, Immediate(0x200000));  // Flip bit 21.
   __ push(rax);
+#ifndef NACL
   __ popfq();
   __ pushfq();
+#endif
   __ pop(rax);
   __ xor_(rax, rdx);  // Different if CPUID is supported.
   __ j(not_zero, &cpuid);
@@ -88,7 +98,9 @@
     __ cpuid();
     // Move the result from ecx:edx to rdi.
     __ movl(rdi, rdx);  // Zero-extended to 64 bits.
+#ifndef NACL
     __ shl(rcx, Immediate(32));
+#endif
     __ or_(rdi, rcx);
 
     // Get the sahf supported flag, from CPUID(0x80000001)
@@ -106,12 +118,19 @@
   __ or_(rax, rcx);
   __ or_(rax, Immediate(1 << CPUID));
 
+
+#ifdef NACL
+  // fix this hack
+  __ movq(rax, Immediate(-1));
+#endif
   // Done.
   __ bind(&done);
   __ movq(rsp, rbp);
   __ pop(rbx);
   __ pop(rcx);
+#ifndef NACL
   __ popfq();
+#endif
   __ pop(rbp);
   __ ret(0);
 #undef __
@@ -285,6 +304,98 @@
   }
 }
 
+
+#ifdef NACL
+void Operand::copyfrom(const Operand& op) {
+  rex_ = op.rex_;
+  len_ = op.len_;
+  *(uint32_t*)(buf_) = *(uint32_t*)(op.buf_);
+  *(uint16_t*)(buf_ + 4) = *(uint16_t*)(op.buf_ + 4);
+}
+
+
+Register Operand::get_base() const {
+  ASSERT(len_ >= 1);
+  // Operand encodes REX ModR/M [SIB] [Disp].
+  byte modrm = buf_[0];
+  ASSERT(modrm < 0xC0);  // Disallow mode 3 (register target).
+  bool has_sib = ((modrm & 0x07) == 0x04);
+  byte mode = modrm & 0xC0;
+  int base_reg = (has_sib ? buf_[1] : modrm) & 0x07;
+  bool is_baseless = (mode == 0) && (base_reg == 0x05);  // No base or RIP base.
+
+  if (is_baseless) 
+    return no_reg;
+  else
+    return Register::toRegister(((rex_ & 1) << 3) | base_reg);
+}
+
+
+int32_t Operand::get_disp() const {
+  ASSERT(len_ >= 1);
+  // Operand encodes REX ModR/M [SIB] [Disp].
+  byte modrm = buf_[0];
+  ASSERT(modrm < 0xC0);  // Disallow mode 3 (register target).
+  bool has_sib = ((modrm & 0x07) == 0x04);
+  byte mode = modrm & 0xC0;
+  int disp_offset = has_sib ? 2 : 1;
+  int base_reg = (has_sib ? buf_[1] : modrm) & 0x07;
+  bool is_baseless = (mode == 0) && (base_reg == 0x05);  // No base or RIP base.
+  int32_t disp_value = 0;
+
+  if (mode == 0x80 || is_baseless) {
+    // Mode 2 or mode 0 with rbp/r13 as base: Word displacement.
+    disp_value = *reinterpret_cast<const int32_t*>(&buf_[disp_offset]);
+  } else if (mode == 0x40) {
+    // Mode 1: Byte displacement.
+    disp_value = static_cast<signed char>(buf_[disp_offset]);
+  }
+
+  return disp_value;
+}
+
+
+Register Operand::get_index() const {
+  ASSERT(len_ >= 1);
+  // Operand encodes REX ModR/M [SIB] [Disp].
+  byte modrm = buf_[0];
+  ASSERT(modrm < 0xC0);  // Disallow mode 3 (register target).
+  bool has_sib = ((modrm & 0x07) == 0x04);
+
+  if (!has_sib) 
+    return no_reg;
+
+  int reg = ((rex_ & 2) << 2) | ((buf_[1] & 0x38) >> 3);
+
+  if (reg == rsp.code_)
+    return no_reg; // rsp cannot be used as index register
+
+  return Register::toRegister(reg);
+}
+
+
+ScaleFactor Operand::get_scale() const {
+
+  if (get_index().is(no_reg))
+    return times_no; // scale makes sense when there is an index
+
+  ASSERT(len_ >= 1);
+  // Operand encodes REX ModR/M [SIB] [Disp].
+  byte modrm = buf_[0];
+  ASSERT(modrm < 0xC0);  // Disallow mode 3 (register target).
+  bool has_sib = ((modrm & 0x07) == 0x04);
+
+  if (!has_sib) 
+    return times_no;
+  
+  int scale = (buf_[1] & 0xC0) >> 6;
+  
+  ASSERT((scale >= times_1) && (scale <= times_8));
+
+  return (ScaleFactor)(scale);
+}
+#endif
+
 // -----------------------------------------------------------------------------
 // Implementation of Assembler.
 
@@ -376,9 +487,14 @@
 
 void Assembler::Align(int m) {
   ASSERT(IsPowerOf2(m));
+#ifdef NACL
+  int delta = (m - (pc_offset() & (m - 1))) & (m - 1);
+  nops(delta);
+#else  
   while ((pc_offset() & (m - 1)) != 0) {
     nop();
   }
+#endif  
 }
 
 
@@ -413,6 +529,12 @@
   bind_to(L, pc_offset());
 }
 
+#ifdef NACL
+void Assembler::bind_aligned(Label* L) {
+  before_bind();
+  bind_to(L, pc_offset());
+}
+#endif
 
 void Assembler::GrowBuffer() {
   ASSERT(buffer_overflow());
@@ -500,15 +622,22 @@
 // Assembler Instruction implementations.
 
 void Assembler::arithmetic_op(byte opcode, Register reg, const Operand& op) {
+#ifdef NACL
+  return arithmetic_op_32(opcode, reg, op);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(reg, op);
   emit(opcode);
   emit_operand(reg, op);
+#endif
 }
 
 
 void Assembler::arithmetic_op(byte opcode, Register reg, Register rm_reg) {
+#ifdef NACL
+  return arithmetic_op_32(opcode, reg, rm_reg); 
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   ASSERT((opcode & 0xC6) == 2);
@@ -522,11 +651,17 @@
     emit(opcode);
     emit_modrm(reg, rm_reg);
   }
+#endif
 }
 
 
 void Assembler::arithmetic_op_16(byte opcode, Register reg, Register rm_reg) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32, 
+      Sandbox::SandboxNone, reg, rm_reg);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   ASSERT((opcode & 0xC6) == 2);
   if (rm_reg.low_bits() == 4) {  // Forces SIB byte.
@@ -547,7 +682,12 @@
 void Assembler::arithmetic_op_16(byte opcode,
                                  Register reg,
                                  const Operand& rm_reg) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32, 
+      Sandbox::SandboxRight, reg, rm_reg);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);
   emit_optional_rex_32(reg, rm_reg);
@@ -557,7 +697,13 @@
 
 
 void Assembler::arithmetic_op_32(byte opcode, Register reg, Register rm_reg) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2,
+      Sandbox::OptionalRex32  | (opcode == 0x3B ? Sandbox::NoRegUseCheck : 0), 
+      (opcode == 0x3B) ? Sandbox::SandboxNone : Sandbox::SandboxLeft, reg, rm_reg);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   ASSERT((opcode & 0xC6) == 2);
   if (rm_reg.low_bits() == 4) {  // Forces SIB byte.
@@ -576,7 +722,13 @@
 void Assembler::arithmetic_op_32(byte opcode,
                                  Register reg,
                                  const Operand& rm_reg) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, 
+      Sandbox::OptionalRex32 | (opcode == 0x3B ? Sandbox::NoRegUseCheck : 0),
+      (opcode == 0x3B) ? Sandbox::SandboxRight : Sandbox::SandboxBoth, reg, rm_reg);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(reg, rm_reg);
   emit(opcode);
@@ -587,6 +739,9 @@
 void Assembler::immediate_arithmetic_op(byte subcode,
                                         Register dst,
                                         Immediate src) {
+#ifdef NACL
+  return immediate_arithmetic_op_32(subcode, dst, src);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
@@ -602,11 +757,16 @@
     emit_modrm(subcode, dst);
     emitl(src.value_);
   }
+#endif
 }
 
+
 void Assembler::immediate_arithmetic_op(byte subcode,
                                         const Operand& dst,
                                         Immediate src) {
+#ifdef NACL
+  return immediate_arithmetic_op_32(subcode, dst, src);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
@@ -619,13 +779,23 @@
     emit_operand(subcode, dst);
     emitl(src.value_);
   }
+#endif
 }
 
 
 void Assembler::immediate_arithmetic_op_16(byte subcode,
                                            Register dst,
                                            Immediate src) {
+#ifdef NACL
+  int size = 1;
+  if (is_int8(src.value_)) size += 3;
+  else if (dst.is(rax)) size += 1 + sizeof(uint16_t);
+  else size += 2 + sizeof(uint16_t);
+  Sandbox sandbox(SANDBOX_DEBUG this, size, Sandbox::OptionalRex32, 
+      Sandbox::SandboxNone, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);  // Operand size override prefix.
   emit_optional_rex_32(dst);
@@ -647,7 +817,12 @@
 void Assembler::immediate_arithmetic_op_16(byte subcode,
                                            const Operand& dst,
                                            Immediate src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2 + src.get_size_8_16(), Sandbox::OptionalRex32, 
+      Sandbox::SandboxRight, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);  // Operand size override prefix.
   emit_optional_rex_32(dst);
@@ -666,7 +841,15 @@
 void Assembler::immediate_arithmetic_op_32(byte subcode,
                                            Register dst,
                                            Immediate src) {
+#ifdef NACL
+  int size = (!is_int8(src.value_) && dst.is(rax)) ? 1 : 2;
+  size += src.get_size_8_32();
+  Sandbox sandbox(SANDBOX_DEBUG this, size, 
+      Sandbox::OptionalRex32 | (subcode == 0x7 ? Sandbox::NoRegUseCheck : 0), 
+      (subcode == 0x7) ? Sandbox::SandboxNone : Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   if (is_int8(src.value_)) {
@@ -687,7 +870,14 @@
 void Assembler::immediate_arithmetic_op_32(byte subcode,
                                            const Operand& dst,
                                            Immediate src) {
+#ifdef NACL
+  int size = 1;
+  size += src.get_size_8_32();
+  Sandbox sandbox(SANDBOX_DEBUG this, size, Sandbox::OptionalRex32, 
+      Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   if (is_int8(src.value_)) {
@@ -705,7 +895,12 @@
 void Assembler::immediate_arithmetic_op_8(byte subcode,
                                           const Operand& dst,
                                           Immediate src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32, 
+      Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   ASSERT(is_int8(src.value_) || is_uint8(src.value_));
@@ -718,7 +913,11 @@
 void Assembler::immediate_arithmetic_op_8(byte subcode,
                                           Register dst,
                                           Immediate src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, dst.code() > 3 ? 4 : 3);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   if (dst.code() > 3) {
     // Use 64-bit mode byte registers.
@@ -732,7 +931,13 @@
 
 
 void Assembler::shift(Register dst, Immediate shift_amount, int subcode) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, shift_amount.value_ == 1 ? 3 : 4, Sandbox::NoFlags,
+      Sandbox::SandboxLeft, dst);
+  //return shift_32(dst, shift_amount, subcode);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   ASSERT(is_uint6(shift_amount.value_));  // illegal shift count
   if (shift_amount.value_ == 1) {
@@ -749,16 +954,25 @@
 
 
 void Assembler::shift(Register dst, int subcode) {
+#ifdef NACL
+  return shift_32(dst, subcode);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xD3);
   emit_modrm(subcode, dst);
+#endif
 }
 
 
 void Assembler::shift_32(Register dst, int subcode) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32, 
+      Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   emit(0xD3);
@@ -767,7 +981,12 @@
 
 
 void Assembler::shift_32(Register dst, Immediate shift_amount, int subcode) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, shift_amount.value_ == 1 ? 2 : 3,
+      Sandbox::OptionalRex32, Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   ASSERT(is_uint5(shift_amount.value_));  // illegal shift count
   if (shift_amount.value_ == 1) {
@@ -784,17 +1003,27 @@
 
 
 void Assembler::bt(const Operand& dst, Register src) {
+#ifdef NACL
+  ASSERT(0); // Illegal instruction in NaCl
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(src, dst);
   emit(0x0F);
   emit(0xA3);
   emit_operand(src, dst);
+#endif
 }
 
 
 void Assembler::bts(const Operand& dst, Register src) {
+#ifdef NACL
+  // PMARCH this inst is illegal
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::NoFlags, 
+      Sandbox::SandboxLeft, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(src, dst);
   emit(0x0F);
@@ -804,7 +1033,11 @@
 
 
 void Assembler::call(Label* L) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 5, Sandbox::CallSandbox);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   // 1110 1000 #32-bit disp.
   emit(0xE8);
@@ -825,7 +1058,11 @@
 
 
 void Assembler::call(Handle<Code> target, RelocInfo::Mode rmode) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 5, Sandbox::CallSandbox); 
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   // 1110 1000 #32-bit disp.
   emit(0xE8);
@@ -834,41 +1071,62 @@
 
 
 void Assembler::call(Register adr) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32 | Sandbox::CallSandbox,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   // Opcode: FF /2 r64.
-  if (adr.high_bit()) {
-    emit_rex_64(adr);
-  }
+  emit_optional_rex_32(adr);
   emit(0xFF);
   emit_modrm(0x2, adr);
 }
 
 
 void Assembler::call(const Operand& op) {
+#ifdef NACL
+  ASSERT(!op.get_base().is(kScratchRegister)
+         && !op.get_index().is(kScratchRegister));
+  movl(kScratchRegister, op);
+  call(kScratchRegister);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   // Opcode: FF /2 m64.
-  emit_rex_64(op);
+  emit_optional_rex_32(op);
   emit(0xFF);
-  emit_operand(2, op);
+  emit_operand(0x2, op);
+#endif
 }
 
 
 void Assembler::clc() {
+#ifdef NACL
+  ASSERT(0); // instruction is not recognised by NaCl
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit(0xF8);
+#endif
 }
 
 void Assembler::cdq() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x99);
 }
 
 
 void Assembler::cmovq(Condition cc, Register dst, Register src) {
+#ifdef NACL
+  return cmovl(cc, dst, src);
+#else
   if (cc == always) {
     movq(dst, src);
   } else if (cc == never) {
@@ -884,10 +1142,14 @@
   emit(0x0f);
   emit(0x40 + cc);
   emit_modrm(dst, src);
+#endif
 }
 
 
 void Assembler::cmovq(Condition cc, Register dst, const Operand& src) {
+#ifdef NACL
+  return cmovl(cc, dst, src);
+#else
   if (cc == always) {
     movq(dst, src);
   } else if (cc == never) {
@@ -901,6 +1163,7 @@
   emit(0x0f);
   emit(0x40 + cc);
   emit_operand(dst, src);
+#endif
 }
 
 
@@ -911,7 +1174,12 @@
     return;
   }
   ASSERT(cc >= 0);
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32, 
+      Sandbox::SandboxLeft, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   // Opcode: 0f 40 + cc /r.
   emit_optional_rex_32(dst, src);
@@ -928,7 +1196,12 @@
     return;
   }
   ASSERT(cc >= 0);
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32, 
+      Sandbox::SandboxBoth, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   // Opcode: 0f 40 + cc /r.
   emit_optional_rex_32(dst, src);
@@ -940,7 +1213,11 @@
 
 void Assembler::cmpb_al(Immediate imm8) {
   ASSERT(is_int8(imm8.value_) || is_uint8(imm8.value_));
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x3c);
   emit(imm8.value_);
@@ -949,7 +1226,11 @@
 
 void Assembler::cpuid() {
   ASSERT(CpuFeatures::IsEnabled(CPUID));
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x0F);
   emit(0xA2);
@@ -957,7 +1238,11 @@
 
 
 void Assembler::cqo() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64();
   emit(0x99);
@@ -965,25 +1250,38 @@
 
 
 void Assembler::decq(Register dst) {
+#ifdef NACL
+  return decl(dst);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xFF);
   emit_modrm(0x1, dst);
+#endif
 }
 
 
 void Assembler::decq(const Operand& dst) {
+#ifdef NACL
+  return decl(dst);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xFF);
   emit_operand(1, dst);
+#endif
 }
 
 
 void Assembler::decl(Register dst) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   emit(0xFF);
@@ -992,7 +1290,12 @@
 
 
 void Assembler::decl(const Operand& dst) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   emit(0xFF);
@@ -1001,7 +1304,11 @@
 
 
 void Assembler::decb(Register dst) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, dst.code() > 3 ? 3 : 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   if (dst.code() > 3) {
     // Register is not one of al, bl, cl, dl.  Its encoding needs REX.
@@ -1013,7 +1320,12 @@
 
 
 void Assembler::decb(const Operand& dst) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32, 
+      Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   emit(0xFE);
@@ -1022,32 +1334,49 @@
 
 
 void Assembler::enter(Immediate size) {
+#ifdef NACL
+  ASSERT(0); // not supported by NaCl
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit(0xC8);
   emitw(size.value_);  // 16 bit operand, always.
   emit(0);
+#endif
 }
 
 
 void Assembler::hlt() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF4);
 }
 
 
 void Assembler::idivq(Register src) {
+#ifdef NACL
+  return idivl(src);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(src);
   emit(0xF7);
   emit_modrm(0x7, src);
+#endif
 }
 
 
 void Assembler::idivl(Register src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, 
+      Sandbox::OptionalRex32, Sandbox::SandboxNone, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(src);
   emit(0xF7);
@@ -1056,35 +1385,50 @@
 
 
 void Assembler::imul(Register src) {
+#ifdef NACL
+  ASSERT(0); // call to imull()
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(src);
   emit(0xF7);
   emit_modrm(0x5, src);
+#endif
 }
 
 
 void Assembler::imul(Register dst, Register src) {
+#ifdef NACL
+  return imull(dst, src);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst, src);
   emit(0x0F);
   emit(0xAF);
   emit_modrm(dst, src);
+#endif
 }
 
 
 void Assembler::imul(Register dst, const Operand& src) {
+#ifdef NACL
+  ASSERT(0); // call to imull(dst, src);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst, src);
   emit(0x0F);
   emit(0xAF);
   emit_operand(dst, src);
+#endif
 }
 
 
 void Assembler::imul(Register dst, Register src, Immediate imm) {
+#ifdef NACL
+  return imull(dst, src, imm);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst, src);
@@ -1097,11 +1441,17 @@
     emit_modrm(dst, src);
     emitl(imm.value_);
   }
+#endif
 }
 
 
 void Assembler::imull(Register dst, Register src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32, 
+      Sandbox::SandboxLeft, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst, src);
   emit(0x0F);
@@ -1111,7 +1461,12 @@
 
 
 void Assembler::imull(Register dst, Register src, Immediate imm) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2 + imm.get_size_8_32(), 
+      Sandbox::OptionalRex32, Sandbox::SandboxLeft, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst, src);
   if (is_int8(imm.value_)) {
@@ -1127,25 +1482,38 @@
 
 
 void Assembler::incq(Register dst) {
+#ifdef NACL
+  return incl(dst);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xFF);
   emit_modrm(0x0, dst);
+#endif
 }
 
 
 void Assembler::incq(const Operand& dst) {
+#ifdef NACL
+  return incl(dst);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xFF);
   emit_operand(0, dst);
+#endif
 }
 
 
 void Assembler::incl(const Operand& dst) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32, 
+      Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   emit(0xFF);
@@ -1154,7 +1522,12 @@
 
 
 void Assembler::incl(Register dst) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32, 
+      Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   emit(0xFF);
@@ -1163,9 +1536,13 @@
 
 
 void Assembler::int3() {
-  EnsureSpace ensure_space(this);
+#ifdef NACL
+  hlt();
+#else  
+  EnsureSpace ensure_space(this);  
   last_pc_ = pc_;
   emit(0xCC);
+#endif  
 }
 
 
@@ -1176,7 +1553,18 @@
   } else if (cc == never) {
     return;
   }
+
+#ifdef NACL
+  int size;
+  if (L->is_bound() &&
+      is_int8((L->pos() - pc_offset()) - 2))
+    size = 2;
+  else
+    size = 6;
+  Sandbox sandbox(SANDBOX_DEBUG this, size);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   ASSERT(is_uint4(cc));
   if (L->is_bound()) {
@@ -1214,7 +1602,11 @@
 void Assembler::j(Condition cc,
                   Handle<Code> target,
                   RelocInfo::Mode rmode) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2 + sizeof(uint32_t));
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   ASSERT(is_uint4(cc));
   // 0000 1111 1000 tttn #32-bit disp.
@@ -1225,7 +1617,17 @@
 
 
 void Assembler::jmp(Label* L) {
+#ifdef NACL
+  int size;
+  if (L->is_bound() 
+      && is_int8((L->pos() - pc_offset() - 1) -  sizeof(int8_t)))
+    size = 2;
+  else
+    size = 5;
+  Sandbox sandbox(SANDBOX_DEBUG this, size);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   const int short_size = sizeof(int8_t);
   const int long_size = sizeof(int32_t);
@@ -1258,7 +1660,11 @@
 
 
 void Assembler::jmp(Handle<Code> target, RelocInfo::Mode rmode) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1 + sizeof(uint32_t));
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   // 1110 1001 #32-bit disp.
   emit(0xE9);
@@ -1267,29 +1673,47 @@
 
 
 void Assembler::jmp(Register target) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32 | Sandbox::JumpSandbox,
+      Sandbox::SandboxLeft, target); 
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   // Opcode FF/4 r64.
-  if (target.high_bit()) {
-    emit_rex_64(target);
-  }
+  emit_optional_rex_32(target);
   emit(0xFF);
   emit_modrm(0x4, target);
 }
 
 
 void Assembler::jmp(const Operand& src) {
+#ifdef NACL
+  ASSERT(!src.get_base().is(kScratchRegister)
+         && !src.get_index().is(kScratchRegister));
+  movq(kScratchRegister, src);
+  jmp(kScratchRegister);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   // Opcode FF/4 m64.
   emit_optional_rex_32(src);
   emit(0xFF);
   emit_operand(0x4, src);
+#endif
 }
 
 
 void Assembler::lea(Register dst, const Operand& src) {
+#ifdef NACL
+  if (is_rsp_rbp(dst))
+    return leal(dst, src);
+
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::NoFlags,
+      Sandbox::SandboxLeft, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(dst, src);
   emit(0x8D);
@@ -1298,7 +1722,12 @@
 
 
 void Assembler::leal(Register dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32, 
+      Sandbox::SandboxLeft, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst, src);
   emit(0x8D);
@@ -1307,11 +1736,15 @@
 
 
 void Assembler::load_rax(void* value, RelocInfo::Mode mode) {
+#ifdef NACL
+  return movq(rax, Operand(r15, reinterpret_cast<uint32_t>(value)));
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit(0x48);  // REX.W
   emit(0xA1);
   emitq(reinterpret_cast<uintptr_t>(value), mode);
+#endif
 }
 
 
@@ -1321,14 +1754,23 @@
 
 
 void Assembler::leave() {
+#ifdef NACL
+  ASSERT(0); // NaCl deos not allow leave inst
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit(0xC9);
+#endif
 }
 
 
 void Assembler::movb(Register dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::NoFlags,
+      Sandbox::SandboxRight, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_32(dst, src);
   emit(0x8A);
@@ -1337,7 +1779,11 @@
 
 
 void Assembler::movb(Register dst, Immediate imm) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_32(dst);
   emit(0xC6);
@@ -1347,7 +1793,12 @@
 
 
 void Assembler::movb(const Operand& dst, Register src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::NoFlags,
+      Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_32(src, dst);
   emit(0x88);
@@ -1356,7 +1807,12 @@
 
 
 void Assembler::movw(const Operand& dst, Register src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);
   emit_optional_rex_32(src, dst);
@@ -1366,7 +1822,12 @@
 
 
 void Assembler::movl(Register dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxBoth, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst, src);
   emit(0x8B);
@@ -1375,7 +1836,12 @@
 
 
 void Assembler::movl(Register dst, Register src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   if (src.low_bits() == 4) {
     emit_optional_rex_32(src, dst);
@@ -1389,8 +1855,14 @@
 }
 
 
+#ifdef NACL
+void Assembler::movl(const Operand& dst, Register src, int flags) {
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32 | flags,
+      Sandbox::SandboxLeft, dst, src);
+#else
 void Assembler::movl(const Operand& dst, Register src) {
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(src, dst);
   emit(0x89);
@@ -1399,7 +1871,12 @@
 
 
 void Assembler::movl(const Operand& dst, Immediate value) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1 + sizeof(uint32_t),
+      Sandbox::OptionalRex32, Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   emit(0xC7);
@@ -1409,7 +1886,12 @@
 
 
 void Assembler::movl(Register dst, Immediate value) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2 + sizeof(uint32_t),
+      Sandbox::OptionalRex32, Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   emit(0xC7);
@@ -1419,7 +1901,12 @@
 
 
 void Assembler::movq(Register dst, const Operand& src) {
+#ifdef NACL
+  return movl(dst, src);
+//  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::NoFlags, Sandbox::SandboxBoth, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(dst, src);
   emit(0x8B);
@@ -1428,7 +1915,15 @@
 
 
 void Assembler::movq(Register dst, Register src) {
+#ifdef NACL
+  if (!(is_rsp_rbp(dst) && is_rsp_rbp(src)))
+    return movl(dst, src);
+  // allow rsp = rbp
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::NoFlags | Sandbox::NoRegUseCheck, 
+      (is_rsp_rbp(dst) && is_rsp_rbp(src)) ? Sandbox::SandboxNone : Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   if (src.low_bits() == 4) {
     emit_rex_64(src, dst);
@@ -1443,7 +1938,12 @@
 
 
 void Assembler::movq(Register dst, Immediate value) {
+#ifdef NACL
+  return movl(dst, value);
+//  Sandbox sandbox(SANDBOX_DEBUG this, 7, Sandbox::NoFlags, Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xC7);
@@ -1453,19 +1953,28 @@
 
 
 void Assembler::movq(const Operand& dst, Register src) {
+#ifdef NACL
+  return movl(dst, src);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(src, dst);
   emit(0x89);
   emit_operand(src, dst);
+#endif
 }
 
 
 void Assembler::movq(Register dst, void* value, RelocInfo::Mode rmode) {
-  // This method must not be used with heap object references. The stored
-  // address is not GC safe. Use the handle version instead.
+#ifdef NACL
+  return movl(dst, value, rmode);
+#endif
   ASSERT(rmode > RelocInfo::LAST_GCED_ENUM);
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 10, Sandbox::NoFlags, Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xB8 | dst.low_bits());
@@ -1474,6 +1983,9 @@
 
 
 void Assembler::movq(Register dst, int64_t value, RelocInfo::Mode rmode) {
+#ifdef NACL
+  return movl(dst, value, rmode);
+#endif
   // Non-relocatable values might not need a 64-bit representation.
   if (rmode == RelocInfo::NONE) {
     // Sadly, there is no zero or sign extending move for 8-bit immediates.
@@ -1487,7 +1999,11 @@
     // Value cannot be represented by 32 bits, so do a full 64 bit immediate
     // value.
   }
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 10, Sandbox::NoFlags, Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xB8 | dst.low_bits());
@@ -1496,7 +2012,12 @@
 
 
 void Assembler::movq(Register dst, ExternalReference ref) {
+#ifdef NACL
+  return movl(dst, ref);
+//  Sandbox sandbox(SANDBOX_DEBUG this, 10, Sandbox::NoFlags, Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xB8 | dst.low_bits());
@@ -1506,19 +2027,28 @@
 
 
 void Assembler::movq(const Operand& dst, Immediate value) {
+#ifdef NACL
+  return movl(dst, value);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xC7);
   emit_operand(0, dst);
   emit(value);
+#endif
 }
 
 
 // Loads the ip-relative location of the src label into the target location
 // (as a 32-bit offset sign extended to 64-bit).
 void Assembler::movl(const Operand& dst, Label* src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1 + sizeof(uint32_t),
+      Sandbox::OptionalRex32, Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   emit(0xC7);
@@ -1540,6 +2070,9 @@
 
 
 void Assembler::movq(Register dst, Handle<Object> value, RelocInfo::Mode mode) {
+#ifdef NACL
+  return movl(dst, value, mode);
+#endif
   // If there is no relocation info, emit the value of the handle efficiently
   // (possibly using less that 8 bytes for the value).
   if (mode == RelocInfo::NONE) {
@@ -1548,7 +2081,11 @@
     ASSERT(value->IsSmi());
     movq(dst, reinterpret_cast<int64_t>(*value), RelocInfo::NONE);
   } else {
+#ifdef NACL
+    Sandbox sandbox(SANDBOX_DEBUG this, 10, Sandbox::NoFlags, Sandbox::SandboxLeft, dst);
+#else
     EnsureSpace ensure_space(this);
+#endif
     last_pc_ = pc_;
     ASSERT(value->IsHeapObject());
     ASSERT(!Heap::InNewSpace(*value));
@@ -1560,7 +2097,12 @@
 
 
 void Assembler::movsxbq(Register dst, const Operand& src) {
+#ifdef NACL
+//  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::NoFlags, Sandbox::SandboxBoth, dst, src);
+  return movsxbl(dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_32(dst, src);
   emit(0x0F);
@@ -1570,7 +2112,12 @@
 
 
 void Assembler::movsxwq(Register dst, const Operand& src) {
+#ifdef NACL
+//  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::NoFlags, Sandbox::SandboxBoth, dst, src);
+  return movsxwl(dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(dst, src);
   emit(0x0F);
@@ -1580,7 +2127,13 @@
 
 
 void Assembler::movsxlq(Register dst, Register src) {
+#ifdef NACL
+//  ASSERT(!(is_rsp_rbp(dst) && is_rsp_rbp(src))); // rsp = rbp may be allowed ?
+//  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::NoFlags, Sandbox::SandboxLeft, dst);
+  ASSERT(0); // should not be used in NACL
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(dst, src);
   emit(0x63);
@@ -1589,7 +2142,12 @@
 
 
 void Assembler::movsxlq(Register dst, const Operand& src) {
+#ifdef NACL
+//  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::NoFlags, Sandbox::SandboxBoth, dst, src);
+  return movl(dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(dst, src);
   emit(0x63);
@@ -1598,7 +2156,12 @@
 
 
 void Assembler::movzxbq(Register dst, const Operand& src) {
+#ifdef NACL
+//  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::NoFlags, Sandbox::SandboxBoth, dst, src);
+  return movzxbl(dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(dst, src);
   emit(0x0F);
@@ -1608,7 +2171,12 @@
 
 
 void Assembler::movzxbl(Register dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32, 
+      Sandbox::SandboxBoth, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst, src);
   emit(0x0F);
@@ -1618,7 +2186,12 @@
 
 
 void Assembler::movzxwq(Register dst, const Operand& src) {
+#ifdef NACL
+//  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::NoFlags, Sandbox::SandboxBoth, dst, src);
+  return movzxwl(dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(dst, src);
   emit(0x0F);
@@ -1628,7 +2201,12 @@
 
 
 void Assembler::movzxwl(Register dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32,
+      Sandbox::SandboxBoth, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst, src);
   emit(0x0F);
@@ -1638,7 +2216,11 @@
 
 
 void Assembler::repmovsb() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::RepMovsSandbox);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF3);
   emit(0xA4);
@@ -1646,7 +2228,11 @@
 
 
 void Assembler::repmovsw() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::RepMovsSandbox);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);  // Operand size override.
   emit(0xF3);
@@ -1655,7 +2241,11 @@
 
 
 void Assembler::repmovsl() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::RepMovsSandbox);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF3);
   emit(0xA5);
@@ -1663,7 +2253,11 @@
 
 
 void Assembler::repmovsq() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::RepMovsSandbox);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF3);
   emit_rex_64();
@@ -1672,25 +2266,38 @@
 
 
 void Assembler::mul(Register src) {
+#ifdef NACL
+  ASSERT(0); // call to mull(src);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(src);
   emit(0xF7);
   emit_modrm(0x4, src);
+#endif
 }
 
 
 void Assembler::neg(Register dst) {
+#ifdef NACL
+  return negl(dst);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xF7);
   emit_modrm(0x3, dst);
+#endif
 }
 
 
 void Assembler::negl(Register dst) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32, 
+      Sandbox::SandboxLeft, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   emit(0xF7);
@@ -1699,41 +2306,62 @@
 
 
 void Assembler::neg(const Operand& dst) {
+#ifdef NACL
+  ASSERT(0); // call to negl(dst);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xF7);
   emit_operand(3, dst);
+#endif
 }
 
 
 void Assembler::nop() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x90);
 }
 
 
 void Assembler::not_(Register dst) {
+#ifdef NACL
+  return notl(dst);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xF7);
   emit_modrm(0x2, dst);
+#endif
 }
 
 
 void Assembler::not_(const Operand& dst) {
+#ifdef NACL
+  ASSERT(0); // call notl()
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(dst);
   emit(0xF7);
   emit_operand(2, dst);
+#endif
 }
 
 
 void Assembler::notl(Register dst) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(dst);
   emit(0xF7);
@@ -1758,7 +2386,11 @@
 
   ASSERT(1 <= n);
   ASSERT(n <= 9);
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   switch (n) {
   case 1:
@@ -1828,53 +2460,86 @@
 }
 
 
+#ifdef NACL
+void Assembler::pop(Register dst, int flags) {
+  // NACL forbits pop(rsp/rbp)
+  if (is_rsp_rbp(dst)) {
+    movl(dst, Operand(rsp, 0));
+    addl(rsp, Immediate(kStackPointerSize));
+    return;
+  }
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32 | flags,
+      Sandbox::SandboxNone, dst);
+#else
 void Assembler::pop(Register dst) {
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
-  if (dst.high_bit()) {
-    emit_rex_64(dst);
-  }
+  emit_optional_rex_32(dst);
   emit(0x58 | dst.low_bits());
 }
 
 
 void Assembler::pop(const Operand& dst) {
+#ifdef NACL
+  // pop stores 8 bytes which can corrupt data in NACL's 32-bit model
+  pop(scratch_reg, Sandbox::NoRegUseCheck);
+  movl(dst, scratch_reg, Sandbox::NoRegUseCheck);
+  return;
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
-  emit_rex_64(dst);  // Could be omitted in some cases.
+  emit_optional_rex_32(dst);
   emit(0x8F);
   emit_operand(0, dst);
+#endif
 }
 
 
 void Assembler::popfq() {
+#ifdef NACL
+  ASSERT(0);  // Illegal instruction
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit(0x9D);
+#endif
 }
 
 
 void Assembler::push(Register src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32 | Sandbox::NoRegUseCheck,
+      Sandbox::SandboxNone, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
-  if (src.high_bit()) {
-    emit_rex_64(src);
-  }
+  emit_optional_rex_32(src);
   emit(0x50 | src.low_bits());
 }
 
 
-void Assembler::push(const Operand& src) {
+void Assembler::push(const Operand& src) {        
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
-  emit_rex_64(src);  // Could be omitted in some cases.
+  emit_optional_rex_32(src);
   emit(0xFF);
   emit_operand(6, src);
 }
 
 
 void Assembler::push(Immediate value) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1 + value.get_size_8_32());
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   if (is_int8(value.value_)) {
     emit(0x6A);
@@ -1887,21 +2552,37 @@
 
 
 void Assembler::pushfq() {
+#ifdef NACL
+  ASSERT(0); // Illegal instruction
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit(0x9C);
+#endif
 }
 
 
 void Assembler::rdtsc() {
+#ifdef NACL
+  // instruction is not recognised by NaCl
+  ASSERT(0);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit(0x0F);
   emit(0x31);
+#endif
 }
 
 
 void Assembler::ret(int imm16) {
+#ifdef NACL
+  ASSERT(is_uint16(imm16));
+  pop(kScratchRegister);
+  if (imm16 != 0)
+    addl(rsp, Immediate(imm16));
+  jmp(kScratchRegister);
+#else 
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   ASSERT(is_uint16(imm16));
@@ -1912,6 +2593,7 @@
     emit(imm16 & 0xFF);
     emit((imm16 >> 8) & 0xFF);
   }
+#endif
 }
 
 
@@ -1920,7 +2602,11 @@
     movb(reg, Immediate(cc == always ? 1 : 0));
     return;
   }
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, reg.code() > 3 ? 4 : 3);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   ASSERT(is_uint4(cc));
   if (reg.code() > 3) {  // Use x64 byte registers, where different.
@@ -1933,7 +2619,11 @@
 
 
 void Assembler::shld(Register dst, Register src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(src, dst);
   emit(0x0F);
@@ -1943,7 +2633,11 @@
 
 
 void Assembler::shrd(Register dst, Register src) {
+#ifdef NACL
+  ASSERT(0); // this instruction is not recognized by NaCl validator
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_rex_64(src, dst);
   emit(0x0F);
@@ -1953,7 +2647,13 @@
 
 
 void Assembler::xchg(Register dst, Register src) {
+#ifdef NACL
+  ASSERT(!is_rsp_rbp(dst) && !is_rsp_rbp(src));
+  int size = (src.is(rax) || dst.is(rax)) ? 2 : 3;
+  Sandbox sandbox(SANDBOX_DEBUG this, size);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   if (src.is(rax) || dst.is(rax)) {  // Single-byte encoding
     Register other = src.is(rax) ? dst : src;
@@ -1972,11 +2672,15 @@
 
 
 void Assembler::store_rax(void* dst, RelocInfo::Mode mode) {
+#ifdef NACL
+  movq(Operand(r15, reinterpret_cast<uint32_t>(dst)), rax);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit(0x48);  // REX.W
   emit(0xA3);
   emitq(reinterpret_cast<uintptr_t>(dst), mode);
+#endif
 }
 
 
@@ -1986,7 +2690,12 @@
 
 
 void Assembler::testb(Register dst, Register src) {
+#ifdef NACL
+  int size = ((src.low_bits() != 4) && !(dst.code() > 3 || src.code() > 3)) ? 2 : 3;
+  Sandbox sandbox(SANDBOX_DEBUG this, size);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   if (src.low_bits() == 4) {
     emit_rex_32(src, dst);
@@ -2005,7 +2714,12 @@
 
 void Assembler::testb(Register reg, Immediate mask) {
   ASSERT(is_int8(mask.value_) || is_uint8(mask.value_));
+#ifdef NACL
+  int size = reg.is(rax) ? 2 : (reg.code() > 3 ? 4 : 3);
+  Sandbox sandbox(SANDBOX_DEBUG this, size);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   if (reg.is(rax)) {
     emit(0xA8);
@@ -2024,7 +2738,12 @@
 
 void Assembler::testb(const Operand& op, Immediate mask) {
   ASSERT(is_int8(mask.value_) || is_uint8(mask.value_));
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, op);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(rax, op);
   emit(0xF6);
@@ -2032,9 +2751,15 @@
   emit(mask.value_);  // Low byte emitted.
 }
 
-
 void Assembler::testb(const Operand& op, Register reg) {
+#ifdef NACL
+  int flags;
+  reg.code() > 3 ? flags = Sandbox::NoFlags : flags = Sandbox::OptionalRex32;
+  Sandbox sandbox(SANDBOX_DEBUG this, reg.code() > 3 ? 2 : 1, flags, 
+      Sandbox::SandboxLeft, op, reg);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   if (reg.code() > 3) {
     // Register is not one of al, bl, cl, dl.  Its encoding needs REX.
@@ -2048,7 +2773,12 @@
 
 
 void Assembler::testl(Register dst, Register src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   if (src.low_bits() == 4) {
     emit_optional_rex_32(src, dst);
@@ -2068,7 +2798,14 @@
     testb(reg, mask);
     return;
   }
+#ifdef NACL
+  int flags;
+  reg.is(rax) ? flags = Sandbox::NoFlags : flags = Sandbox::OptionalRex32;
+  Sandbox sandbox(SANDBOX_DEBUG this, sizeof(uint32_t) + (reg.is(rax) ? 1 : 2),
+      flags, Sandbox::SandboxNone, reg, rax);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   if (reg.is(rax)) {
     emit(0xA9);
@@ -2088,7 +2825,12 @@
     testb(op, mask);
     return;
   }
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1 + sizeof(uint32_t), Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, op);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(rax, op);
   emit(0xF7);
@@ -2098,15 +2840,22 @@
 
 
 void Assembler::testq(const Operand& op, Register reg) {
+#ifdef NACL
+  ASSERT(0); // call testl()
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   emit_rex_64(reg, op);
   emit(0x85);
   emit_operand(reg, op);
+#endif
 }
 
 
 void Assembler::testq(Register dst, Register src) {
+#ifdef NACL
+  return testl(dst, src);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   if (src.low_bits() == 4) {
@@ -2118,10 +2867,14 @@
     emit(0x85);
     emit_modrm(dst, src);
   }
+#endif
 }
 
 
 void Assembler::testq(Register dst, Immediate mask) {
+#ifdef NACL
+  return testl(dst, mask);
+#else
   EnsureSpace ensure_space(this);
   last_pc_ = pc_;
   if (dst.is(rax)) {
@@ -2134,6 +2887,7 @@
     emit_modrm(0, dst);
     emit(mask);
   }
+#endif
 }
 
 
@@ -2141,14 +2895,22 @@
 
 
 void Assembler::fld(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xD9, 0xC0, i);
 }
 
 
 void Assembler::fld1() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xE8);
@@ -2156,7 +2918,11 @@
 
 
 void Assembler::fldz() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xEE);
@@ -2164,7 +2930,11 @@
 
 
 void Assembler::fldpi() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xEB);
@@ -2172,7 +2942,12 @@
 
 
 void Assembler::fld_s(const Operand& adr) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xD9);
@@ -2181,7 +2956,12 @@
 
 
 void Assembler::fld_d(const Operand& adr) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xDD);
@@ -2190,7 +2970,12 @@
 
 
 void Assembler::fstp_s(const Operand& adr) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xD9);
@@ -2199,7 +2984,12 @@
 
 
 void Assembler::fstp_d(const Operand& adr) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xDD);
@@ -2209,14 +2999,23 @@
 
 void Assembler::fstp(int index) {
   ASSERT(is_uint3(index));
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDD, 0xD8, index);
 }
 
 
 void Assembler::fild_s(const Operand& adr) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xDB);
@@ -2225,7 +3024,12 @@
 
 
 void Assembler::fild_d(const Operand& adr) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xDF);
@@ -2234,7 +3038,12 @@
 
 
 void Assembler::fistp_s(const Operand& adr) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xDB);
@@ -2244,7 +3053,12 @@
 
 void Assembler::fisttp_s(const Operand& adr) {
   ASSERT(CpuFeatures::IsEnabled(SSE3));
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xDB);
@@ -2254,7 +3068,12 @@
 
 void Assembler::fisttp_d(const Operand& adr) {
   ASSERT(CpuFeatures::IsEnabled(SSE3));
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xDD);
@@ -2263,7 +3082,12 @@
 
 
 void Assembler::fist_s(const Operand& adr) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xDB);
@@ -2272,7 +3096,12 @@
 
 
 void Assembler::fistp_d(const Operand& adr) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xDF);
@@ -2281,7 +3110,11 @@
 
 
 void Assembler::fabs() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xE1);
@@ -2289,7 +3122,11 @@
 
 
 void Assembler::fchs() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xE0);
@@ -2297,7 +3134,11 @@
 
 
 void Assembler::fcos() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xFF);
@@ -2305,7 +3146,11 @@
 
 
 void Assembler::fsin() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xFE);
@@ -2313,21 +3158,34 @@
 
 
 void Assembler::fadd(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDC, 0xC0, i);
 }
 
 
 void Assembler::fsub(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDC, 0xE8, i);
 }
 
 
 void Assembler::fisub_s(const Operand& adr) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, adr);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_optional_rex_32(adr);
   emit(0xDA);
@@ -2336,56 +3194,88 @@
 
 
 void Assembler::fmul(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDC, 0xC8, i);
 }
 
 
 void Assembler::fdiv(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDC, 0xF8, i);
 }
 
 
 void Assembler::faddp(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDE, 0xC0, i);
 }
 
 
 void Assembler::fsubp(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDE, 0xE8, i);
 }
 
 
 void Assembler::fsubrp(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDE, 0xE0, i);
 }
 
 
 void Assembler::fmulp(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDE, 0xC8, i);
 }
 
 
 void Assembler::fdivp(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDE, 0xF8, i);
 }
 
 
 void Assembler::fprem() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xF8);
@@ -2393,7 +3283,11 @@
 
 
 void Assembler::fprem1() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xF5);
@@ -2401,14 +3295,22 @@
 
 
 void Assembler::fxch(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xD9, 0xC8, i);
 }
 
 
 void Assembler::fincstp() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xF7);
@@ -2416,14 +3318,22 @@
 
 
 void Assembler::ffree(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDD, 0xC0, i);
 }
 
 
 void Assembler::ftst() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xE4);
@@ -2431,14 +3341,22 @@
 
 
 void Assembler::fucomp(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit_farith(0xDD, 0xE8, i);
 }
 
 
 void Assembler::fucompp() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xDA);
   emit(0xE9);
@@ -2446,7 +3364,11 @@
 
 
 void Assembler::fucomi(int i) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xDB);
   emit(0xE8 + i);
@@ -2454,7 +3376,11 @@
 
 
 void Assembler::fucomip() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xDF);
   emit(0xE9);
@@ -2462,7 +3388,11 @@
 
 
 void Assembler::fcompp() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xDE);
   emit(0xD9);
@@ -2470,7 +3400,11 @@
 
 
 void Assembler::fnstsw_ax() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xDF);
   emit(0xE0);
@@ -2478,14 +3412,22 @@
 
 
 void Assembler::fwait() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 1);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x9B);
 }
 
 
 void Assembler::frndint() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xD9);
   emit(0xFC);
@@ -2493,7 +3435,11 @@
 
 
 void Assembler::fnclex() {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 2);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xDB);
   emit(0xE2);
@@ -2503,7 +3449,11 @@
 void Assembler::sahf() {
   // TODO(X64): Test for presence. Not all 64-bit intel CPU's have sahf
   // in 64-bit mode. Test CpuID.
+#ifdef NACL
+  ASSERT(0); // this instruction is not supported by NaCl
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x9E);
 }
@@ -2519,7 +3469,12 @@
 // SSE 2 operations.
 
 void Assembler::movd(XMMRegister dst, Register src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);
   emit_optional_rex_32(dst, src);
@@ -2530,7 +3485,12 @@
 
 
 void Assembler::movd(Register dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);
   emit_optional_rex_32(dst, src);
@@ -2541,7 +3501,12 @@
 
 
 void Assembler::movq(XMMRegister dst, Register src) {
+#ifdef NACL
+  return movl(dst, src);
+//  Sandbox sandbox(SANDBOX_DEBUG this, 5);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);
   emit_rex_64(dst, src);
@@ -2552,7 +3517,11 @@
 
 
 void Assembler::movq(Register dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 5);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);
   emit_rex_64(dst, src);
@@ -2564,7 +3533,12 @@
 
 void Assembler::extractps(Register dst, XMMRegister src, byte imm8) {
   ASSERT(is_uint2(imm8));
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 6, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);
   emit_optional_rex_32(dst, src);
@@ -2577,7 +3551,12 @@
 
 
 void Assembler::movsd(const Operand& dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);  // double
   emit_optional_rex_32(src, dst);
@@ -2588,7 +3567,12 @@
 
 
 void Assembler::movsd(XMMRegister dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);  // double
   emit_optional_rex_32(dst, src);
@@ -2599,7 +3583,12 @@
 
 
 void Assembler::movsd(XMMRegister dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32,
+      Sandbox::SandboxRight, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);  // double
   emit_optional_rex_32(dst, src);
@@ -2609,8 +3598,45 @@
 }
 
 
+void Assembler::movss(XMMRegister dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32,
+      Sandbox::SandboxRight, dst, src);
+#else
+  EnsureSpace ensure_space(this);
+#endif
+  last_pc_ = pc_;
+  emit(0xF3);  // single
+  emit_optional_rex_32(dst, src);
+  emit(0x0F);
+  emit(0x10);  // load
+  emit_sse_operand(dst, src);
+}
+
+
+void Assembler::movss(const Operand& src, XMMRegister dst) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, src, dst);
+#else
+  EnsureSpace ensure_space(this);
+#endif
+  last_pc_ = pc_;
+  emit(0xF3);  // single
+  emit_optional_rex_32(dst, src);
+  emit(0x0F);
+  emit(0x11);  // store
+  emit_sse_operand(dst, src);
+}
+
+
 void Assembler::cvttss2si(Register dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32,
+      Sandbox::SandboxRight, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF3);
   emit_optional_rex_32(dst, src);
@@ -2621,7 +3647,12 @@
 
 
 void Assembler::cvttsd2si(Register dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32,
+      Sandbox::SandboxRight, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);
   emit_optional_rex_32(dst, src);
@@ -2632,7 +3663,13 @@
 
 
 void Assembler::cvttsd2siq(Register dst, XMMRegister src) {
+#ifdef NACL
+  // PMARCH do not convert this to cvttsd2sil as it will fail for float number
+  // with exponent >31
+  Sandbox sandbox(SANDBOX_DEBUG this, 5);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);
   emit_rex_64(dst, src);
@@ -2642,8 +3679,29 @@
 }
 
 
+void Assembler::cvttsd2siq(Register dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::NoFlags,
+      Sandbox::SandboxLeft, src);
+#else
+  EnsureSpace ensure_space(this);
+#endif
+  last_pc_ = pc_;
+  emit(0xF2);
+  emit_rex_64(dst, src);
+  emit(0x0F);
+  emit(0x2C);
+  emit_operand(dst, src);
+}
+
+
 void Assembler::cvtlsi2sd(XMMRegister dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32,
+      Sandbox::SandboxRight, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);
   emit_optional_rex_32(dst, src);
@@ -2654,7 +3712,12 @@
 
 
 void Assembler::cvtlsi2sd(XMMRegister dst, Register src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);
   emit_optional_rex_32(dst, src);
@@ -2664,8 +3727,29 @@
 }
 
 
+void Assembler::cvtlsi2ss(XMMRegister dst, Register src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
+  EnsureSpace ensure_space(this);
+#endif
+  last_pc_ = pc_;
+  emit(0xF3);
+  emit_optional_rex_32(dst, src);
+  emit(0x0F);
+  emit(0x2A);
+  emit_sse_operand(dst, src);
+}
+
+
 void Assembler::cvtqsi2sd(XMMRegister dst, Register src) {
+#ifdef NACL
+  // PMARCH do not covert this to cvtlsi2sd, see uses
+  Sandbox sandbox(SANDBOX_DEBUG this, 5);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);
   emit_rex_64(dst, src);
@@ -2676,7 +3760,12 @@
 
 
 void Assembler::cvtss2sd(XMMRegister dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF3);
   emit_optional_rex_32(dst, src);
@@ -2686,8 +3775,76 @@
 }
 
 
+void Assembler::cvtss2sd(XMMRegister dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32,
+      Sandbox::SandboxRight, dst, src);
+#else
+  EnsureSpace ensure_space(this);
+#endif
+  last_pc_ = pc_;
+  emit(0xF3);
+  emit_optional_rex_32(dst, src);
+  emit(0x0F);
+  emit(0x5A);
+  emit_sse_operand(dst, src);
+}
+
+
+void Assembler::cvtsd2ss(XMMRegister dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
+  EnsureSpace ensure_space(this);
+#endif
+  last_pc_ = pc_;
+  emit(0xF2);
+  emit_optional_rex_32(dst, src);
+  emit(0x0F);
+  emit(0x5A);
+  emit_sse_operand(dst, src);
+}
+
+
+void Assembler::cvtsd2si(Register dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
+  EnsureSpace ensure_space(this);
+#endif
+  last_pc_ = pc_;
+  emit(0xF2);
+  emit_optional_rex_32(dst, src);
+  emit(0x0F);
+  emit(0x2D);
+  emit_sse_operand(dst, src);
+}
+
+
+void Assembler::cvtsd2siq(Register dst, XMMRegister src) {
+#ifdef NACL
+  return cvtsd2si(dst, src);
+#else
+  EnsureSpace ensure_space(this);
+  last_pc_ = pc_;
+  emit(0xF2);
+  emit_rex_64(dst, src);
+  emit(0x0F);
+  emit(0x2D);
+  emit_sse_operand(dst, src);
+#endif
+}
+
+
 void Assembler::addsd(XMMRegister dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);
   emit_optional_rex_32(dst, src);
@@ -2698,7 +3855,12 @@
 
 
 void Assembler::mulsd(XMMRegister dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);
   emit_optional_rex_32(dst, src);
@@ -2709,7 +3871,12 @@
 
 
 void Assembler::subsd(XMMRegister dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);
   emit_optional_rex_32(dst, src);
@@ -2720,7 +3887,12 @@
 
 
 void Assembler::divsd(XMMRegister dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);
   emit_optional_rex_32(dst, src);
@@ -2731,7 +3903,12 @@
 
 
 void Assembler::xorpd(XMMRegister dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);
   emit_optional_rex_32(dst, src);
@@ -2742,7 +3919,12 @@
 
 
 void Assembler::sqrtsd(XMMRegister dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0xF2);
   emit_optional_rex_32(dst, src);
@@ -2753,7 +3935,12 @@
 
 
 void Assembler::ucomisd(XMMRegister dst, XMMRegister src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+#else
   EnsureSpace ensure_space(this);
+#endif
   last_pc_ = pc_;
   emit(0x66);
   emit_optional_rex_32(dst, src);
@@ -2763,6 +3950,22 @@
 }
 
 
+void Assembler::ucomisd(XMMRegister dst, const Operand& src) {
+#ifdef NACL
+  Sandbox sandbox(SANDBOX_DEBUG this, 3, Sandbox::OptionalRex32,
+      Sandbox::SandboxRight, dst, src);
+#else
+  EnsureSpace ensure_space(this);
+#endif
+  last_pc_ = pc_;
+  emit(0x66);
+  emit_optional_rex_32(dst, src);
+  emit(0x0f);
+  emit(0x2e);
+  emit_sse_operand(dst, src);
+}
+
+
 void Assembler::emit_sse_operand(XMMRegister reg, const Operand& adr) {
   Register ireg = { reg.code() };
   emit_operand(ireg, adr);
@@ -2872,7 +4075,253 @@
 }
 
 
+#ifdef NACL
+void Assembler::before_bind() {
+  Align(NACL_CHUNK);
+}
 
+
+// NACL_CHANGE(pmarch) patches nops
+void Assembler::nops(int n) {
+  ASSERT(n>=0 && n<32);
+  EnsureSpace ensure_space(this);
+
+  if(n>20) {
+    n -= 2;
+    //2 byte jmp
+    emit(0xEB);
+    emit(n);
+  }
+
+  static const uint8_t opt_nop01[] = {0x90, 0};
+  static const uint8_t opt_nop02[] = {0x66, 0x90, 0};
+  static const uint8_t opt_nop03[] = {0x66, 0x87, 0xc9, 0};
+  static const uint8_t opt_nop04[] = {0x66, 0x90, 0x66, 0x90, 0};
+  static const uint8_t opt_nop05[] = {0x90, 0x8d, 0x7c, 0x27, 0x00, 0};
+  static const uint8_t opt_nop06[] = {0x66, 0x90, 0x8d, 0x76, 0x00, 0x90, 0};
+  static const uint8_t opt_nop07[] = {0x8d, 0x76, 0x00, 0x8d, 0x7c, 0x27, 0x00, 0};
+  static const uint8_t opt_nop08[] = {0x66, 0x90, 0x89, 0xf6, 0x8d, 0x7c, 0x27, 0x00, 0};
+  static const uint8_t opt_nop09[] = {0x8d, 0x76, 0x00, 0x89, 0xf6, 0x8d, 0x74, 0x26, 0x00, 0};
+  static const uint8_t opt_nop10[] = {0x8d, 0x74, 0x26, 0x00, 0x66, 0x90, 0x8d, 0x74, 0x26, 0x00, 0};
+  static const uint8_t opt_nop11[] = {0x89, 0xf6, 0x90, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0x7c, 0x27, 0x00, 0};
+  static const uint8_t opt_nop12[] = {0x66, 0x90, 0x8d, 0x76, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop13[] = {0x90, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0x74, 0x26, 0x00, 0};
+  static const uint8_t opt_nop14[] = {0x66, 0x90, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0x74, 0x26, 0x00, 0};
+  static const uint8_t opt_nop15[] = {0x89, 0xf6, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop16[] = {0x89, 0xf6, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbf, 0x00, 0x00, 0x00, 0x00, 0x66, 0x90, 0};
+  static const uint8_t opt_nop17[] = {0x89, 0xf6, 0x8d, 0x7c, 0x27, 0x00, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop18[] = {0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0x74, 0x26, 0x00, 0};
+  static const uint8_t opt_nop19[] = {0x8d, 0x7c, 0x27, 0x00, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x66, 0x90, 0};
+  static const uint8_t opt_nop20[] = {0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop21[] = {0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x76, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x7c, 0x27, 0x00, 0};
+  static const uint8_t opt_nop22[] = {0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x66, 0x90, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop23[] = {0x0f, 0x1f, 0x00, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbf, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop24[] = {0x90, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbf, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop25[] = {0x87, 0xc9, 0x8d, 0x76, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop26[] = {0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop27[] = {0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x66, 0x87, 0xd2, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop28[] = {0x8d, 0x76, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x76, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x0f, 0x1f, 0x84, 0x00, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop29[] = {0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x90, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop30[] = {0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x76, 0x00, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+  static const uint8_t opt_nop31[] = {0x8d, 0xb6, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xb4, 0x26, 0x00, 0x00, 0x00, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0x8d, 0x74, 0x26, 0x00, 0x8d, 0xbc, 0x27, 0x00, 0x00, 0x00, 0x00, 0};
+
+  if(n>0) {
+   static const uint8_t* opt_nops[] = {opt_nop01, opt_nop02, opt_nop03, opt_nop04, opt_nop05, opt_nop06, opt_nop07, opt_nop08, opt_nop09, opt_nop10, opt_nop11, opt_nop12, opt_nop13, opt_nop14, opt_nop15, opt_nop16, opt_nop17, opt_nop18, opt_nop19, opt_nop20, opt_nop21, opt_nop22, opt_nop23, opt_nop24, opt_nop25, opt_nop26, opt_nop27, opt_nop28, opt_nop29, opt_nop30, opt_nop31, 0};
+    memcpy(pc_, opt_nops[n-1], n);
+    pc_+=n;
+  }
+}
+
+
+void Assembler::next_nacl_boundle() {
+  int left = NACL_CHUNK - (pc_offset() & (NACL_CHUNK-1));
+  //scroll to next chunk and start over
+  nops(left);
+}
+
+
+// NACL_CHANGE(pmarch) this is used by sandboxing code
+void Assembler::no_sbx_movl(Register dst, Register src) {
+  EnsureSpace ensure_space(this);
+  last_pc_ = pc_;
+  if (src.low_bits() == 4) {
+    emit_optional_rex_32(src, dst);
+    emit(0x89);
+    emit_modrm(src, dst);
+  } else {
+    emit_optional_rex_32(dst, src);
+    emit(0x8B);
+    emit_modrm(dst, src);
+  }
+}
+
+void Assembler::no_sbx_addq(Register reg, Register rm_reg) {
+  EnsureSpace ensure_space(this);
+  last_pc_ = pc_;
+  if (rm_reg.low_bits() == 4)  {  // Forces SIB byte.
+    // Swap reg and rm_reg and change opcode operand order.
+    emit_rex_64(rm_reg, reg);
+    emit(0x3 ^ 0x02);
+    emit_modrm(rm_reg, reg);
+  } else {
+    emit_rex_64(reg, rm_reg);
+    emit(0x3);
+    emit_modrm(reg, rm_reg);
+  }
+}
+
+
+void Assembler::no_sbx_andl(Register dst, Immediate src) {
+  EnsureSpace ensure_space(this);
+  last_pc_ = pc_;
+  emit_optional_rex_32(dst);
+  if (is_int8(src.value_)) {
+    emit(0x83);
+    emit_modrm(0x4, dst);
+    emit(src.value_);
+  } else if (dst.is(rax)) {
+    emit(0x05 | (0x4 << 3));
+    emitl(src.value_);
+  } else {
+    emit(0x81);
+    emit_modrm(0x4, dst);
+    emitl(src.value_);
+  }
+}
+
+
+
+void Assembler::movl(Register dst, void* value, RelocInfo::Mode rmode) {
+  ASSERT(rmode > RelocInfo::LAST_GCED_ENUM);
+  Sandbox sandbox(SANDBOX_DEBUG this, 1 + sizeof(uint32_t), Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, dst);
+  last_pc_ = pc_;
+  emit_optional_rex_32(dst);
+  emit(0xB8 | dst.low_bits());
+  emitl(reinterpret_cast<uint32_t>(value), rmode);
+}
+
+
+void Assembler::movl(Register dst, int32_t value, RelocInfo::Mode rmode) {
+  // Non-relocatable values might not need a 64-bit representation.
+  if (rmode == RelocInfo::NONE) {
+    // Sadly, there is no zero or sign extending move for 8-bit immediates.
+      movl(dst, Immediate(static_cast<int32_t>(value)));
+  }
+  Sandbox sandbox(SANDBOX_DEBUG this, 1 + sizeof(uint32_t), Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, dst);
+  last_pc_ = pc_;
+  emit_optional_rex_32(dst);
+  emit(0xB8 | dst.low_bits());
+  emitl(value, rmode);
+}
+
+
+void Assembler::movl(Register dst, ExternalReference ref) {
+  Sandbox sandbox(SANDBOX_DEBUG this, 1 + sizeof(uint32_t), Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, dst);
+  last_pc_ = pc_;
+  emit_optional_rex_32(dst);
+  emit(0xB8 | dst.low_bits());
+  emitl(reinterpret_cast<uintptr_t>(ref.address()),
+        RelocInfo::EXTERNAL_REFERENCE);
+}
+
+
+void Assembler::movl(Register dst, Handle<Object> value, RelocInfo::Mode mode) {
+  // If there is no relocation info, emit the value of the handle efficiently
+  // (possibly using less that 8 bytes for the value).
+  if (mode == RelocInfo::NONE) {
+    // There is no possible reason to store a heap pointer without relocation
+    // info, so it must be a smi.
+    ASSERT(value->IsSmi());
+    movl(dst, reinterpret_cast<int32_t>(*value), RelocInfo::NONE);
+  } else {
+    Sandbox sandbox(SANDBOX_DEBUG this, 1 + sizeof(uint32_t), Sandbox::OptionalRex32,
+        Sandbox::SandboxLeft, dst);
+    last_pc_ = pc_;
+    ASSERT(value->IsHeapObject());
+    ASSERT(!Heap::InNewSpace(*value));
+    emit_optional_rex_32(dst);
+    emit(0xB8 | dst.low_bits());
+    emitl(reinterpret_cast<uintptr_t>(value.location()), mode);
+  }
+}
+
+
+void Assembler::movsxbl(Register dst, const Operand& src) {
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32,
+      Sandbox::SandboxBoth, dst, src);
+  last_pc_ = pc_;
+  emit_optional_rex_32(dst, src);
+  emit(0x0F);
+  emit(0xBE);
+  emit_operand(dst, src);
+}
+
+
+void Assembler::movsxwl(Register dst, const Operand& src) {
+  Sandbox sandbox(SANDBOX_DEBUG this, 2, Sandbox::OptionalRex32,
+      Sandbox::SandboxBoth, dst, src);
+  last_pc_ = pc_;
+  emit_optional_rex_32(dst, src);
+  emit(0x0F);
+  emit(0xBF);
+  emit_operand(dst, src);
+}
+
+
+void Assembler::movl(XMMRegister dst, Register src) {
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxNone, dst, src);
+  last_pc_ = pc_;
+  emit(0x66);
+  emit_optional_rex_32(dst, src);
+  emit(0x0F);
+  emit(0x6E);
+  emit_sse_operand(dst, src);
+}
+
+
+void Assembler::movl(Register dst, XMMRegister src) {
+  Sandbox sandbox(SANDBOX_DEBUG this, 4, Sandbox::OptionalRex32,
+      Sandbox::SandboxLeft, dst, src);
+  last_pc_ = pc_;
+  emit(0x66);
+  emit_optional_rex_32(dst, src);
+  emit(0x0F);
+  emit(0x7E);
+  emit_sse_operand(dst, src);
+}
+
+
+void Assembler::no_sbx_lea(Register dst, const Operand& src) {
+  EnsureSpace ensure_space(this);
+  last_pc_ = pc_;
+  emit_rex_64(dst, src);
+  emit(0x8D);
+  emit_operand(dst, src);
+}
+
+
+void Assembler::_shift(Register dst, Immediate shift_amount, int subcode) {
+  Sandbox sandbox(SANDBOX_DEBUG this, shift_amount.value_ == 1 ? 3 : 4, Sandbox::NoFlags,
+      Sandbox::SandboxLeft, dst);
+  last_pc_ = pc_;
+  ASSERT(is_uint6(shift_amount.value_));  // illegal shift count
+  if (shift_amount.value_ == 1) {
+    emit_rex_64(dst);
+    emit(0xD1);
+    emit_modrm(subcode, dst);
+  } else {
+    emit_rex_64(dst);
+    emit(0xC1);
+    emit_modrm(subcode, dst);
+    emit(shift_amount.value_);
+  }
+}
+#endif // NACL
+
 } }  // namespace v8::internal
 
 #endif  // V8_TARGET_ARCH_X64
Index: src/x64/frames-x64.cc
===================================================================
--- src/x64/frames-x64.cc	(revision 4925)
+++ src/x64/frames-x64.cc	(working copy)
@@ -57,7 +57,7 @@
   // Fill in the state.
   state->fp = fp;
   state->sp = sp;
-  state->pc_address = reinterpret_cast<Address*>(sp - 1 * kPointerSize);
+  state->pc_address = reinterpret_cast<Address*>(sp - 1 * kStackPointerSize);
   // Determine frame type.
   return EXIT;
 }
@@ -97,14 +97,14 @@
     arguments = function->shared()->formal_parameter_count() + 1;
   }
   const int offset = StandardFrameConstants::kCallerSPOffset;
-  return fp() + offset + (arguments * kPointerSize);
+  return fp() + offset + (arguments * kStackPointerSize);
 }
 
 
 byte* ArgumentsAdaptorFrame::GetCallerStackPointer() const {
   const int arguments = Smi::cast(GetExpression(0))->value();
   const int offset = StandardFrameConstants::kCallerSPOffset;
-  return fp() + offset + (arguments + 1) * kPointerSize;
+  return fp() + offset + (arguments + 1) * kStackPointerSize;
 }
 
 
Index: src/x64/stub-cache-x64.cc
===================================================================
--- src/x64/stub-cache-x64.cc	(revision 4925)
+++ src/x64/stub-cache-x64.cc	(working copy)
@@ -49,8 +49,13 @@
                        StubCache::Table table,
                        Register name,
                        Register offset) {
+#ifdef NACL
+  ASSERT_EQ(4, kPointerSize);
+  ASSERT_EQ(8, sizeof(StubCache::Entry));
+#else
   ASSERT_EQ(8, kPointerSize);
   ASSERT_EQ(16, sizeof(StubCache::Entry));
+#endif
   // The offset register holds the entry offset times four (due to masking
   // and shifting optimizations).
   ExternalReference key_offset(SCTableReference::keyReference(table));
@@ -61,12 +66,21 @@
   // Multiply entry offset by 16 to get the entry address. Since the
   // offset register already holds the entry offset times four, multiply
   // by a further four.
+#ifdef NACL
+  __ cmpl(name, Operand(kScratchRegister, offset, times_2, 0));
+#else
   __ cmpl(name, Operand(kScratchRegister, offset, times_4, 0));
+#endif
   __ j(not_equal, &miss);
   // Get the code entry from the cache.
   // Use key_offset + kPointerSize, rather than loading value_offset.
+#ifdef NACL
   __ movq(kScratchRegister,
+          Operand(kScratchRegister, offset, times_2, kPointerSize));
+#else
+  __ movq(kScratchRegister,
           Operand(kScratchRegister, offset, times_4, kPointerSize));
+#endif
   // Check that the flags match what we're looking for.
   __ movl(offset, FieldOperand(kScratchRegister, Code::kFlagsOffset));
   __ and_(offset, Immediate(~Code::kFlagsNotUsedInLookup));
@@ -75,6 +89,9 @@
 
   // Jump to the first instruction in the code stub.
   __ addq(kScratchRegister, Immediate(Code::kHeaderSize - kHeapObjectTag));
+#ifdef NACL
+  NACL_PATCH_INSTRUCTION_START(kScratchRegister);
+#endif
   __ jmp(kScratchRegister);
 
   __ bind(&miss);
@@ -172,7 +189,12 @@
   USE(extra);  // The register extra is not used on the X64 platform.
   // Make sure that code is valid. The shifting code relies on the
   // entry size being 16.
+
+#ifdef NACL
+  ASSERT(sizeof(Entry) == 8);
+#else 
   ASSERT(sizeof(Entry) == 16);
+#endif
 
   // Make sure the flags do not name a specific type.
   ASSERT(Code::ExtractTypeFromFlags(flags) == 0);
@@ -396,13 +418,13 @@
   //  -- rsp[8] : last argument in the internal frame of the caller
   // -----------------------------------
   __ movq(scratch, Operand(rsp, 0));
-  __ subq(rsp, Immediate(4 * kPointerSize));
+  __ subq(rsp, Immediate(4 * kStackPointerSize));
   __ movq(Operand(rsp, 0), scratch);
   __ Move(scratch, Smi::FromInt(0));
-  __ movq(Operand(rsp, 1 * kPointerSize), scratch);
-  __ movq(Operand(rsp, 2 * kPointerSize), scratch);
-  __ movq(Operand(rsp, 3 * kPointerSize), scratch);
-  __ movq(Operand(rsp, 4 * kPointerSize), scratch);
+  __ movq(Operand(rsp, 1 * kStackPointerSize), scratch);
+  __ movq(Operand(rsp, 2 * kStackPointerSize), scratch);
+  __ movq(Operand(rsp, 3 * kStackPointerSize), scratch);
+  __ movq(Operand(rsp, 4 * kStackPointerSize), scratch);
 }
 
 
@@ -416,8 +438,8 @@
   //  -- rsp[40] : last argument in the internal frame
   // -----------------------------------
   __ movq(scratch, Operand(rsp, 0));
-  __ movq(Operand(rsp, 4 * kPointerSize), scratch);
-  __ addq(rsp, Immediate(kPointerSize * 4));
+  __ movq(Operand(rsp, 4 * kStackPointerSize), scratch);
+  __ addq(rsp, Immediate(kStackPointerSize * 4));
 }
 
 
@@ -446,16 +468,16 @@
   __ movq(rsi, FieldOperand(rdi, JSFunction::kContextOffset));
 
   // Pass the additional arguments FastHandleApiCall expects.
-  __ movq(Operand(rsp, 4 * kPointerSize), rdi);
+  __ movq(Operand(rsp, 4 * kStackPointerSize), rdi);
   bool info_loaded = false;
   Object* callback = optimization.api_call_info()->callback();
   if (Heap::InNewSpace(callback)) {
     info_loaded = true;
     __ Move(rcx, Handle<CallHandlerInfo>(optimization.api_call_info()));
     __ movq(rbx, FieldOperand(rcx, CallHandlerInfo::kCallbackOffset));
-    __ movq(Operand(rsp, 3 * kPointerSize), rbx);
+    __ movq(Operand(rsp, 3 * kStackPointerSize), rbx);
   } else {
-    __ Move(Operand(rsp, 3 * kPointerSize), Handle<Object>(callback));
+    __ Move(Operand(rsp, 3 * kStackPointerSize), Handle<Object>(callback));
   }
   Object* call_data = optimization.api_call_info()->data();
   if (Heap::InNewSpace(call_data)) {
@@ -463,9 +485,9 @@
       __ Move(rcx, Handle<CallHandlerInfo>(optimization.api_call_info()));
     }
     __ movq(rbx, FieldOperand(rcx, CallHandlerInfo::kDataOffset));
-    __ movq(Operand(rsp, 2 * kPointerSize), rbx);
+    __ movq(Operand(rsp, 2 * kStackPointerSize), rbx);
   } else {
-    __ Move(Operand(rsp, 2 * kPointerSize), Handle<Object>(call_data));
+    __ Move(Operand(rsp, 2 * kStackPointerSize), Handle<Object>(call_data));
   }
 
   // Set the number of arguments.
@@ -753,7 +775,7 @@
 
   // Get the receiver from the stack.
   const int argc = arguments().immediate();
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+  __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
 
   // Check that the receiver isn't a smi.
   if (check != NUMBER_CHECK) {
@@ -791,7 +813,7 @@
       if (object->IsGlobalObject()) {
         ASSERT(depth == kInvalidProtoDepth);
         __ movq(rdx, FieldOperand(rdx, GlobalObject::kGlobalReceiverOffset));
-        __ movq(Operand(rsp, (argc + 1) * kPointerSize), rdx);
+        __ movq(Operand(rsp, (argc + 1) * kStackPointerSize), rdx);
       }
       break;
 
@@ -896,7 +918,7 @@
 
   // Get the receiver from the stack.
   const int argc = arguments().immediate();
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+  __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
 
   // Check that the receiver isn't a smi.
   __ JumpIfSmi(rdx, &miss);
@@ -915,7 +937,7 @@
   // necessary.
   if (object->IsGlobalObject()) {
     __ movq(rdx, FieldOperand(rdx, GlobalObject::kGlobalReceiverOffset));
-    __ movq(Operand(rsp, (argc + 1) * kPointerSize), rdx);
+    __ movq(Operand(rsp, (argc + 1) * kStackPointerSize), rdx);
   }
 
   // Invoke the function.
@@ -955,7 +977,7 @@
 
   // Get the receiver from the stack.
   const int argc = arguments().immediate();
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+  __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
 
   // Check that the receiver isn't a smi.
   __ JumpIfSmi(rdx, &miss);
@@ -971,7 +993,7 @@
   if (argc == 0) {
     // Noop, return the length.
     __ movq(rax, FieldOperand(rdx, JSArray::kLengthOffset));
-    __ ret((argc + 1) * kPointerSize);
+    __ ret((argc + 1) * kStackPointerSize);
   } else {
     // Get the elements array of the object.
     __ movq(rbx, FieldOperand(rdx, JSArray::kElementsOffset));
@@ -1000,9 +1022,9 @@
       __ Integer32ToSmiField(FieldOperand(rdx, JSArray::kLengthOffset), rax);
 
       // Push the element.
-      __ movq(rcx, Operand(rsp, argc * kPointerSize));
+      __ movq(rcx, Operand(rsp, argc * kStackPointerSize));
       __ lea(rdx, FieldOperand(rbx,
-                               rax, times_pointer_size,
+                               rax, times_heap_pointer_size,
                                FixedArray::kHeaderSize - argc * kPointerSize));
       __ movq(Operand(rdx, 0), rcx);
 
@@ -1012,7 +1034,7 @@
       __ JumpIfNotSmi(rcx, &with_write_barrier);
 
       __ bind(&exit);
-      __ ret((argc + 1) * kPointerSize);
+      __ ret((argc + 1) * kStackPointerSize);
 
       __ bind(&with_write_barrier);
 
@@ -1021,7 +1043,7 @@
       RecordWriteStub stub(rbx, rdx, rcx);
       __ CallStub(&stub);
 
-      __ ret((argc + 1) * kPointerSize);
+      __ ret((argc + 1) * kStackPointerSize);
 
       __ bind(&attempt_to_grow_elements);
       ExternalReference new_space_allocation_top =
@@ -1036,7 +1058,7 @@
 
       // Check if it's the end of elements.
       __ lea(rdx, FieldOperand(rbx,
-                               rax, times_pointer_size,
+                               rax, times_heap_pointer_size,
                                FixedArray::kHeaderSize - argc * kPointerSize));
       __ cmpq(rdx, rcx);
       __ j(not_equal, &call_builtin);
@@ -1048,7 +1070,7 @@
       // We fit and could grow elements.
       __ movq(kScratchRegister, new_space_allocation_top);
       __ movq(Operand(kScratchRegister, 0), rcx);
-      __ movq(rcx, Operand(rsp, argc * kPointerSize));
+      __ movq(rcx, Operand(rsp, argc * kStackPointerSize));
 
       // Push the argument...
       __ movq(Operand(rdx, 0), rcx);
@@ -1059,7 +1081,7 @@
       }
 
       // Restore receiver to rdx as finish sequence assumes it's here.
-      __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+      __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
 
       // Increment element's and array's sizes.
       __ SmiAddConstant(FieldOperand(rbx, FixedArray::kLengthOffset),
@@ -1068,7 +1090,7 @@
       __ Integer32ToSmi(rax, rax);
       __ movq(FieldOperand(rdx, JSArray::kLengthOffset), rax);
       // Elements are in new space, so write barrier is not required.
-      __ ret((argc + 1) * kPointerSize);
+      __ ret((argc + 1) * kStackPointerSize);
 
       __ bind(&call_builtin);
     }
@@ -1112,7 +1134,7 @@
 
   // Get the receiver from the stack.
   const int argc = arguments().immediate();
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+  __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
 
   // Check that the receiver isn't a smi.
   __ JumpIfSmi(rdx, &miss);
@@ -1136,7 +1158,7 @@
   // Get the last element.
   __ Move(r9, Factory::the_hole_value());
   __ movq(rax, FieldOperand(rbx,
-                            rcx, times_pointer_size,
+                            rcx, times_heap_pointer_size,
                             FixedArray::kHeaderSize));
   // Check if element is already the hole.
   __ cmpq(rax, r9);
@@ -1148,15 +1170,15 @@
 
   // Fill with the hole and return original value.
   __ movq(FieldOperand(rbx,
-                       rcx, times_pointer_size,
+                       rcx, times_heap_pointer_size,
                        FixedArray::kHeaderSize),
           r9);
-  __ ret((argc + 1) * kPointerSize);
+  __ ret((argc + 1) * kStackPointerSize);
 
   __ bind(&return_undefined);
 
   __ Move(rax, Factory::undefined_value());
-  __ ret((argc + 1) * kPointerSize);
+  __ ret((argc + 1) * kStackPointerSize);
 
   __ bind(&call_builtin);
   __ TailCallExternalReference(ExternalReference(Builtins::c_ArrayPop),
@@ -1215,7 +1237,7 @@
   LookupPostInterceptor(holder, name, &lookup);
 
   // Get the receiver from the stack.
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+  __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
 
   CallInterceptorCompiler compiler(this, arguments(), rcx);
   compiler.Compile(masm(),
@@ -1229,7 +1251,7 @@
                    &miss);
 
   // Restore receiver.
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+  __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
 
   // Check that the function really is a function.
   __ JumpIfSmi(rax, &miss);
@@ -1240,7 +1262,7 @@
   // necessary.
   if (object->IsGlobalObject()) {
     __ movq(rdx, FieldOperand(rdx, GlobalObject::kGlobalReceiverOffset));
-    __ movq(Operand(rsp, (argc + 1) * kPointerSize), rdx);
+    __ movq(Operand(rsp, (argc + 1) * kStackPointerSize), rdx);
   }
 
   // Invoke the function.
@@ -1278,7 +1300,7 @@
   const int argc = arguments().immediate();
 
   // Get the receiver from the stack.
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+  __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
 
   // If the object is the holder then we know that it's a global
   // object which can only happen for contextual calls. In this case,
@@ -1317,7 +1339,7 @@
   // Patch the receiver on the stack with the global proxy.
   if (object->IsGlobalObject()) {
     __ movq(rdx, FieldOperand(rdx, GlobalObject::kGlobalReceiverOffset));
-    __ movq(Operand(rsp, (argc + 1) * kPointerSize), rdx);
+    __ movq(Operand(rsp, (argc + 1) * kStackPointerSize), rdx);
   }
 
   // Setup the context (function already in edi).
@@ -1941,6 +1963,9 @@
 
   // Do a tail-call of the compiled function.
   __ lea(rcx, FieldOperand(rax, Code::kHeaderSize));
+#ifdef NACL
+  NACL_PATCH_INSTRUCTION_START(rcx);
+#endif
   __ jmp(rcx);
 
   return GetCodeWithFlags(flags, "LazyCompileStub");
@@ -2270,7 +2295,7 @@
   __ lea(r9, Operand(rdx, JSObject::kHeaderSize));
   // Calculate the location of the first argument. The stack contains only the
   // return address on top of the argc arguments.
-  __ lea(rcx, Operand(rsp, rax, times_pointer_size, 0));
+  __ lea(rcx, Operand(rsp, rax, times_stack_pointer_size, 0));
 
   // rax: argc
   // rcx: first argument
@@ -2287,7 +2312,7 @@
       int arg_number = shared->GetThisPropertyAssignmentArgument(i);
       __ movq(rbx, r8);
       __ cmpq(rax, Immediate(arg_number));
-      __ cmovq(above, rbx, Operand(rcx, arg_number * -kPointerSize));
+      __ cmovq(above, rbx, Operand(rcx, arg_number * -kStackPointerSize));
       // Store value in the property.
       __ movq(Operand(r9, i * kPointerSize), rbx);
     } else {
@@ -2315,7 +2340,7 @@
   // rbx: argc
   // Remove caller arguments and receiver from the stack and return.
   __ pop(rcx);
-  __ lea(rsp, Operand(rsp, rbx, times_pointer_size, 1 * kPointerSize));
+  __ lea(rsp, Operand(rsp, rbx, times_stack_pointer_size, 1 * kStackPointerSize));
   __ push(rcx);
   __ IncrementCounter(&Counters::constructed_objects, 1);
   __ IncrementCounter(&Counters::constructed_objects_stub, 1);
Index: src/x64/macro-assembler-x64.h
===================================================================
--- src/x64/macro-assembler-x64.h	(revision 4925)
+++ src/x64/macro-assembler-x64.h	(working copy)
@@ -225,6 +225,10 @@
                                              Register src,
                                              int power);
 
+  void PositiveSmiTimesPowerOfTwoToInteger32(Register dst,
+                                             Register src,
+                                             int power);
+
   // Divide a positive smi's integer value by a power of two.
   // Provides result as 32-bit integer value.
   void PositiveSmiDivPowerOfTwoToInteger32(Register dst,
@@ -404,7 +408,8 @@
 
   void SmiShiftLeftConstant(Register dst,
                             Register src,
-                            int shift_value);
+                            int shift_value,
+                            Label* on_not_smi_result);
   void SmiShiftLogicalRightConstant(Register dst,
                                   Register src,
                                   int shift_value,
@@ -417,7 +422,9 @@
   // Uses and clobbers rcx, so dst may not be rcx.
   void SmiShiftLeft(Register dst,
                     Register src1,
-                    Register src2);
+                    Register src2,
+                    Label* on_not_smi_result);
+
   // Shifts a smi value to the right, shifting in zero bits at the top, and
   // returns the unsigned intepretation of the result if that is a smi.
   // Uses and clobbers rcx, so dst may not be rcx.
@@ -546,7 +553,8 @@
                                Register map,
                                Register instance_type);
 
-  // FCmp is similar to integer cmp, but requires unsigned
+  // FCmp compares and pops the two values on top of the FPU stack.
+  // The flag results are similar to integer cmp, but requires unsigned
   // jcc instructions (je, ja, jae, jb, jbe, je, and jz).
   void FCmp();
 
Index: src/x64/virtual-frame-x64.cc
===================================================================
--- src/x64/virtual-frame-x64.cc	(revision 4925)
+++ src/x64/virtual-frame-x64.cc	(working copy)
@@ -115,25 +115,45 @@
     Handle<Object> undefined = Factory::undefined_value();
     FrameElement initial_value =
         FrameElement::ConstantElement(undefined, FrameElement::SYNCED);
-    if (count == 1) {
-      __ Push(undefined);
-    } else if (count < kLocalVarBound) {
-      // For less locals the unrolled loop is more compact.
-      __ movq(kScratchRegister, undefined, RelocInfo::EMBEDDED_OBJECT);
+    if (count < kLocalVarBound) {
+      // For fewer locals the unrolled loop is more compact.
+
+      // Hope for one of the first eight registers, where the push operation
+      // takes only one byte (kScratchRegister needs the REX.W bit).
+      Result tmp = cgen()->allocator()->Allocate();
+      ASSERT(tmp.is_valid());
+      __ movq(tmp.reg(), undefined, RelocInfo::EMBEDDED_OBJECT);
       for (int i = 0; i < count; i++) {
-        __ push(kScratchRegister);
+        __ push(tmp.reg());
       }
     } else {
       // For more locals a loop in generated code is more compact.
       Label alloc_locals_loop;
       Result cnt = cgen()->allocator()->Allocate();
       ASSERT(cnt.is_valid());
-      __ movq(cnt.reg(), Immediate(count));
       __ movq(kScratchRegister, undefined, RelocInfo::EMBEDDED_OBJECT);
+#ifdef DEBUG
+      Label loop_size;
+      __ bind(&loop_size);
+#endif
+      if (is_uint8(count)) {
+        // Loading imm8 is shorter than loading imm32.
+        // Loading only partial byte register, and using decb below.
+        __ movb(cnt.reg(), Immediate(count));
+      } else {
+        __ movl(cnt.reg(), Immediate(count));
+      }
       __ bind(&alloc_locals_loop);
       __ push(kScratchRegister);
-      __ decl(cnt.reg());
+      if (is_uint8(count)) {
+        __ decb(cnt.reg());
+      } else {
+        __ decl(cnt.reg());
+      }
       __ j(not_zero, &alloc_locals_loop);
+#ifdef DEBUG
+      CHECK(masm()->SizeOfCodeGeneratedSince(&loop_size) < kLocalVarBound);
+#endif
     }
     for (int i = 0; i < count; i++) {
       elements_.Add(initial_value);
@@ -262,7 +282,7 @@
   if (num_virtual_elements < count) {
     int num_dropped = count - num_virtual_elements;
     stack_pointer_ -= num_dropped;
-    __ addq(rsp, Immediate(num_dropped * kPointerSize));
+    __ addq(rsp, Immediate(num_dropped * kStackPointerSize));
   }
 
   // Discard elements from the virtual frame and free any registers.
@@ -560,7 +580,7 @@
   if (stack_pointer_ < expected->stack_pointer_) {
     int difference = expected->stack_pointer_ - stack_pointer_;
     stack_pointer_ = expected->stack_pointer_;
-    __ subq(rsp, Immediate(difference * kPointerSize));
+    __ subq(rsp, Immediate(difference * kStackPointerSize));
   }
 
   MergeMoveRegistersToMemory(expected);
@@ -571,7 +591,7 @@
   if (stack_pointer_ > expected->stack_pointer_) {
     int difference = stack_pointer_ - expected->stack_pointer_;
     stack_pointer_ = expected->stack_pointer_;
-    __ addq(rsp, Immediate(difference * kPointerSize));
+    __ addq(rsp, Immediate(difference * kStackPointerSize));
   }
 
   // At this point, the frames should be identical.
@@ -768,7 +788,7 @@
       return temp;
     }
 
-    __ addq(rsp, Immediate(kPointerSize));
+    __ addq(rsp, Immediate(kStackPointerSize));
   }
   ASSERT(!element.is_memory());
 
Index: src/x64/frames-x64.h
===================================================================
--- src/x64/frames-x64.h	(revision 4925)
+++ src/x64/frames-x64.h	(working copy)
@@ -45,44 +45,44 @@
 
 class StackHandlerConstants : public AllStatic {
  public:
-  static const int kNextOffset  = 0 * kPointerSize;
-  static const int kFPOffset    = 1 * kPointerSize;
-  static const int kStateOffset = 2 * kPointerSize;
-  static const int kPCOffset    = 3 * kPointerSize;
+  static const int kNextOffset  = 0 * kStackPointerSize;
+  static const int kFPOffset    = 1 * kStackPointerSize;
+  static const int kStateOffset = 2 * kStackPointerSize;
+  static const int kPCOffset    = 3 * kStackPointerSize;
 
-  static const int kSize = 4 * kPointerSize;
+  static const int kSize = 4 * kStackPointerSize;
 };
 
 
 class EntryFrameConstants : public AllStatic {
  public:
-  static const int kCallerFPOffset      = -10 * kPointerSize;
-  static const int kArgvOffset          = 6 * kPointerSize;
+  static const int kCallerFPOffset      = -10 * kStackPointerSize;
+  static const int kArgvOffset          = 6 * kStackPointerSize;
 };
 
 
 class ExitFrameConstants : public AllStatic {
  public:
-  static const int kCodeOffset      = -2 * kPointerSize;
-  static const int kSPOffset        = -1 * kPointerSize;
+  static const int kCodeOffset      = -2 * kStackPointerSize;
+  static const int kSPOffset        = -1 * kStackPointerSize;
 
-  static const int kCallerFPOffset  = +0 * kPointerSize;
-  static const int kCallerPCOffset  = +1 * kPointerSize;
+  static const int kCallerFPOffset  = +0 * kStackPointerSize;
+  static const int kCallerPCOffset  = +1 * kStackPointerSize;
 
   // FP-relative displacement of the caller's SP.  It points just
   // below the saved PC.
-  static const int kCallerSPDisplacement = +2 * kPointerSize;
+  static const int kCallerSPDisplacement = +2 * kStackPointerSize;
 };
 
 
 class StandardFrameConstants : public AllStatic {
  public:
-  static const int kExpressionsOffset = -3 * kPointerSize;
-  static const int kMarkerOffset      = -2 * kPointerSize;
-  static const int kContextOffset     = -1 * kPointerSize;
-  static const int kCallerFPOffset    =  0 * kPointerSize;
-  static const int kCallerPCOffset    = +1 * kPointerSize;
-  static const int kCallerSPOffset    = +2 * kPointerSize;
+  static const int kExpressionsOffset = -3 * kStackPointerSize;
+  static const int kMarkerOffset      = -2 * kStackPointerSize;
+  static const int kContextOffset     = -1 * kStackPointerSize;
+  static const int kCallerFPOffset    =  0 * kStackPointerSize;
+  static const int kCallerPCOffset    = +1 * kStackPointerSize;
+  static const int kCallerSPOffset    = +2 * kStackPointerSize;
 };
 
 
@@ -90,12 +90,12 @@
  public:
   // FP-relative.
   static const int kLocal0Offset = StandardFrameConstants::kExpressionsOffset;
-  static const int kSavedRegistersOffset = +2 * kPointerSize;
+  static const int kSavedRegistersOffset = +2 * kStackPointerSize;
   static const int kFunctionOffset = StandardFrameConstants::kMarkerOffset;
 
   // Caller SP-relative.
-  static const int kParam0Offset   = -2 * kPointerSize;
-  static const int kReceiverOffset = -1 * kPointerSize;
+  static const int kParam0Offset   = -2 * kStackPointerSize;
+  static const int kReceiverOffset = -1 * kStackPointerSize;
 };
 
 
Index: src/x64/assembler-x64.h
===================================================================
--- src/x64/assembler-x64.h	(revision 4925)
+++ src/x64/assembler-x64.h	(working copy)
@@ -39,6 +39,12 @@
 
 #include "serialize.h"
 
+#ifdef NACL
+#define NACL_CHUNK 32
+#define NACL_PATCH_INSTRUCTION_START(reg) \
+  __ movl(reg, Operand(reg, Code::kExternalInstructionsOffset-Code::kHeaderSize))
+#endif
+
 namespace v8 {
 namespace internal {
 
@@ -46,23 +52,23 @@
 
 // Test whether a 64-bit value is in a specific range.
 static inline bool is_uint32(int64_t x) {
-  static const int64_t kUInt32Mask = V8_INT64_C(0xffffffff);
-  return x == (x & kUInt32Mask);
+  static const uint64_t kMaxUInt32 = V8_UINT64_C(0xffffffff);
+  return static_cast<uint64_t>(x) <= kMaxUInt32;
 }
 
 static inline bool is_int32(int64_t x) {
-  static const int64_t kMinIntValue = V8_INT64_C(-0x80000000);
-  return is_uint32(x - kMinIntValue);
+  static const int64_t kMinInt32 = -V8_INT64_C(0x80000000);
+  return is_uint32(x - kMinInt32);
 }
 
 static inline bool uint_is_int32(uint64_t x) {
-  static const uint64_t kMaxIntValue = V8_UINT64_C(0x80000000);
-  return x < kMaxIntValue;
+  static const uint64_t kMaxInt32 = V8_UINT64_C(0x7fffffff);
+  return x <= kMaxInt32;
 }
 
 static inline bool is_uint32(uint64_t x) {
-  static const uint64_t kMaxUIntValue = V8_UINT64_C(0x100000000);
-  return x < kMaxUIntValue;
+  static const uint64_t kMaxUInt32 = V8_UINT64_C(0xffffffff);
+  return x <= kMaxUInt32;
 }
 
 // CPU Registers.
@@ -135,6 +141,10 @@
 const Register r14 = { 14 };
 const Register r15 = { 15 };
 const Register no_reg = { -1 };
+#ifdef NACL
+// r11 is used as a scratch register in sandboxing memory access oeprations 
+const Register scratch_reg = r11;
+#endif
 
 
 struct XMMRegister {
@@ -174,6 +184,9 @@
 const XMMRegister xmm13 = { 13 };
 const XMMRegister xmm14 = { 14 };
 const XMMRegister xmm15 = { 15 };
+#ifdef NACL
+const XMMRegister no_xmm_reg = { -1 };
+#endif
 
 enum Condition {
   // any value < 0 is considered no_condition
@@ -268,6 +281,18 @@
  public:
   explicit Immediate(int32_t value) : value_(value) {}
 
+#ifdef NACL
+  inline int32_t get_size_8_32() {
+    if (is_int8(value_)) return 1;
+    return sizeof(uint32_t);
+  }
+  
+  inline int32_t get_size_8_16() {
+    if (is_int8(value_)) return 1;
+    return sizeof(uint16_t);
+  }
+#endif
+
  private:
   int32_t value_;
 
@@ -284,7 +309,10 @@
   times_4 = 2,
   times_8 = 3,
   times_int_size = times_4,
-  times_pointer_size = times_8
+  times_pointer_size = times_8,
+  times_stack_pointer_size = times_8,
+  times_heap_pointer_size = times_4,
+  times_no = -1
 };
 
 
@@ -309,6 +337,16 @@
   // this must not overflow.
   Operand(const Operand& base, int32_t offset);
 
+#ifdef NACL
+  // TODO(pmarch) fix consts here
+  Register get_base() const;
+  Register get_index() const;
+  ScaleFactor get_scale() const;
+  int32_t get_disp() const;
+  void copyfrom(const Operand& op);
+  inline int get_size() const;
+#endif
+
  private:
   byte rex_;
   byte buf_[6];
@@ -329,6 +367,9 @@
   inline void set_disp32(int disp);
 
   friend class Assembler;
+#ifdef NACL
+  friend class Sandbox;
+#endif
 };
 
 
@@ -399,7 +440,11 @@
   // (There is a 15 byte limit on x64 instruction length that rules out some
   // otherwise valid instructions.)
   // This allows for a single, fast space check per instruction.
+#ifdef NACL
+  static const int kGap = 64;
+#else
   static const int kGap = 32;
+#endif
 
  public:
   // Create an assembler. Instructions and relocation information are emitted
@@ -431,6 +476,9 @@
   // These functions convert between absolute Addresses of Code objects and
   // the relative displacements stored in the code.
   static inline Address target_address_at(Address pc);
+#ifdef NACL  
+  static inline void set_target_address_at(Address pc, Address target, uint32_t extraoffset);
+#endif
   static inline void set_target_address_at(Address pc, Address target);
 
   // This sets the branch destination (which is in the instruction on x64).
@@ -511,7 +559,11 @@
   void push(const Operand& src);
   void push(Label* label, RelocInfo::Mode relocation_mode);
 
+#ifdef NACL
+  void pop(Register dst, int flags = 0);
+#else
   void pop(Register dst);
+#endif
   void pop(const Operand& dst);
 
   void enter(Immediate size);
@@ -526,9 +578,29 @@
   // memory location.
   void movw(const Operand& dst, Register src);
 
+#ifdef NACL
+  // sandboxing instructions
+  void no_sbx_movl(Register dst, Register src);
+  void no_sbx_andl(Register dst, Immediate src);
+  void no_sbx_addq(Register dst, Register src);
+  void no_sbx_lea(Register dst, const Operand& src);
+
+  void movl(Register dst, void* value, RelocInfo::Mode rmode);
+  void movl(Register dst, int32_t value, RelocInfo::Mode rmode);
+  void movl(Register dst, ExternalReference ref);
+  void movl(Register dst, Handle<Object> value, RelocInfo::Mode mode);
+  void movsxbl(Register dst, const Operand& src);
+  void movsxwl(Register dst, const Operand& src);
+  void movl(XMMRegister dst, Register src);
+  void movl(Register dst, XMMRegister src);
+#endif
   void movl(Register dst, Register src);
   void movl(Register dst, const Operand& src);
+#ifdef NACL
+  void movl(const Operand& dst, Register src, int flags = 0);
+#else
   void movl(const Operand& dst, Register src);
+#endif
   void movl(const Operand& dst, Immediate imm);
   // Load a 32-bit immediate value, zero-extended to 64 bits.
   void movl(Register dst, Immediate imm32);
@@ -825,6 +897,12 @@
     shift(dst, imm8, 0x0);
   }
 
+#ifdef NACL
+  void roll(Register dst, Immediate imm8) {
+    shift_32(dst, imm8, 0x0);
+  }
+#endif
+
   void rcr(Register dst, Immediate imm8) {
     shift(dst, imm8, 0x3);
   }
@@ -1006,6 +1084,13 @@
   // but it may be bound only once.
 
   void bind(Label* L);  // binds an unbound label L to the current code position
+#ifdef NACL
+  // binds label at the beginning of a NaCl bundle
+  // this is used to guarantee that the space between a patch site and
+  // instruction that point to the patch site is the same at the moment of
+  // binding the label and the moment emitting the point-instruction
+  void bind_aligned(Label* L);
+#endif
 
   // Calls
   // Call near relative 32-bit displacement, relative to next instruction.
@@ -1110,17 +1195,28 @@
   void movsd(XMMRegister dst, XMMRegister src);
   void movsd(XMMRegister dst, const Operand& src);
 
+  void movss(XMMRegister dst, const Operand& src);
+  void movss(const Operand& dst, XMMRegister src);
+
   void cvttss2si(Register dst, const Operand& src);
   void cvttsd2si(Register dst, const Operand& src);
   void cvttsd2siq(Register dst, XMMRegister src);
+  void cvttsd2siq(Register dst, const Operand& src);
 
   void cvtlsi2sd(XMMRegister dst, const Operand& src);
   void cvtlsi2sd(XMMRegister dst, Register src);
   void cvtqsi2sd(XMMRegister dst, const Operand& src);
   void cvtqsi2sd(XMMRegister dst, Register src);
 
+  void cvtlsi2ss(XMMRegister dst, Register src);
+
   void cvtss2sd(XMMRegister dst, XMMRegister src);
+  void cvtss2sd(XMMRegister dst, const Operand& src);
+  void cvtsd2ss(XMMRegister dst, XMMRegister src);
 
+  void cvtsd2si(Register dst, XMMRegister src);
+  void cvtsd2siq(Register dst, XMMRegister src);
+
   void addsd(XMMRegister dst, XMMRegister src);
   void subsd(XMMRegister dst, XMMRegister src);
   void mulsd(XMMRegister dst, XMMRegister src);
@@ -1130,6 +1226,7 @@
   void sqrtsd(XMMRegister dst, XMMRegister src);
 
   void ucomisd(XMMRegister dst, XMMRegister src);
+  void ucomisd(XMMRegister dst, const Operand& src);
 
   // The first argument is the reg field, the second argument is the r/m field.
   void emit_sse_operand(XMMRegister dst, XMMRegister src);
@@ -1183,6 +1280,18 @@
   static const int kMaximalBufferSize = 512*MB;
   static const int kMinimalBufferSize = 4*KB;
 
+#ifdef NACL
+  // patches nops
+  void nops(int n);
+  // alignes labels
+  void before_bind();
+  // nops to next boundle
+  void next_nacl_boundle();
+
+  // is dst reg rsp or rbp?
+  inline bool is_rsp_rbp(Register dst);
+#endif
+
  private:
   byte* addr_at(int pos)  { return buffer_ + pos; }
   byte byte_at(int pos)  { return buffer_[pos]; }
@@ -1199,6 +1308,9 @@
   void emit(byte x) { *pc_++ = x; }
   inline void emitl(uint32_t x);
   inline void emitq(uint64_t x, RelocInfo::Mode rmode);
+#ifdef NACL
+  inline void emitl(uint32_t x, RelocInfo::Mode rmode);
+#endif
   inline void emitw(uint16_t x);
   inline void emit_code_target(Handle<Code> target, RelocInfo::Mode rmode);
   void emit(Immediate x) { emitl(x.value_); }
@@ -1351,6 +1463,9 @@
 
   // Emit machine code for a shift operation.
   void shift(Register dst, Immediate shift_amount, int subcode);
+#ifdef NACL
+  void _shift(Register dst, Immediate shift_amount, int subcode);
+#endif
   void shift_32(Register dst, Immediate shift_amount, int subcode);
   // Shift dst by cl % 64 bits.
   void shift(Register dst, int subcode);
@@ -1369,6 +1484,9 @@
   friend class CodePatcher;
   friend class EnsureSpace;
   friend class RegExpMacroAssemblerX64;
+#ifdef NACL
+  friend class Sandbox;
+#endif
 
   // Code buffer:
   // The buffer into which code and relocation info are generated.
Index: src/x64/disasm-x64.cc
===================================================================
--- src/x64/disasm-x64.cc	(revision 4925)
+++ src/x64/disasm-x64.cc	(working copy)
@@ -1028,9 +1028,9 @@
         if (opcode == 0x57) {
           mnemonic = "xorpd";
         } else if (opcode == 0x2E) {
+          mnemonic = "ucomisd";
+        } else if (opcode == 0x2F) {
           mnemonic = "comisd";
-        } else if (opcode == 0x2F) {
-          mnemonic = "ucomisd";
         } else {
           UnimplementedInstruction();
         }
@@ -1057,7 +1057,7 @@
       // CVTSI2SD: integer to XMM double conversion.
       int mod, regop, rm;
       get_modrm(*current, &mod, &regop, &rm);
-      AppendToBuffer("%s %s,", mnemonic, NameOfXMMRegister(regop));
+      AppendToBuffer("%sd %s,", mnemonic, NameOfXMMRegister(regop));
       current += PrintRightOperand(current);
     } else if ((opcode & 0xF8) == 0x58 || opcode == 0x51) {
       // XMM arithmetic. Mnemonic was retrieved at the start of this function.
@@ -1070,7 +1070,25 @@
     }
   } else if (group_1_prefix_ == 0xF3) {
     // Instructions with prefix 0xF3.
-    if (opcode == 0x2C) {
+    if (opcode == 0x11 || opcode == 0x10) {
+      // MOVSS: Move scalar double-precision fp to/from/between XMM registers.
+      AppendToBuffer("movss ");
+      int mod, regop, rm;
+      get_modrm(*current, &mod, &regop, &rm);
+      if (opcode == 0x11) {
+        current += PrintRightOperand(current);
+        AppendToBuffer(",%s", NameOfXMMRegister(regop));
+      } else {
+        AppendToBuffer("%s,", NameOfXMMRegister(regop));
+        current += PrintRightOperand(current);
+      }
+    } else if (opcode == 0x2A) {
+      // CVTSI2SS: integer to XMM single conversion.
+      int mod, regop, rm;
+      get_modrm(*current, &mod, &regop, &rm);
+      AppendToBuffer("%ss %s,", mnemonic, NameOfXMMRegister(regop));
+      current += PrintRightOperand(current);
+    } else if (opcode == 0x2C) {
       // CVTTSS2SI: Convert scalar single-precision FP to dword integer.
       // Assert that mod is not 3, so source is memory, not an XMM register.
       ASSERT_NE(0xC0, *current & 0xC0);
@@ -1146,8 +1164,8 @@
   switch (opcode) {
     case 0x1F:
       return "nop";
-    case 0x2A:  // F2 prefix.
-      return "cvtsi2sd";
+    case 0x2A:  // F2/F3 prefix.
+      return "cvtsi2s";
     case 0x31:
       return "rdtsc";
     case 0x51:  // F2 prefix.
Index: src/x64/virtual-frame-x64.h
===================================================================
--- src/x64/virtual-frame-x64.h	(revision 4925)
+++ src/x64/virtual-frame-x64.h	(working copy)
@@ -200,14 +200,19 @@
   inline void PrepareForReturn();
 
   // Number of local variables after when we use a loop for allocating.
-  static const int kLocalVarBound = 7;
+#ifdef NACL
+  // NACL_CHANGE(pmarch): because of sandboxing, the loop code is increased
+  static const int kLocalVarBound = 39;
+#else  
+  static const int kLocalVarBound = 14;
+#endif
 
   // Allocate and initialize the frame-allocated locals.
   void AllocateStackSlots();
 
   // An element of the expression stack as an assembly operand.
   Operand ElementAt(int index) const {
-    return Operand(rsp, index * kPointerSize);
+    return Operand(rsp, index * kStackPointerSize);
   }
 
   // Random-access store to a frame-top relative frame element.  The result
@@ -229,7 +234,7 @@
   Operand LocalAt(int index) {
     ASSERT(0 <= index);
     ASSERT(index < local_count());
-    return Operand(rbp, kLocal0Offset - index * kPointerSize);
+    return Operand(rbp, kLocal0Offset - index * kStackPointerSize);
   }
 
   // Push a copy of the value of a local frame slot on top of the frame.
@@ -267,7 +272,7 @@
   Operand ParameterAt(int index) {
     ASSERT(-1 <= index);  // -1 is the receiver.
     ASSERT(index < parameter_count());
-    return Operand(rbp, (1 + parameter_count() - index) * kPointerSize);
+    return Operand(rbp, (1 + parameter_count() - index) * kStackPointerSize);
   }
 
   // Push a copy of the value of a parameter frame slot on top of the frame.
@@ -445,7 +450,7 @@
   static const int kFunctionOffset = JavaScriptFrameConstants::kFunctionOffset;
   static const int kContextOffset = StandardFrameConstants::kContextOffset;
 
-  static const int kHandlerSize = StackHandlerConstants::kSize / kPointerSize;
+  static const int kHandlerSize = StackHandlerConstants::kSize / kStackPointerSize;
   static const int kPreallocatedElements = 5 + 8;  // 8 expression stack slots.
 
   ZoneList<FrameElement> elements_;
@@ -491,7 +496,7 @@
   int fp_relative(int index) {
     ASSERT(index < element_count());
     ASSERT(frame_pointer() < element_count());  // FP is on the frame.
-    return (frame_pointer() - index) * kPointerSize;
+    return (frame_pointer() - index) * kStackPointerSize;
   }
 
   // Record an occurrence of a register in the virtual frame.  This has the
Index: src/x64/register-allocator-x64-inl.h
===================================================================
--- src/x64/register-allocator-x64-inl.h	(revision 4925)
+++ src/x64/register-allocator-x64-inl.h	(working copy)
@@ -58,11 +58,11 @@
     5,   // r8
     6,   // r9
     -1,  // r10  Scratch register.
-    9,   // r11
-    10,  // r12
+    -1,  // r11
+     8,  // r12
     -1,  // r13  Roots array.  This is callee saved.
     7,   // r14
-    8    // r15
+    -1,  // r15
   };
   return kNumbers[reg.code()];
 }
@@ -70,8 +70,10 @@
 
 Register RegisterAllocator::ToRegister(int num) {
   ASSERT(num >= 0 && num < kNumRegisters);
+// PMARCH REVISIT
   const Register kRegisters[] =
-      { rax, rbx, rcx, rdx, rdi, r8, r9, r14, r15, r11, r12 };
+      { rax, rbx, rcx, rdx, rdi, r8, r9, r14, r12 };
+      // 0    1    2    3    4    5   6   7    8
   return kRegisters[num];
 }
 
Index: src/x64/cpu-x64.cc
===================================================================
--- src/x64/cpu-x64.cc	(revision 4925)
+++ src/x64/cpu-x64.cc	(working copy)
@@ -73,6 +73,8 @@
   // instead
   // __asm { int 3 }
   __debugbreak();
+#elif defined(NACL)
+  asm("hlt");
 #else
   asm("int $3");
 #endif
Index: src/x64/builtins-x64.cc
===================================================================
--- src/x64/builtins-x64.cc	(revision 4925)
+++ src/x64/builtins-x64.cc	(working copy)
@@ -99,8 +99,8 @@
 
   // Remove caller arguments from the stack.
   __ pop(rcx);
-  SmiIndex index = masm->SmiToIndex(rbx, rbx, kPointerSizeLog2);
-  __ lea(rsp, Operand(rsp, index.reg, index.scale, 1 * kPointerSize));
+  SmiIndex index = masm->SmiToIndex(rbx, rbx, kStackPointerSizeLog2);
+  __ lea(rsp, Operand(rsp, index.reg, index.scale, 1 * kStackPointerSize));
   __ push(rcx);
 }
 
@@ -127,14 +127,14 @@
 
     // Copy receiver and all expected arguments.
     const int offset = StandardFrameConstants::kCallerSPOffset;
-    __ lea(rax, Operand(rbp, rax, times_pointer_size, offset));
+    __ lea(rax, Operand(rbp, rax, times_stack_pointer_size, offset));
     __ movq(rcx, Immediate(-1));  // account for receiver
 
     Label copy;
     __ bind(&copy);
     __ incq(rcx);
     __ push(Operand(rax, 0));
-    __ subq(rax, Immediate(kPointerSize));
+    __ subq(rax, Immediate(kStackPointerSize));
     __ cmpq(rcx, rbx);
     __ j(less, &copy);
     __ jmp(&invoke);
@@ -146,14 +146,14 @@
 
     // Copy receiver and all actual arguments.
     const int offset = StandardFrameConstants::kCallerSPOffset;
-    __ lea(rdi, Operand(rbp, rax, times_pointer_size, offset));
+    __ lea(rdi, Operand(rbp, rax, times_stack_pointer_size, offset));
     __ movq(rcx, Immediate(-1));  // account for receiver
 
     Label copy;
     __ bind(&copy);
     __ incq(rcx);
     __ push(Operand(rdi, 0));
-    __ subq(rdi, Immediate(kPointerSize));
+    __ subq(rdi, Immediate(kStackPointerSize));
     __ cmpq(rcx, rax);
     __ j(less, &copy);
 
@@ -212,7 +212,7 @@
   //    if it is a function.
   Label non_function;
   // The function to call is at position n+1 on the stack.
-  __ movq(rdi, Operand(rsp, rax, times_pointer_size, 1 * kPointerSize));
+  __ movq(rdi, Operand(rsp, rax, times_stack_pointer_size, 1 * kStackPointerSize));
   __ JumpIfSmi(rdi, &non_function);
   __ CmpObjectType(rdi, JS_FUNCTION_TYPE, rcx);
   __ j(not_equal, &non_function);
@@ -223,7 +223,7 @@
     // Change context eagerly in case we need the global receiver.
     __ movq(rsi, FieldOperand(rdi, JSFunction::kContextOffset));
 
-    __ movq(rbx, Operand(rsp, rax, times_pointer_size, 0));
+    __ movq(rbx, Operand(rsp, rax, times_stack_pointer_size, 0));
     __ JumpIfSmi(rbx, &convert_to_object);
 
     __ CompareRoot(rbx, Heap::kNullValueRootIndex);
@@ -249,7 +249,7 @@
     __ SmiToInteger32(rax, rax);
     __ LeaveInternalFrame();
     // Restore the function to rdi.
-    __ movq(rdi, Operand(rsp, rax, times_pointer_size, 1 * kPointerSize));
+    __ movq(rdi, Operand(rsp, rax, times_stack_pointer_size, 1 * kStackPointerSize));
     __ jmp(&patch_receiver);
 
     // Use the global receiver object from the called function as the
@@ -263,7 +263,7 @@
     __ movq(rbx, FieldOperand(rbx, GlobalObject::kGlobalReceiverOffset));
 
     __ bind(&patch_receiver);
-    __ movq(Operand(rsp, rax, times_pointer_size, 0), rbx);
+    __ movq(Operand(rsp, rax, times_stack_pointer_size, 0), rbx);
 
     __ jmp(&shift_arguments);
   }
@@ -274,7 +274,7 @@
   //     receiver, so overwrite the first argument which will ultimately
   //     become the receiver.
   __ bind(&non_function);
-  __ movq(Operand(rsp, rax, times_pointer_size, 0), rdi);
+  __ movq(Operand(rsp, rax, times_stack_pointer_size, 0), rdi);
   __ xor_(rdi, rdi);
 
   // 4. Shift arguments and return address one slot down on the stack
@@ -284,8 +284,8 @@
   { Label loop;
     __ movq(rcx, rax);
     __ bind(&loop);
-    __ movq(rbx, Operand(rsp, rcx, times_pointer_size, 0));
-    __ movq(Operand(rsp, rcx, times_pointer_size, 1 * kPointerSize), rbx);
+    __ movq(rbx, Operand(rsp, rcx, times_stack_pointer_size, 0));
+    __ movq(Operand(rsp, rcx, times_stack_pointer_size, 1 * kStackPointerSize), rbx);
     __ decq(rcx);
     __ j(not_sign, &loop);  // While non-negative (to copy return address).
     __ pop(rbx);  // Discard copy of return address.
@@ -312,6 +312,9 @@
                           SharedFunctionInfo::kFormalParameterCountOffset));
   __ movq(rdx, FieldOperand(rdx, SharedFunctionInfo::kCodeOffset));
   __ lea(rdx, FieldOperand(rdx, Code::kHeaderSize));
+#ifdef NACL
+  NACL_PATCH_INSTRUCTION_START(rdx);
+#endif
   __ cmpq(rax, rbx);
   __ j(not_equal,
        Handle<Code>(builtin(ArgumentsAdaptorTrampoline)),
@@ -335,9 +338,9 @@
   // rbp[2]: function arguments
   // rbp[3]: receiver
   // rbp[4]: function
-  static const int kArgumentsOffset = 2 * kPointerSize;
-  static const int kReceiverOffset = 3 * kPointerSize;
-  static const int kFunctionOffset = 4 * kPointerSize;
+  static const int kArgumentsOffset = 2 * kStackPointerSize;
+  static const int kReceiverOffset = 3 * kStackPointerSize;
+  static const int kFunctionOffset = 4 * kStackPointerSize;
   __ push(Operand(rbp, kFunctionOffset));
   __ push(Operand(rbp, kArgumentsOffset));
   __ InvokeBuiltin(Builtins::APPLY_PREPARE, CALL_FUNCTION);
@@ -353,7 +356,11 @@
   __ subq(rcx, kScratchRegister);
   // Make rdx the space we need for the array when it is unrolled onto the
   // stack.
-  __ PositiveSmiTimesPowerOfTwoToInteger64(rdx, rax, kPointerSizeLog2);
+#ifdef NACL
+  __ PositiveSmiTimesPowerOfTwoToInteger32(rdx, rax, kStackPointerSizeLog2);
+#else
+  __ PositiveSmiTimesPowerOfTwoToInteger64(rdx, rax, kStackPointerSizeLog2);
+#endif
   // Check if the arguments will overflow the stack.
   __ cmpq(rcx, rdx);
   __ j(greater, &okay);  // Signed comparison.
@@ -367,8 +374,8 @@
 
   // Push current index and limit.
   const int kLimitOffset =
-      StandardFrameConstants::kExpressionsOffset - 1 * kPointerSize;
-  const int kIndexOffset = kLimitOffset - 1 * kPointerSize;
+      StandardFrameConstants::kExpressionsOffset - 1 * kStackPointerSize;
+  const int kIndexOffset = kLimitOffset - 1 * kStackPointerSize;
   __ push(rax);  // limit
   __ push(Immediate(0));  // index
 
@@ -447,7 +454,7 @@
   __ InvokeFunction(rdi, actual, CALL_FUNCTION);
 
   __ LeaveInternalFrame();
-  __ ret(3 * kPointerSize);  // remove function, receiver, and arguments
+  __ ret(3 * kStackPointerSize);  // remove function, receiver, and arguments
 }
 
 
@@ -708,14 +715,14 @@
                        call_generic_code);
   __ IncrementCounter(&Counters::array_function_native, 1);
   __ movq(rax, rbx);
-  __ ret(kPointerSize);
+  __ ret(kStackPointerSize);
 
   // Check for one argument. Bail out if argument is not smi or if it is
   // negative.
   __ bind(&argc_one_or_more);
   __ cmpq(rax, Immediate(1));
   __ j(not_equal, &argc_two_or_more);
-  __ movq(rdx, Operand(rsp, kPointerSize));  // Get the argument from the stack.
+  __ movq(rdx, Operand(rsp, kStackPointerSize));  // Get the argument from the stack.
   __ JumpIfNotPositiveSmi(rdx, call_generic_code);
 
   // Handle construction of an empty array of a certain size. Bail out if size
@@ -739,7 +746,7 @@
                   call_generic_code);
   __ IncrementCounter(&Counters::array_function_native, 1);
   __ movq(rax, rbx);
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 
   // Handle construction of an array from a list of arguments.
   __ bind(&argc_two_or_more);
@@ -769,7 +776,7 @@
   // esp[8]: last argument
 
   // Location of the last argument
-  __ lea(r9, Operand(rsp, kPointerSize));
+  __ lea(r9, Operand(rsp, kStackPointerSize));
 
   // Location of the first array element (Parameter fill_with_holes to
   // AllocateJSArrayis false, so the FixedArray is returned in rcx).
@@ -785,7 +792,7 @@
   __ movq(rcx, rax);
   __ jmp(&entry);
   __ bind(&loop);
-  __ movq(kScratchRegister, Operand(r9, rcx, times_pointer_size, 0));
+  __ movq(kScratchRegister, Operand(r9, rcx, times_stack_pointer_size, 0));
   __ movq(Operand(rdx, 0), kScratchRegister);
   __ addq(rdx, Immediate(kPointerSize));
   __ bind(&entry);
@@ -798,7 +805,7 @@
   // esp[0]: return address
   // esp[8]: last argument
   __ pop(rcx);
-  __ lea(rsp, Operand(rsp, rax, times_pointer_size, 1 * kPointerSize));
+  __ lea(rsp, Operand(rsp, rax, times_stack_pointer_size, 1 * kStackPointerSize));
   __ push(rcx);
   __ movq(rax, rbx);
   __ ret(0);
@@ -893,6 +900,9 @@
   __ movq(rbx, FieldOperand(rdi, JSFunction::kSharedFunctionInfoOffset));
   __ movq(rbx, FieldOperand(rbx, SharedFunctionInfo::kConstructStubOffset));
   __ lea(rbx, FieldOperand(rbx, Code::kHeaderSize));
+#ifdef NACL
+  NACL_PATCH_INSTRUCTION_START(rbx);
+#endif
   __ jmp(rbx);
 
   // edi: called object
@@ -901,7 +911,7 @@
   // CALL_NON_FUNCTION expects the non-function constructor as receiver
   // (instead of the original receiver from the call site).  The receiver is
   // stack element argc+1.
-  __ movq(Operand(rsp, rax, times_pointer_size, kPointerSize), rdi);
+  __ movq(Operand(rsp, rax, times_stack_pointer_size, kStackPointerSize), rdi);
   // Set expected number of arguments to zero (not changing rax).
   __ movq(rbx, Immediate(0));
   __ GetBuiltinEntry(rdx, Builtins::CALL_NON_FUNCTION_AS_CONSTRUCTOR);
@@ -1021,7 +1031,7 @@
     // rdi: start of next object (will be start of FixedArray)
     // rdx: number of elements in properties array
     __ AllocateInNewSpace(FixedArray::kHeaderSize,
-                          times_pointer_size,
+                          times_heap_pointer_size,
                           rdx,
                           rdi,
                           rax,
@@ -1109,7 +1119,7 @@
   __ movq(rcx, rax);
   __ jmp(&entry);
   __ bind(&loop);
-  __ push(Operand(rbx, rcx, times_pointer_size, 0));
+  __ push(Operand(rbx, rcx, times_stack_pointer_size, 0));
   __ bind(&entry);
   __ decq(rcx);
   __ j(greater_equal, &loop);
@@ -1149,13 +1159,13 @@
 
   // Restore the arguments count and leave the construct frame.
   __ bind(&exit);
-  __ movq(rbx, Operand(rsp, kPointerSize));  // get arguments count
+  __ movq(rbx, Operand(rsp, kStackPointerSize));  // get arguments count
   __ LeaveConstructFrame();
 
   // Remove caller arguments from the stack and return.
   __ pop(rcx);
-  SmiIndex index = masm->SmiToIndex(rbx, rbx, kPointerSizeLog2);
-  __ lea(rsp, Operand(rsp, index.reg, index.scale, 1 * kPointerSize));
+  SmiIndex index = masm->SmiToIndex(rbx, rbx, kStackPointerSizeLog2);
+  __ lea(rsp, Operand(rsp, index.reg, index.scale, 1 * kStackPointerSize));
   __ push(rcx);
   __ IncrementCounter(&Counters::constructed_objects, 1);
   __ ret(0);
@@ -1259,7 +1269,7 @@
   __ xor_(rcx, rcx);  // Set loop variable to 0.
   __ jmp(&entry);
   __ bind(&loop);
-  __ movq(kScratchRegister, Operand(rbx, rcx, times_pointer_size, 0));
+  __ movq(kScratchRegister, Operand(rbx, rcx, times_heap_pointer_size, 0));
   __ push(Operand(kScratchRegister, 0));  // dereference handle
   __ addq(rcx, Immediate(1));
   __ bind(&entry);
@@ -1282,7 +1292,7 @@
   // invocation.
   __ LeaveInternalFrame();
   // TODO(X64): Is argument correct? Is there a receiver to remove?
-  __ ret(1 * kPointerSize);  // remove receiver
+  __ ret(1 * kStackPointerSize);  // remove receiver
 }
 
 
Index: src/x64/ic-x64.cc
===================================================================
--- src/x64/ic-x64.cc	(revision 4925)
+++ src/x64/ic-x64.cc	(working copy)
@@ -45,71 +45,93 @@
 #define __ ACCESS_MASM(masm)
 
 
+static void GenerateGlobalInstanceTypeCheck(MacroAssembler* masm,
+                                            Register type,
+                                            Label* global_object) {
+  // Register usage:
+  //   type: holds the receiver instance type on entry.
+  __ cmpb(type, Immediate(JS_GLOBAL_OBJECT_TYPE));
+  __ j(equal, global_object);
+  __ cmpb(type, Immediate(JS_BUILTINS_OBJECT_TYPE));
+  __ j(equal, global_object);
+  __ cmpb(type, Immediate(JS_GLOBAL_PROXY_TYPE));
+  __ j(equal, global_object);
+}
+
+
+// Generated code falls through if the receiver is a regular non-global
+// JS object with slow properties and no interceptors.
+static void GenerateDictionaryLoadReceiverCheck(MacroAssembler* masm,
+                                                Register receiver,
+                                                Register r0,
+                                                Register r1,
+                                                Label* miss) {
+  // Register usage:
+  //   receiver: holds the receiver on entry and is unchanged.
+  //   r0: used to hold receiver instance type.
+  //       Holds the property dictionary on fall through.
+  //   r1: used to hold receivers map.
+
+  __ JumpIfSmi(receiver, miss);
+
+  // Check that the receiver is a valid JS object.
+  __ movq(r1, FieldOperand(receiver, HeapObject::kMapOffset));
+  __ movb(r0, FieldOperand(r1, Map::kInstanceTypeOffset));
+  __ cmpb(r0, Immediate(FIRST_JS_OBJECT_TYPE));
+  __ j(below, miss);
+
+  // If this assert fails, we have to check upper bound too.
+  ASSERT(LAST_TYPE == JS_FUNCTION_TYPE);
+
+  GenerateGlobalInstanceTypeCheck(masm, r0, miss);
+
+  // Check for non-global object that requires access check.
+  __ testb(FieldOperand(r1, Map::kBitFieldOffset),
+           Immediate((1 << Map::kIsAccessCheckNeeded) |
+                     (1 << Map::kHasNamedInterceptor)));
+  __ j(not_zero, miss);
+
+  __ movq(r0, FieldOperand(receiver, JSObject::kPropertiesOffset));
+  __ CompareRoot(FieldOperand(r0, HeapObject::kMapOffset),
+                 Heap::kHashTableMapRootIndex);
+  __ j(not_equal, miss);
+}
+
+
 // Helper function used to load a property from a dictionary backing storage.
 // This function may return false negatives, so miss_label
 // must always call a backup property load that is complete.
-// This function is safe to call if the receiver has fast properties,
-// or if name is not a symbol, and will jump to the miss_label in that case.
+// This function is safe to call if name is not a symbol, and will jump to
+// the miss_label in that case.
+// The generated code assumes that the receiver has slow properties,
+// is not a global object and does not have interceptors.
 static void GenerateDictionaryLoad(MacroAssembler* masm,
                                    Label* miss_label,
+                                   Register elements,
+                                   Register name,
                                    Register r0,
                                    Register r1,
-                                   Register r2,
-                                   Register name,
-                                   Register r4,
-                                   Register result,
-                                   DictionaryCheck check_dictionary) {
+                                   Register result) {
   // Register use:
   //
-  // r0   - used to hold the property dictionary and is unchanged.
+  // elements - holds the property dictionary on entry and is unchanged.
   //
-  // r1   - used to hold the receiver and is unchanged.
+  // name - holds the name of the property on entry and is unchanged.
   //
-  // r2   - used to hold the capacity of the property dictionary.
+  // r0   - used to hold the capacity of the property dictionary.
   //
-  // name - holds the name of the property and is unchanged.
+  // r1   - used to hold the index into the property dictionary.
   //
-  // r4   - used to hold the index into the property dictionary.
-  //
   // result - holds the result on exit if the load succeeded.
 
   Label done;
 
-  // Check for the absence of an interceptor.
-  // Load the map into r0.
-  __ movq(r0, FieldOperand(r1, JSObject::kMapOffset));
-
-  // Bail out if the receiver has a named interceptor.
-  __ testl(FieldOperand(r0, Map::kBitFieldOffset),
-           Immediate(1 << Map::kHasNamedInterceptor));
-  __ j(not_zero, miss_label);
-
-  // Bail out if we have a JS global proxy object.
-  __ movzxbq(r0, FieldOperand(r0, Map::kInstanceTypeOffset));
-  __ cmpb(r0, Immediate(JS_GLOBAL_PROXY_TYPE));
-  __ j(equal, miss_label);
-
-  // Possible work-around for http://crbug.com/16276.
-  __ cmpb(r0, Immediate(JS_GLOBAL_OBJECT_TYPE));
-  __ j(equal, miss_label);
-  __ cmpb(r0, Immediate(JS_BUILTINS_OBJECT_TYPE));
-  __ j(equal, miss_label);
-
-  // Load properties array.
-  __ movq(r0, FieldOperand(r1, JSObject::kPropertiesOffset));
-
-  if (check_dictionary == CHECK_DICTIONARY) {
-    // Check that the properties array is a dictionary.
-    __ Cmp(FieldOperand(r0, HeapObject::kMapOffset), Factory::hash_table_map());
-    __ j(not_equal, miss_label);
-  }
-
   // Compute the capacity mask.
   const int kCapacityOffset =
       StringDictionary::kHeaderSize +
       StringDictionary::kCapacityIndex * kPointerSize;
-  __ SmiToInteger32(r2, FieldOperand(r0, kCapacityOffset));
-  __ decl(r2);
+  __ SmiToInteger32(r0, FieldOperand(elements, kCapacityOffset));
+  __ decl(r0);
 
   // Generate an unrolled loop that performs a few probes before
   // giving up. Measurements done on Gmail indicate that 2 probes
@@ -120,19 +142,19 @@
       StringDictionary::kElementsStartIndex * kPointerSize;
   for (int i = 0; i < kProbes; i++) {
     // Compute the masked index: (hash + i + i * i) & mask.
-    __ movl(r4, FieldOperand(name, String::kHashFieldOffset));
-    __ shrl(r4, Immediate(String::kHashShift));
+    __ movl(r1, FieldOperand(name, String::kHashFieldOffset));
+    __ shrl(r1, Immediate(String::kHashShift));
     if (i > 0) {
-      __ addl(r4, Immediate(StringDictionary::GetProbeOffset(i)));
+      __ addl(r1, Immediate(StringDictionary::GetProbeOffset(i)));
     }
-    __ and_(r4, r2);
+    __ and_(r1, r0);
 
     // Scale the index by multiplying by the entry size.
     ASSERT(StringDictionary::kEntrySize == 3);
-    __ lea(r4, Operand(r4, r4, times_2, 0));  // r4 = r4 * 3
+    __ lea(r1, Operand(r1, r1, times_2, 0));  // r1 = r1 * 3
 
     // Check if the key is identical to the name.
-    __ cmpq(name, Operand(r0, r4, times_pointer_size,
+    __ cmpq(name, Operand(elements, r1, times_heap_pointer_size,
                           kElementsStartOffset - kHeapObjectTag));
     if (i != kProbes - 1) {
       __ j(equal, &done);
@@ -144,14 +166,16 @@
   // Check that the value is a normal property.
   __ bind(&done);
   const int kDetailsOffset = kElementsStartOffset + 2 * kPointerSize;
-  __ Test(Operand(r0, r4, times_pointer_size, kDetailsOffset - kHeapObjectTag),
+  __ Test(Operand(elements, r1, times_heap_pointer_size,
+                  kDetailsOffset - kHeapObjectTag),
           Smi::FromInt(PropertyDetails::TypeField::mask()));
   __ j(not_zero, miss_label);
 
   // Get the value at the masked, scaled index.
   const int kValueOffset = kElementsStartOffset + kPointerSize;
   __ movq(result,
-          Operand(r0, r4, times_pointer_size, kValueOffset - kHeapObjectTag));
+          Operand(elements, r1, times_heap_pointer_size,
+                  kValueOffset - kHeapObjectTag));
 }
 
 
@@ -234,7 +258,7 @@
     // Check if the key matches.
     __ cmpq(key, FieldOperand(elements,
                               r2,
-                              times_pointer_size,
+                              times_heap_pointer_size,
                               NumberDictionary::kElementsStartOffset));
     if (i != (kProbes - 1)) {
       __ j(equal, &done);
@@ -248,14 +272,14 @@
   const int kDetailsOffset =
       NumberDictionary::kElementsStartOffset + 2 * kPointerSize;
   ASSERT_EQ(NORMAL, 0);
-  __ Test(FieldOperand(elements, r2, times_pointer_size, kDetailsOffset),
+  __ Test(FieldOperand(elements, r2, times_heap_pointer_size, kDetailsOffset),
           Smi::FromInt(PropertyDetails::TypeField::mask()));
   __ j(not_zero, miss);
 
   // Get the value at the masked, scaled index.
   const int kValueOffset =
       NumberDictionary::kElementsStartOffset + kPointerSize;
-  __ movq(result, FieldOperand(elements, r2, times_pointer_size, kValueOffset));
+  __ movq(result, FieldOperand(elements, r2, times_heap_pointer_size, kValueOffset));
 }
 
 
@@ -283,7 +307,11 @@
   // to the offset to get the map address.
   Address map_address = test_instruction_address + delta + 2;
   // Patch the map check.
+#ifdef NACL
+  NaClCode::PatchWord(reinterpret_cast<Object**>(map_address), map);
+#else
   *(reinterpret_cast<Object**>(map_address)) = map;
+#endif
   return true;
 }
 
@@ -327,6 +355,8 @@
   //  -- rsp[0]  : return address
   // -----------------------------------
 
+  __ IncrementCounter(&Counters::keyed_load_miss, 1);
+
   __ pop(rbx);
   __ push(rdx);  // receiver
   __ push(rax);  // name
@@ -360,6 +390,7 @@
 static void GenerateKeyedLoadReceiverCheck(MacroAssembler* masm,
                                            Register receiver,
                                            Register map,
+                                           int interceptor_bit,
                                            Label* slow) {
   // Register use:
   //   receiver - holds the receiver and is unchanged.
@@ -379,7 +410,8 @@
 
   // Check bit field.
   __ testb(FieldOperand(map, Map::kBitFieldOffset),
-           Immediate(KeyedLoadIC::kSlowCaseBitFieldMask));
+           Immediate((1 << Map::kIsAccessCheckNeeded) |
+                     (1 << interceptor_bit)));
   __ j(not_zero, slow);
 }
 
@@ -500,14 +532,15 @@
   Label slow, check_string, index_smi, index_string;
   Label check_pixel_array, probe_dictionary, check_number_dictionary;
 
-  GenerateKeyedLoadReceiverCheck(masm, rdx, rcx, &slow);
-
   // Check that the key is a smi.
   __ JumpIfNotSmi(rax, &check_string);
   __ bind(&index_smi);
   // Now the key is known to be a smi. This place is also jumped to from below
   // where a numeric string is converted to a smi.
 
+  GenerateKeyedLoadReceiverCheck(
+      masm, rdx, rcx, Map::kHasIndexedInterceptor, &slow);
+
   GenerateFastArrayLoad(masm,
                         rdx,
                         rax,
@@ -557,6 +590,9 @@
   __ bind(&check_string);
   GenerateKeyStringCheck(masm, rax, rcx, rbx, &index_string, &slow);
 
+  GenerateKeyedLoadReceiverCheck(
+      masm, rdx, rcx, Map::kHasNamedInterceptor, &slow);
+
   // If the receiver is a fast-case object, check the keyed lookup
   // cache. Otherwise probe the dictionary leaving result in rcx.
   __ movq(rbx, FieldOperand(rdx, JSObject::kPropertiesOffset));
@@ -599,7 +635,7 @@
   // Load in-object property.
   __ movzxbq(rcx, FieldOperand(rbx, Map::kInstanceSizeOffset));
   __ addq(rcx, rdi);
-  __ movq(rax, FieldOperand(rdx, rcx, times_pointer_size, 0));
+  __ movq(rax, FieldOperand(rdx, rcx, times_heap_pointer_size, 0));
   __ IncrementCounter(&Counters::keyed_load_generic_lookup_cache, 1);
   __ ret(0);
 
@@ -608,15 +644,13 @@
   __ bind(&probe_dictionary);
   // rdx: receiver
   // rax: key
-  GenerateDictionaryLoad(masm,
-                         &slow,
-                         rbx,
-                         rdx,
-                         rcx,
-                         rax,
-                         rdi,
-                         rax,
-                         DICTIONARY_CHECK_DONE);
+  // rbx: elements
+
+  __ movq(rcx, FieldOperand(rdx, JSObject::kMapOffset));
+  __ movb(rcx, FieldOperand(rcx, Map::kInstanceTypeOffset));
+  GenerateGlobalInstanceTypeCheck(masm, rcx, &slow);
+
+  GenerateDictionaryLoad(masm, &slow, rbx, rax, rcx, rdi, rax);
   __ IncrementCounter(&Counters::keyed_load_generic_symbol, 1);
   __ ret(0);
 
@@ -672,7 +706,7 @@
   //  -- rdx    : receiver
   //  -- rsp[0] : return address
   // -----------------------------------
-  Label slow, failed_allocation;
+  Label slow;
 
   // Check that the object isn't a smi.
   __ JumpIfSmi(rdx, &slow);
@@ -731,7 +765,7 @@
       __ movl(rcx, Operand(rbx, rcx, times_4, 0));
       break;
     case kExternalFloatArray:
-      __ fld_s(Operand(rbx, rcx, times_4, 0));
+      __ cvtss2sd(xmm0, Operand(rbx, rcx, times_4, 0));
       break;
     default:
       UNREACHABLE();
@@ -743,21 +777,22 @@
   // For integer array types:
   // rcx: value
   // For floating-point array type:
-  // FP(0): value
+  // xmm0: value as double.
 
-  if (array_type == kExternalIntArray ||
-      array_type == kExternalUnsignedIntArray) {
-    // For the Int and UnsignedInt array types, we need to see whether
+#ifdef NACL
+  ASSERT(kSmiValueSize == 31);
+#else
+  ASSERT(kSmiValueSize == 32);
+#endif
+                        
+  if (array_type == kExternalUnsignedIntArray) {
+    // For the UnsignedInt array type, we need to see whether
     // the value can be represented in a Smi. If not, we need to convert
     // it to a HeapNumber.
     Label box_int;
-    if (array_type == kExternalIntArray) {
-      __ JumpIfNotValidSmiValue(rcx, &box_int);
-    } else {
-      ASSERT_EQ(array_type, kExternalUnsignedIntArray);
-      __ JumpIfUIntNotValidSmiValue(rcx, &box_int);
-    }
 
+    __ JumpIfUIntNotValidSmiValue(rcx, &box_int);
+
     __ Integer32ToSmi(rax, rcx);
     __ ret(0);
 
@@ -765,42 +800,28 @@
 
     // Allocate a HeapNumber for the int and perform int-to-double
     // conversion.
-    __ push(rcx);
-    if (array_type == kExternalIntArray) {
-      __ fild_s(Operand(rsp, 0));
-    } else {
-      ASSERT(array_type == kExternalUnsignedIntArray);
-      // The value is zero-extended on the stack, because all pushes are
-      // 64-bit and we loaded the value from memory with movl.
-      __ fild_d(Operand(rsp, 0));
-    }
-    __ pop(rcx);
-    // FP(0): value
-    __ AllocateHeapNumber(rcx, rbx, &failed_allocation);
+    // The value is zero-extended since we loaded the value from memory
+    // with movl.
+    __ cvtqsi2sd(xmm0, rcx);
+
+    __ AllocateHeapNumber(rcx, rbx, &slow);
     // Set the value.
+    __ movsd(FieldOperand(rcx, HeapNumber::kValueOffset), xmm0);
     __ movq(rax, rcx);
-    __ fstp_d(FieldOperand(rax, HeapNumber::kValueOffset));
     __ ret(0);
   } else if (array_type == kExternalFloatArray) {
     // For the floating-point array type, we need to always allocate a
     // HeapNumber.
-    __ AllocateHeapNumber(rcx, rbx, &failed_allocation);
+    __ AllocateHeapNumber(rcx, rbx, &slow);
     // Set the value.
+    __ movsd(FieldOperand(rcx, HeapNumber::kValueOffset), xmm0);
     __ movq(rax, rcx);
-    __ fstp_d(FieldOperand(rax, HeapNumber::kValueOffset));
     __ ret(0);
   } else {
     __ Integer32ToSmi(rax, rcx);
     __ ret(0);
   }
 
-  // If we fail allocation of the HeapNumber, we still have a value on
-  // top of the FPU stack. Remove it.
-  __ bind(&failed_allocation);
-  __ ffree();
-  __ fincstp();
-  // Fall through to slow case.
-
   // Slow case: Jump to runtime.
   __ bind(&slow);
   __ IncrementCounter(&Counters::keyed_load_external_array_slow, 1);
@@ -1004,7 +1025,7 @@
   // rbx: receiver's elements array (a FixedArray)
   // rcx: index
   Label non_smi_value;
-  __ movq(FieldOperand(rbx, rcx, times_pointer_size, FixedArray::kHeaderSize),
+  __ movq(FieldOperand(rbx, rcx, times_heap_pointer_size, FixedArray::kHeaderSize),
           rax);
   __ JumpIfNotSmi(rax, &non_smi_value);
   __ ret(0);
@@ -1086,10 +1107,8 @@
       break;
     case kExternalFloatArray:
       // Need to perform int-to-float conversion.
-      __ push(rdx);
-      __ fild_s(Operand(rsp, 0));
-      __ pop(rdx);
-      __ fstp_s(Operand(rbx, rdi, times_4, 0));
+      __ cvtlsi2ss(xmm0, rdx);
+      __ movss(Operand(rbx, rdi, times_4, 0), xmm0);
       break;
     default:
       UNREACHABLE();
@@ -1110,53 +1129,41 @@
   // The WebGL specification leaves the behavior of storing NaN and
   // +/-Infinity into integer arrays basically undefined. For more
   // reproducible behavior, convert these to zero.
-  __ fld_d(FieldOperand(rax, HeapNumber::kValueOffset));
+  __ movsd(xmm0, FieldOperand(rax, HeapNumber::kValueOffset));
   __ movq(rbx, FieldOperand(rbx, ExternalArray::kExternalPointerOffset));
   // rdi: untagged index
   // rbx: base pointer of external storage
   // top of FPU stack: value
   if (array_type == kExternalFloatArray) {
-    __ fstp_s(Operand(rbx, rdi, times_4, 0));
+    __ cvtsd2ss(xmm0, xmm0);
+    __ movss(Operand(rbx, rdi, times_4, 0), xmm0);
     __ ret(0);
   } else {
     // Need to perform float-to-int conversion.
-    // Test the top of the FP stack for NaN.
-    Label is_nan;
-    __ fucomi(0);
-    __ j(parity_even, &is_nan);
+    // Test the value for NaN.
 
-    __ push(rdx);  // Make room on the stack.  Receiver is no longer needed.
-    __ fistp_d(Operand(rsp, 0));
-    __ pop(rdx);
+    // Convert to int32 and store the low byte/word.
+    // If the value is NaN or +/-infinity, the result is 0x80000000,
+    // which is automatically zero when taken mod 2^n, n < 32.
     // rdx: value (converted to an untagged integer)
     // rdi: untagged index
     // rbx: base pointer of external storage
     switch (array_type) {
       case kExternalByteArray:
       case kExternalUnsignedByteArray:
+        __ cvtsd2si(rdx, xmm0);
         __ movb(Operand(rbx, rdi, times_1, 0), rdx);
         break;
       case kExternalShortArray:
       case kExternalUnsignedShortArray:
+        __ cvtsd2si(rdx, xmm0);
         __ movw(Operand(rbx, rdi, times_2, 0), rdx);
         break;
       case kExternalIntArray:
       case kExternalUnsignedIntArray: {
-        // We also need to explicitly check for +/-Infinity. These are
-        // converted to MIN_INT, but we need to be careful not to
-        // confuse with legal uses of MIN_INT.  Since MIN_INT truncated
-        // to 8 or 16 bits is zero, we only perform this test when storing
-        // 32-bit ints.
-        Label not_infinity;
-        // This test would apparently detect both NaN and Infinity,
-        // but we've already checked for NaN using the FPU hardware
-        // above.
-        __ movzxwq(rcx, FieldOperand(rax, HeapNumber::kValueOffset + 6));
-        __ and_(rcx, Immediate(0x7FF0));
-        __ cmpw(rcx, Immediate(0x7FF0));
-        __ j(not_equal, &not_infinity);
-        __ movq(rdx, Immediate(0));
-        __ bind(&not_infinity);
+        // Convert to int64, so that NaN and infinities become
+        // 0x8000000000000000, which is zero mod 2^32.
+        __ cvtsd2siq(rdx, xmm0);
         __ movl(Operand(rbx, rdi, times_4, 0), rdx);
         break;
       }
@@ -1165,31 +1172,6 @@
         break;
     }
     __ ret(0);
-
-    __ bind(&is_nan);
-    // rdi: untagged index
-    // rbx: base pointer of external storage
-    __ ffree();
-    __ fincstp();
-    __ movq(rdx, Immediate(0));
-    switch (array_type) {
-      case kExternalByteArray:
-      case kExternalUnsignedByteArray:
-        __ movb(Operand(rbx, rdi, times_1, 0), rdx);
-        break;
-      case kExternalShortArray:
-      case kExternalUnsignedShortArray:
-        __ movw(Operand(rbx, rdi, times_2, 0), rdx);
-        break;
-      case kExternalIntArray:
-      case kExternalUnsignedIntArray:
-        __ movl(Operand(rbx, rdi, times_4, 0), rdx);
-        break;
-      default:
-        UNREACHABLE();
-        break;
-    }
-    __ ret(0);
   }
 
   // Slow case: call runtime.
@@ -1212,8 +1194,15 @@
   // rsp[argc * 8]            : argument 1
   // rsp[(argc + 1) * 8]      : argument 0 = receiver
   // -----------------------------------
+
+  if (id == IC::kCallIC_Miss) {
+    __ IncrementCounter(&Counters::call_miss, 1);
+  } else {
+    __ IncrementCounter(&Counters::keyed_call_miss, 1);
+  }
+
   // Get the receiver of the function from the stack; 1 ~ return address.
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+  __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
 
   // Enter an internal frame.
   __ EnterInternalFrame();
@@ -1233,22 +1222,25 @@
   __ LeaveInternalFrame();
 
   // Check if the receiver is a global object of some sort.
-  Label invoke, global;
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));  // receiver
-  __ JumpIfSmi(rdx, &invoke);
-  __ CmpObjectType(rdx, JS_GLOBAL_OBJECT_TYPE, rcx);
-  __ j(equal, &global);
-  __ CmpInstanceType(rcx, JS_BUILTINS_OBJECT_TYPE);
-  __ j(not_equal, &invoke);
+  // This can happen only for regular CallIC but not KeyedCallIC.
+  if (id == IC::kCallIC_Miss) {
+    Label invoke, global;
+    __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));  // receiver
+    __ JumpIfSmi(rdx, &invoke);
+    __ CmpObjectType(rdx, JS_GLOBAL_OBJECT_TYPE, rcx);
+    __ j(equal, &global);
+    __ CmpInstanceType(rcx, JS_BUILTINS_OBJECT_TYPE);
+    __ j(not_equal, &invoke);
 
-  // Patch the receiver on the stack.
-  __ bind(&global);
-  __ movq(rdx, FieldOperand(rdx, GlobalObject::kGlobalReceiverOffset));
-  __ movq(Operand(rsp, (argc + 1) * kPointerSize), rdx);
+    // Patch the receiver on the stack.
+    __ bind(&global);
+    __ movq(rdx, FieldOperand(rdx, GlobalObject::kGlobalReceiverOffset));
+    __ movq(Operand(rsp, (argc + 1) * kStackPointerSize), rdx);
+    __ bind(&invoke);
+  }
 
   // Invoke the function.
   ParameterCount actual(argc);
-  __ bind(&invoke);
   __ InvokeFunction(rdi, actual, JUMP_FUNCTION);
 }
 
@@ -1309,13 +1301,12 @@
 }
 
 
-static void GenerateNormalHelper(MacroAssembler* masm,
-                                 int argc,
-                                 bool is_global_object,
-                                 Label* miss) {
+static void GenerateFunctionTailCall(MacroAssembler* masm,
+                                     int argc,
+                                     Label* miss) {
   // ----------- S t a t e -------------
   // rcx                    : function name
-  // rdx                    : receiver
+  // rdi                    : function
   // rsp[0]                 : return address
   // rsp[8]                 : argument argc
   // rsp[16]                : argument argc - 1
@@ -1323,21 +1314,11 @@
   // rsp[argc * 8]          : argument 1
   // rsp[(argc + 1) * 8]    : argument 0 = receiver
   // -----------------------------------
-  // Search dictionary - put result in register rdx.
-  GenerateDictionaryLoad(
-     masm, miss, rax, rdx, rbx, rcx, rdi, rdi, CHECK_DICTIONARY);
-
   __ JumpIfSmi(rdi, miss);
   // Check that the value is a JavaScript function.
   __ CmpObjectType(rdi, JS_FUNCTION_TYPE, rdx);
   __ j(not_equal, miss);
 
-  // Patch the receiver with the global proxy if necessary.
-  if (is_global_object) {
-    __ movq(rdx, FieldOperand(rdx, GlobalObject::kGlobalReceiverOffset));
-    __ movq(Operand(rsp, (argc + 1) * kPointerSize), rdx);
-  }
-
   // Invoke the function.
   ParameterCount actual(argc);
   __ InvokeFunction(rdi, actual, JUMP_FUNCTION);
@@ -1355,57 +1336,19 @@
   // rsp[argc * 8]          : argument 1
   // rsp[(argc + 1) * 8]    : argument 0 = receiver
   // -----------------------------------
-  Label miss, global_object, non_global_object;
+  Label miss;
 
   // Get the receiver of the function from the stack.
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+  __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
 
-  // Check that the receiver isn't a smi.
-  __ JumpIfSmi(rdx, &miss);
+  GenerateDictionaryLoadReceiverCheck(masm, rdx, rax, rbx, &miss);
 
-  // Check that the receiver is a valid JS object.
-  // Because there are so many map checks and type checks, do not
-  // use CmpObjectType, but load map and type into registers.
-  __ movq(rbx, FieldOperand(rdx, HeapObject::kMapOffset));
-  __ movb(rax, FieldOperand(rbx, Map::kInstanceTypeOffset));
-  __ cmpb(rax, Immediate(FIRST_JS_OBJECT_TYPE));
-  __ j(below, &miss);
+  // rax: elements
+  // Search the dictionary placing the result in rdi.
+  GenerateDictionaryLoad(masm, &miss, rax, rcx, rbx, rdi, rdi);
 
-  // If this assert fails, we have to check upper bound too.
-  ASSERT(LAST_TYPE == JS_FUNCTION_TYPE);
+  GenerateFunctionTailCall(masm, argc, &miss);
 
-  // Check for access to global object.
-  __ cmpb(rax, Immediate(JS_GLOBAL_OBJECT_TYPE));
-  __ j(equal, &global_object);
-  __ cmpb(rax, Immediate(JS_BUILTINS_OBJECT_TYPE));
-  __ j(not_equal, &non_global_object);
-
-  // Accessing global object: Load and invoke.
-  __ bind(&global_object);
-  // Check that the global object does not require access checks.
-  __ movb(rbx, FieldOperand(rbx, Map::kBitFieldOffset));
-  __ testb(rbx, Immediate(1 << Map::kIsAccessCheckNeeded));
-  __ j(not_equal, &miss);
-  GenerateNormalHelper(masm, argc, true, &miss);
-
-  // Accessing non-global object: Check for access to global proxy.
-  Label global_proxy, invoke;
-  __ bind(&non_global_object);
-  __ cmpb(rax, Immediate(JS_GLOBAL_PROXY_TYPE));
-  __ j(equal, &global_proxy);
-  // Check that the non-global, non-global-proxy object does not
-  // require access checks.
-  __ movb(rbx, FieldOperand(rbx, Map::kBitFieldOffset));
-  __ testb(rbx, Immediate(1 << Map::kIsAccessCheckNeeded));
-  __ j(not_equal, &miss);
-  __ bind(&invoke);
-  GenerateNormalHelper(masm, argc, false, &miss);
-
-  // Global object proxy access: Check access rights.
-  __ bind(&global_proxy);
-  __ CheckAccessGlobalProxy(rdx, rax, &miss);
-  __ jmp(&invoke);
-
   __ bind(&miss);
 }
 
@@ -1436,7 +1379,7 @@
   // -----------------------------------
 
   // Get the receiver of the function from the stack; 1 ~ return address.
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+  __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
   GenerateMonomorphicCacheProbe(masm, argc, Code::CALL_IC);
   GenerateMiss(masm, argc);
 }
@@ -1485,7 +1428,7 @@
   // -----------------------------------
 
   // Get the receiver of the function from the stack; 1 ~ return address.
-  __ movq(rdx, Operand(rsp, (argc + 1) * kPointerSize));
+  __ movq(rdx, Operand(rsp, (argc + 1) * kStackPointerSize));
 
   Label do_call, slow_call, slow_load, slow_reload_receiver;
   Label check_number_dictionary, check_string, lookup_monomorphic_cache;
@@ -1498,7 +1441,8 @@
   // Now the key is known to be a smi. This place is also jumped to from below
   // where a numeric string is converted to a smi.
 
-  GenerateKeyedLoadReceiverCheck(masm, rdx, rax, &slow_call);
+  GenerateKeyedLoadReceiverCheck(
+      masm, rdx, rax, Map::kHasIndexedInterceptor, &slow_call);
 
   GenerateFastArrayLoad(
       masm, rdx, rcx, rax, rbx, rdi, &check_number_dictionary, &slow_load);
@@ -1508,21 +1452,15 @@
   // receiver in rdx is not used after this point.
   // rcx: key
   // rdi: function
+  GenerateFunctionTailCall(masm, argc, &slow_call);
 
-  // Check that the value in edi is a JavaScript function.
-  __ JumpIfSmi(rdi, &slow_call);
-  __ CmpObjectType(rdi, JS_FUNCTION_TYPE, rax);
-  __ j(not_equal, &slow_call);
-  // Invoke the function.
-  ParameterCount actual(argc);
-  __ InvokeFunction(rdi, actual, JUMP_FUNCTION);
-
   __ bind(&check_number_dictionary);
   // eax: elements
   // ecx: smi key
   // Check whether the elements is a number dictionary.
   __ CompareRoot(FieldOperand(rax, HeapObject::kMapOffset),
                  Heap::kHashTableMapRootIndex);
+  __ j(not_equal, &slow_load);
   __ SmiToInteger32(rbx, rcx);
   // ebx: untagged index
   GenerateNumberDictionaryLoad(masm, &slow_load, rax, rcx, rbx, r9, rdi, rdi);
@@ -1550,15 +1488,15 @@
   // If the receiver is a regular JS object with slow properties then do
   // a quick inline probe of the receiver's dictionary.
   // Otherwise do the monomorphic cache probe.
-  GenerateKeyedLoadReceiverCheck(masm, rdx, rax, &lookup_monomorphic_cache);
+  GenerateKeyedLoadReceiverCheck(
+      masm, rdx, rax, Map::kHasNamedInterceptor, &lookup_monomorphic_cache);
 
   __ movq(rbx, FieldOperand(rdx, JSObject::kPropertiesOffset));
   __ CompareRoot(FieldOperand(rbx, HeapObject::kMapOffset),
                  Heap::kHashTableMapRootIndex);
   __ j(not_equal, &lookup_monomorphic_cache);
 
-  GenerateDictionaryLoad(
-      masm, &slow_load, rbx, rdx, rax, rcx, rdi, rdi, DICTIONARY_CHECK_DONE);
+  GenerateDictionaryLoad(masm, &slow_load, rbx, rcx, rax, rdi, rdi);
   __ IncrementCounter(&Counters::keyed_call_generic_lookup_dict, 1);
   __ jmp(&do_call);
 
@@ -1620,6 +1558,8 @@
   //  -- rsp[0] : return address
   // -----------------------------------
 
+  __ IncrementCounter(&Counters::load_miss, 1);
+
   __ pop(rbx);
   __ push(rax);  // receiver
   __ push(rcx);  // name
@@ -1683,38 +1623,15 @@
   //  -- rcx    : name
   //  -- rsp[0] : return address
   // -----------------------------------
-  Label miss, probe, global;
+  Label miss;
 
-  // Check that the receiver isn't a smi.
-  __ JumpIfSmi(rax, &miss);
+  GenerateDictionaryLoadReceiverCheck(masm, rax, rdx, rbx, &miss);
 
-  // Check that the receiver is a valid JS object.
-  __ CmpObjectType(rax, FIRST_JS_OBJECT_TYPE, rbx);
-  __ j(below, &miss);
-
-  // If this assert fails, we have to check upper bound too.
-  ASSERT(LAST_TYPE == JS_FUNCTION_TYPE);
-
-  // Check for access to global object (unlikely).
-  __ CmpInstanceType(rbx, JS_GLOBAL_PROXY_TYPE);
-  __ j(equal, &global);
-
-  // Check for non-global object that requires access check.
-  __ testl(FieldOperand(rbx, Map::kBitFieldOffset),
-           Immediate(1 << Map::kIsAccessCheckNeeded));
-  __ j(not_zero, &miss);
-
+  //  rdx: elements
   // Search the dictionary placing the result in rax.
-  __ bind(&probe);
-  GenerateDictionaryLoad(masm, &miss, rdx, rax, rbx,
-                         rcx, rdi, rax, CHECK_DICTIONARY);
+  GenerateDictionaryLoad(masm, &miss, rdx, rcx, rbx, rdi, rax);
   __ ret(0);
 
-  // Global object access: Check access rights.
-  __ bind(&global);
-  __ CheckAccessGlobalProxy(rax, rdx, &miss);
-  __ jmp(&probe);
-
   // Cache miss: Jump to runtime.
   __ bind(&miss);
   GenerateMiss(masm);
@@ -1751,14 +1668,32 @@
   // immediate move instruction, so we add 2 to get the
   // offset to the last 8 bytes.
   Address map_address = test_instruction_address + delta + 2;
+#ifdef NACL
+  NaClCode::PatchWord(reinterpret_cast<Object**>(map_address), map);
+#else
   *(reinterpret_cast<Object**>(map_address)) = map;
+#endif
 
   // The offset is in the 32-bit displacement of a seven byte
   // memory-to-register move instruction (REX.W 0x88 ModR/M disp32),
   // so we add 3 to get the offset of the displacement.
+
+  int sandbox_delta = 0;
+  // check if we have optional REX
+  if (*(test_instruction_address + delta + kOffsetToLoadInstruction) == 0x8b)
+    sandbox_delta = 2;
+  else if (*(test_instruction_address + delta + kOffsetToLoadInstruction + 1) == 0x8b)
+    sandbox_delta = 3;
+  else
+    ASSERT(0);
+  
   Address offset_address =
-      test_instruction_address + delta + kOffsetToLoadInstruction + 3;
+      test_instruction_address + delta + kOffsetToLoadInstruction + sandbox_delta + 3+1;
+#ifdef NACL
+  NaClCode::PatchWord(reinterpret_cast<int*>(offset_address), offset - kHeapObjectTag);
+#else
   *reinterpret_cast<int*>(offset_address) = offset - kHeapObjectTag;
+#endif
   return true;
 }
 
Index: src/x64/fast-codegen-x64.cc
===================================================================
--- src/x64/fast-codegen-x64.cc	(revision 4925)
+++ src/x64/fast-codegen-x64.cc	(working copy)
@@ -49,7 +49,7 @@
 void FastCodeGenerator::EmitLoadReceiver() {
   // Offset 2 is due to return address and saved frame pointer.
   int index = 2 + scope()->num_parameters();
-  __ movq(receiver_reg(), Operand(rbp, index * kPointerSize));
+  __ movq(receiver_reg(), Operand(rbp, index * kStackPointerSize));
 }
 
 
@@ -238,7 +238,7 @@
   __ LoadRoot(rax, Heap::kUndefinedValueRootIndex);
   __ movq(rsp, rbp);
   __ pop(rbp);
-  __ ret((scope()->num_parameters() + 1) * kPointerSize);
+  __ ret((scope()->num_parameters() + 1) * kStackPointerSize);
 }
 
 
Index: src/x64/assembler-x64-inl.h
===================================================================
--- src/x64/assembler-x64-inl.h	(revision 4925)
+++ src/x64/assembler-x64-inl.h	(working copy)
@@ -182,11 +182,43 @@
 }
 
 
+#ifdef NACL
+bool Assembler::is_rsp_rbp(Register dst) {
+  return dst.is(rsp) || dst.is(rbp);
+}
+
+
+// NACL_CHANGE(pmarch) fix pointers in reloc info
+void Assembler::set_target_address_at(Address pc, Address target, uint32_t extraoffset) {
+  int32_t val = reinterpret_cast<uint32_t>(target) - (reinterpret_cast<uint32_t>(pc) + sizeof(int32_t) + extraoffset);
+  int32_t* p = reinterpret_cast<int32_t*>(pc);
+
+  ASSERT((reinterpret_cast<uintptr_t>(target) & (NACL_CHUNK-1)) == 0);
+
+  if(NaClCode::IsProtectedCode(pc)) {
+    NaClCode::PatchWord(p, val);
+  }else{
+    *p = val;
+  }
+  CPU::FlushICache(p, sizeof(int32_t));
+}
+
+
 void Assembler::set_target_address_at(Address pc, Address target) {
+  set_target_address_at(pc, target, 0);
+}
+
+
+#else  // NACL
+
+
+void Assembler::set_target_address_at(Address pc, Address target) {
   Memory::int32_at(pc) = static_cast<int32_t>(target - pc - 4);
   CPU::FlushICache(pc, sizeof(int32_t));
 }
+#endif
 
+
 Handle<Object> Assembler::code_target_object_handle_at(Address pc) {
   return code_targets_[Memory::int32_at(pc)];
 }
@@ -238,6 +270,17 @@
 }
 
 
+// NACL_CHANGE(pmarch) fix pointers in reloc info
+#ifdef NACL
+void RelocInfo::set_target_address(Address target, uint32_t extraoffset) {
+  ASSERT(IsCodeTarget(rmode_) || rmode_ == RUNTIME_ENTRY);
+  if (IsCodeTarget(rmode_)) {
+    Assembler::set_target_address_at(pc_, target, extraoffset);
+  } else {
+    Memory::Address_at(pc_) = target;
+  }
+}
+#else
 void RelocInfo::set_target_address(Address target) {
   ASSERT(IsCodeTarget(rmode_) || rmode_ == RUNTIME_ENTRY);
   if (IsCodeTarget(rmode_)) {
@@ -246,6 +289,7 @@
     Memory::Address_at(pc_) = target;
   }
 }
+#endif
 
 
 Object* RelocInfo::target_object() {
@@ -395,6 +439,23 @@
 }
 
 
+#ifdef NACL
+inline int Operand::get_size() const {
+  ASSERT((len_ >= 1) && (len_ <= 6));
+  return len_;
+}
+
+
+void Assembler::emitl(uint32_t x, RelocInfo::Mode rmode) {
+  Memory::uint32_at(pc_) = x;
+  if (rmode != RelocInfo::NONE) {
+    RecordRelocInfo(rmode, x);
+  }
+  pc_ += sizeof(uint32_t);
+}
+#endif
+
+
 } }  // namespace v8::internal
 
 #endif  // V8_X64_ASSEMBLER_X64_INL_H_
Index: src/x64/jump-target-x64.cc
===================================================================
--- src/x64/jump-target-x64.cc	(revision 4925)
+++ src/x64/jump-target-x64.cc	(working copy)
@@ -206,7 +206,7 @@
       int difference = frame->stack_pointer_ - (frame->element_count() - 1);
       if (difference > 0) {
         frame->stack_pointer_ -= difference;
-        __ addq(rsp, Immediate(difference * kPointerSize));
+        __ addq(rsp, Immediate(difference * kStackPointerSize));
       }
     } else {
       ASSERT(direction_ == BIDIRECTIONAL);
@@ -237,7 +237,7 @@
     int difference = frame->stack_pointer_ - (frame->element_count() - 1);
     if (difference > 0) {
       frame->stack_pointer_ -= difference;
-      __ addq(rsp, Immediate(difference * kPointerSize));
+      __ addq(rsp, Immediate(difference * kStackPointerSize));
     }
 
     __ bind(&entry_label_);
Index: src/x64/full-codegen-x64.cc
===================================================================
--- src/x64/full-codegen-x64.cc	(revision 4925)
+++ src/x64/full-codegen-x64.cc	(working copy)
@@ -103,7 +103,7 @@
         Slot* slot = scope()->parameter(i)->slot();
         if (slot != NULL && slot->type() == Slot::CONTEXT) {
           int parameter_offset = StandardFrameConstants::kCallerSPOffset +
-                                     (num_parameters - 1 - i) * kPointerSize;
+                                     (num_parameters - 1 - i) * kStackPointerSize;
           // Load parameter from stack.
           __ movq(rax, Operand(rbp, parameter_offset));
           // Store it in the context.
@@ -130,7 +130,7 @@
         __ push(Operand(rbp, JavaScriptFrameConstants::kFunctionOffset));
       }
       // The receiver is just before the parameters on the caller's stack.
-      int offset = scope()->num_parameters() * kPointerSize;
+      int offset = scope()->num_parameters() * kStackPointerSize;
       __ lea(rdx,
              Operand(rbp, StandardFrameConstants::kCallerSPOffset + offset));
       __ push(rdx);
@@ -213,7 +213,7 @@
     // patch with the code required by the debugger.
     __ movq(rsp, rbp);
     __ pop(rbp);
-    __ ret((scope()->num_parameters() + 1) * kPointerSize);
+    __ ret((scope()->num_parameters() + 1) * kStackPointerSize);
 #ifdef ENABLE_DEBUGGER_SUPPORT
     // Add padding that will be overwritten by a debugger breakpoint.  We
     // have just generated "movq rsp, rbp; pop rbp; ret k" with length 7
@@ -1024,12 +1024,12 @@
 
   // Generate code for doing the condition check.
   __ bind(&loop);
-  __ movq(rax, Operand(rsp, 0 * kPointerSize));  // Get the current index.
-  __ cmpq(rax, Operand(rsp, 1 * kPointerSize));  // Compare to the array length.
+  __ movq(rax, Operand(rsp, 0 * kStackPointerSize));  // Get the current index.
+  __ cmpq(rax, Operand(rsp, 1 * kStackPointerSize));  // Compare to the array length.
   __ j(above_equal, loop_statement.break_target());
 
   // Get the current entry of the array into register rbx.
-  __ movq(rbx, Operand(rsp, 2 * kPointerSize));
+  __ movq(rbx, Operand(rsp, 2 * kStackPointerSize));
   SmiIndex index = __ SmiToIndex(rax, rax, kPointerSizeLog2);
   __ movq(rbx, FieldOperand(rbx,
                             index.reg,
@@ -1038,12 +1038,12 @@
 
   // Get the expected map from the stack or a zero map in the
   // permanent slow case into register rdx.
-  __ movq(rdx, Operand(rsp, 3 * kPointerSize));
+  __ movq(rdx, Operand(rsp, 3 * kStackPointerSize));
 
   // Check if the expected map still matches that of the enumerable.
   // If not, we have to filter the key.
   Label update_each;
-  __ movq(rcx, Operand(rsp, 4 * kPointerSize));
+  __ movq(rcx, Operand(rsp, 4 * kStackPointerSize));
   __ cmpq(rdx, FieldOperand(rcx, HeapObject::kMapOffset));
   __ j(equal, &update_each);
 
@@ -1074,7 +1074,7 @@
   // Generate code for going to the next element by incrementing the
   // index (smi) stored on top of the stack.
   __ bind(loop_statement.continue_target());
-  __ SmiAddConstant(Operand(rsp, 0 * kPointerSize), Smi::FromInt(1));
+  __ SmiAddConstant(Operand(rsp, 0 * kStackPointerSize), Smi::FromInt(1));
   __ jmp(&loop);
 
   // Slow case for the stack limit check.
@@ -1085,7 +1085,7 @@
 
   // Remove the pointers stored on the stack.
   __ bind(loop_statement.break_target());
-  __ addq(rsp, Immediate(5 * kPointerSize));
+  __ addq(rsp, Immediate(5 * kStackPointerSize));
 
   // Exit and decrement the loop depth.
   __ bind(&exit);
@@ -1518,12 +1518,13 @@
     case KEYED_PROPERTY: {
       __ push(rax);  // Preserve value.
       VisitForValue(prop->obj(), kStack);
-      VisitForValue(prop->key(), kStack);
-      __ movq(rax, Operand(rsp, 2 * kPointerSize));
+      VisitForValue(prop->key(), kAccumulator);
+      __ movq(rcx, rax);
+      __ pop(rdx);
+      __ pop(rax);
       Handle<Code> ic(Builtins::builtin(Builtins::KeyedStoreIC_Initialize));
       __ call(ic, RelocInfo::CODE_TARGET);
       __ nop();  // Signal no inlined code.
-      __ Drop(3);  // Receiver, key, and extra copy of value.
       break;
     }
   }
@@ -1617,7 +1618,7 @@
   // adding fast properties.
   if (expr->starts_initialization_block()) {
     __ push(result_register());
-    __ push(Operand(rsp, kPointerSize));  // Receiver is now under value.
+    __ push(Operand(rsp, kStackPointerSize));  // Receiver is now under value.
     __ CallRuntime(Runtime::kToSlowProperties, 1);
     __ pop(result_register());
   }
@@ -1637,7 +1638,7 @@
   // If the assignment ends an initialization block, revert to fast case.
   if (expr->ends_initialization_block()) {
     __ push(rax);  // Result of assignment, saved even if not needed.
-    __ push(Operand(rsp, kPointerSize));  // Receiver is under value.
+    __ push(Operand(rsp, kStackPointerSize));  // Receiver is under value.
     __ CallRuntime(Runtime::kToFastProperties, 1);
     __ pop(rax);
     DropAndApply(1, context_, rax);
@@ -1656,7 +1657,7 @@
   if (expr->starts_initialization_block()) {
     __ push(result_register());
     // Receiver is now under the key and value.
-    __ push(Operand(rsp, 2 * kPointerSize));
+    __ push(Operand(rsp, 2 * kStackPointerSize));
     __ CallRuntime(Runtime::kToSlowProperties, 1);
     __ pop(result_register());
   }
@@ -1793,23 +1794,23 @@
     }
 
     // Push copy of the function - found below the arguments.
-    __ push(Operand(rsp, (arg_count + 1) * kPointerSize));
+    __ push(Operand(rsp, (arg_count + 1) * kStackPointerSize));
 
     // Push copy of the first argument or undefined if it doesn't exist.
     if (arg_count > 0) {
-      __ push(Operand(rsp, arg_count * kPointerSize));
+      __ push(Operand(rsp, arg_count * kStackPointerSize));
     } else {
       __ PushRoot(Heap::kUndefinedValueRootIndex);
     }
 
     // Push the receiver of the enclosing function and do runtime call.
-    __ push(Operand(rbp, (2 + scope()->num_parameters()) * kPointerSize));
+    __ push(Operand(rbp, (2 + scope()->num_parameters()) * kStackPointerSize));
     __ CallRuntime(Runtime::kResolvePossiblyDirectEval, 3);
 
     // The runtime call returns a pair of values in rax (function) and
     // rdx (receiver). Touch up the stack with the right values.
-    __ movq(Operand(rsp, (arg_count + 0) * kPointerSize), rdx);
-    __ movq(Operand(rsp, (arg_count + 1) * kPointerSize), rax);
+    __ movq(Operand(rsp, (arg_count + 0) * kStackPointerSize), rdx);
+    __ movq(Operand(rsp, (arg_count + 1) * kStackPointerSize), rax);
 
     // Record source position for debugger.
     SetSourcePosition(expr->position());
@@ -1916,7 +1917,7 @@
   // Load function, arg_count into rdi and rax.
   __ Set(rax, arg_count);
   // Function is in rsp[arg_count + 1].
-  __ movq(rdi, Operand(rsp, rax, times_pointer_size, kPointerSize));
+  __ movq(rdi, Operand(rsp, rax, times_stack_pointer_size, kStackPointerSize));
 
   Handle<Code> construct_builtin(Builtins::builtin(Builtins::JSConstructCall));
   __ Call(construct_builtin, RelocInfo::CONSTRUCT_CALL);
@@ -2925,10 +2926,10 @@
             __ push(rax);
             break;
           case NAMED_PROPERTY:
-            __ movq(Operand(rsp, kPointerSize), rax);
+            __ movq(Operand(rsp, kStackPointerSize), rax);
             break;
           case KEYED_PROPERTY:
-            __ movq(Operand(rsp, 2 * kPointerSize), rax);
+            __ movq(Operand(rsp, 2 * kStackPointerSize), rax);
             break;
         }
         break;
@@ -3197,7 +3198,7 @@
 
 
 void FullCodeGenerator::StoreToFrameField(int frame_offset, Register value) {
-  ASSERT(IsAligned(frame_offset, kPointerSize));
+  ASSERT(IsAligned(frame_offset, kStackPointerSize));
   __ movq(Operand(rbp, frame_offset), value);
 }
 
Index: src/x64/register-allocator-x64.h
===================================================================
--- src/x64/register-allocator-x64.h	(revision 4925)
+++ src/x64/register-allocator-x64.h	(working copy)
@@ -33,7 +33,8 @@
 
 class RegisterAllocatorConstants : public AllStatic {
  public:
-  static const int kNumRegisters = 11;
+  // PMARCH revisit
+  static const int kNumRegisters = 9;
   static const int kInvalidRegister = -1;
 };
 
Index: src/x64/regexp-macro-assembler-x64.cc
===================================================================
--- src/x64/regexp-macro-assembler-x64.cc	(revision 4925)
+++ src/x64/regexp-macro-assembler-x64.cc	(working copy)
@@ -162,11 +162,40 @@
 }
 
 
+#ifdef NACL
+inline void RegExpMacroAssemblerX64::AddInstructionStartToRegister(Register reg) {
+  //NACL_CHANGE: added this function
+
+  //find a tmp that is different than reg
+  Register tmp = (reg.is(rax)) ? rbx : rax;
+
+  //spill current value of tmp
+  //TODO(janse): see if I really need spill or if I can trash tmp
+  //__ push(tmp);
+
+  // load code object and deref external code point
+  __ movl(tmp, code_object_pointer());
+  __ addl(tmp, Immediate(Code::kHeaderSize - kHeapObjectTag));
+  NACL_PATCH_INSTRUCTION_START(tmp);
+
+  // add the computed value
+  __ addq(reg, tmp);
+
+  //restore value of tmp
+  //__ pop(tmp);
+}
+#endif
+
+
 void RegExpMacroAssemblerX64::Backtrack() {
   CheckPreemption();
   // Pop Code* offset from backtrack stack, add Code* and jump to location.
   Pop(rbx);
+#ifdef NACL
+  AddInstructionStartToRegister(rbx);
+#else
   __ addq(rbx, code_object_pointer());
+#endif
   __ jmp(rbx);
 }
 
@@ -175,6 +204,11 @@
   __ bind(label);
 }
 
+#ifdef NACL
+void RegExpMacroAssemblerX64::Bind_Aligned(Label* label) {
+  __ bind_aligned(label);
+}
+#endif
 
 void RegExpMacroAssemblerX64::CheckCharacter(uint32_t c, Label* on_equal) {
   __ cmpl(current_character(), Immediate(c));
@@ -262,6 +296,9 @@
   BranchOrBacktrack(not_equal, on_failure);
 
   __ lea(rbx, Operand(rsi, rdi, times_1, 0));
+#ifdef NACL
+  ASSERT(0);  // TODO(pmarch) make sure this code behaves correctly with 32-bit operations
+#endif
   for (int i = 1, n = str.length(); i < n; ) {
     if (mode_ == ASCII) {
       if (i + 8 <= n) {
@@ -354,7 +391,11 @@
     }
 
     __ lea(r9, Operand(rsi, rdx, times_1, 0));
+#ifdef NACL
+    __ lea(r12, Operand(rsi, rdi, times_1, 0));
+#else
     __ lea(r11, Operand(rsi, rdi, times_1, 0));
+#endif
     __ addq(rbx, r9);  // End of capture
     // ---------------------
     // r11 - current input character address
@@ -364,7 +405,11 @@
     Label loop;
     __ bind(&loop);
     __ movzxbl(rdx, Operand(r9, 0));
+#ifdef NACL
+    __ movzxbl(rax, Operand(r12, 0));
+#else
     __ movzxbl(rax, Operand(r11, 0));
+#endif
     // al - input character
     // dl - capture character
     __ cmpb(rax, rdx);
@@ -383,14 +428,22 @@
 
     __ bind(&loop_increment);
     // Increment pointers into match and capture strings.
+#ifdef NACL
+    __ addq(r12, Immediate(1));
+#else
     __ addq(r11, Immediate(1));
+#endif
     __ addq(r9, Immediate(1));
     // Compare to end of capture, and loop if not done.
     __ cmpq(r9, rbx);
     __ j(below, &loop);
 
     // Compute new value of character position after the matched part.
+#ifdef NACL
+    __ movq(rdi, r12);
+#else
     __ movq(rdi, r11);
+#endif
     __ subq(rdi, rsi);
   } else {
     ASSERT(mode_ == UC16);
@@ -722,12 +775,12 @@
 #else
   // GCC passes arguments in rdi, rsi, rdx, rcx, r8, r9 (and then on stack).
   // Push register parameters on stack for reference.
-  ASSERT_EQ(kInputString, -1 * kPointerSize);
-  ASSERT_EQ(kStartIndex, -2 * kPointerSize);
-  ASSERT_EQ(kInputStart, -3 * kPointerSize);
-  ASSERT_EQ(kInputEnd, -4 * kPointerSize);
-  ASSERT_EQ(kRegisterOutput, -5 * kPointerSize);
-  ASSERT_EQ(kStackHighEnd, -6 * kPointerSize);
+  ASSERT_EQ(kInputString, -1 * kStackPointerSize);
+  ASSERT_EQ(kStartIndex, -2 * kStackPointerSize);
+  ASSERT_EQ(kInputStart, -3 * kStackPointerSize);
+  ASSERT_EQ(kInputEnd, -4 * kStackPointerSize);
+  ASSERT_EQ(kRegisterOutput, -5 * kStackPointerSize);
+  ASSERT_EQ(kStackHighEnd, -6 * kStackPointerSize);
   __ push(rdi);
   __ push(rsi);
   __ push(rdx);
@@ -753,7 +806,7 @@
   __ j(below_equal, &stack_limit_hit);
   // Check if there is room for the variable number of registers above
   // the stack limit.
-  __ cmpq(rcx, Immediate(num_registers_ * kPointerSize));
+  __ cmpq(rcx, Immediate(num_registers_ * kStackPointerSize));
   __ j(above_equal, &stack_ok);
   // Exit with OutOfMemory exception. There is not enough space on the stack
   // for our working registers.
@@ -770,7 +823,7 @@
   __ bind(&stack_ok);
 
   // Allocate space on stack for registers.
-  __ subq(rsp, Immediate(num_registers_ * kPointerSize));
+  __ subq(rsp, Immediate(num_registers_ * kStackPointerSize));
   // Load string length.
   __ movq(rsi, Operand(rbp, kInputEnd));
   // Load input position.
@@ -797,16 +850,22 @@
     __ movq(rcx, Immediate(kRegisterZero));
     Label init_loop;
     __ bind(&init_loop);
+#ifdef NACL
+    // sandboxing issue: negative index
+    __ lea(rdx, Operand(rbp, rcx, times_1, 0));
+    __ movq(Operand(rdx, 0), rax);
+#else
     __ movq(Operand(rbp, rcx, times_1, 0), rax);
-    __ subq(rcx, Immediate(kPointerSize));
+#endif
+    __ subq(rcx, Immediate(kStackPointerSize));
     __ cmpq(rcx,
-            Immediate(kRegisterZero - num_saved_registers_ * kPointerSize));
+            Immediate(kRegisterZero - num_saved_registers_ * kStackPointerSize));
     __ j(greater, &init_loop);
   }
   // Ensure that we have written to each stack page, in order. Skipping a page
   // on Windows can cause segmentation faults. Assuming page size is 4k.
   const int kPageSize = 4096;
-  const int kRegistersPerPage = kPageSize / kPointerSize;
+  const int kRegistersPerPage = kPageSize / kStackPointerSize;
   for (int i = num_saved_registers_ + kRegistersPerPage - 1;
       i < num_registers_;
       i += kRegistersPerPage) {
@@ -1108,7 +1167,7 @@
   __ movq(r8, rbp);
   // First argument: Next address on the stack (will be address of
   // return address).
-  __ lea(rcx, Operand(rsp, -kPointerSize));
+  __ lea(rcx, Operand(rsp, -kStackPointerSize));
 #else
   // Third argument: RegExp code frame pointer.
   __ movq(rdx, rbp);
@@ -1116,7 +1175,7 @@
   __ movq(rsi, code_object_pointer());
   // First argument: Next address on the stack (will be address of
   // return address).
-  __ lea(rdi, Operand(rsp, -kPointerSize));
+  __ lea(rdi, Operand(rsp, -kStackPointerSize));
 #endif
   ExternalReference stack_check =
       ExternalReference::re_check_stack_guard_state();
@@ -1214,7 +1273,7 @@
   if (num_registers_ <= register_index) {
     num_registers_ = register_index + 1;
   }
-  return Operand(rbp, kRegisterZero - register_index * kPointerSize);
+  return Operand(rbp, kRegisterZero - register_index * kStackPointerSize);
 }
 
 
@@ -1283,6 +1342,7 @@
     // instead.
     int patch_position = position - kIntSize;
     int offset = masm_->long_at(patch_position);
+
     masm_->long_at_put(patch_position,
                        offset
                        + position
@@ -1335,7 +1395,6 @@
   __ load_rax(stack_limit);
   __ cmpq(backtrack_stackpointer(), rax);
   __ j(above, &no_stack_overflow);
-
   SafeCall(&stack_overflow_label_);
 
   __ bind(&no_stack_overflow);
Index: src/x64/codegen-x64.cc
===================================================================
--- src/x64/codegen-x64.cc	(revision 4925)
+++ src/x64/codegen-x64.cc	(working copy)
@@ -563,7 +563,7 @@
   // Leave the frame and return popping the arguments and the
   // receiver.
   frame_->Exit();
-  masm_->ret((scope()->num_parameters() + 1) * kPointerSize);
+  masm_->ret((scope()->num_parameters() + 1) * kStackPointerSize);
 #ifdef ENABLE_DEBUGGER_SUPPORT
   // Add padding that will be overwritten by a debugger breakpoint.
   // frame_->Exit() generates "movq rsp, rbp; pop rbp; ret k"
@@ -590,9 +590,15 @@
       && (allocator()->count(rdi) == (frame()->is_used(rdi) ? 1 : 0))
       && (allocator()->count(r8) == (frame()->is_used(r8) ? 1 : 0))
       && (allocator()->count(r9) == (frame()->is_used(r9) ? 1 : 0))
+#ifndef NACL
+      // r11 is reserved for sandboxing memory access operations
       && (allocator()->count(r11) == (frame()->is_used(r11) ? 1 : 0))
+#endif
       && (allocator()->count(r14) == (frame()->is_used(r14) ? 1 : 0))
+#ifndef NACL
+      // r15 is reserved register in NaCl
       && (allocator()->count(r15) == (frame()->is_used(r15) ? 1 : 0))
+#endif
       && (allocator()->count(r12) == (frame()->is_used(r12) ? 1 : 0));
 }
 #endif
@@ -830,7 +836,7 @@
       __ j(below, &build_args);
 
       // Check that applicand.apply is Function.prototype.apply.
-      __ movq(rax, Operand(rsp, kPointerSize));
+      __ movq(rax, Operand(rsp, kStackPointerSize));
       is_smi = masm_->CheckSmi(rax);
       __ j(is_smi, &build_args);
       __ CmpObjectType(rax, JS_FUNCTION_TYPE, rcx);
@@ -841,7 +847,7 @@
       __ j(not_equal, &build_args);
 
       // Check that applicand is a function.
-      __ movq(rdi, Operand(rsp, 2 * kPointerSize));
+      __ movq(rdi, Operand(rsp, 2 * kStackPointerSize));
       is_smi = masm_->CheckSmi(rdi);
       __ j(is_smi, &build_args);
       __ CmpObjectType(rdi, JS_FUNCTION_TYPE, rcx);
@@ -882,7 +888,7 @@
       __ testl(rcx, rcx);
       __ j(zero, &invoke);
       __ bind(&loop);
-      __ push(Operand(rdx, rcx, times_pointer_size, 1 * kPointerSize));
+      __ push(Operand(rdx, rcx, times_stack_pointer_size, 1 * kStackPointerSize));
       __ decl(rcx);
       __ j(not_zero, &loop);
 
@@ -894,7 +900,7 @@
       // the result of the function call, but leave the spilled frame
       // unchanged, with 3 elements, so it is correct when we compile the
       // slow-case code.
-      __ addq(rsp, Immediate(2 * kPointerSize));
+      __ addq(rsp, Immediate(2 * kStackPointerSize));
       __ push(rax);
       // Stack now has 1 element:
       //   rsp[0]: result
@@ -923,10 +929,10 @@
     // Flip applicand.apply and applicand on the stack, so
     // applicand looks like the receiver of the applicand.apply call.
     // Then process it as a normal function call.
-    __ movq(rax, Operand(rsp, 3 * kPointerSize));
-    __ movq(rbx, Operand(rsp, 2 * kPointerSize));
-    __ movq(Operand(rsp, 2 * kPointerSize), rax);
-    __ movq(Operand(rsp, 3 * kPointerSize), rbx);
+    __ movq(rax, Operand(rsp, 3 * kStackPointerSize));
+    __ movq(rbx, Operand(rsp, 2 * kStackPointerSize));
+    __ movq(Operand(rsp, 2 * kStackPointerSize), rax);
+    __ movq(Operand(rsp, 3 * kStackPointerSize), rbx);
 
     CallFunctionStub call_function(2, NOT_IN_LOOP, NO_CALL_FUNCTION_FLAGS);
     Result res = frame_->CallStub(&call_function, 3);
@@ -2097,7 +2103,7 @@
     ASSERT(StackHandlerConstants::kNextOffset == 0);
     __ movq(kScratchRegister, handler_address);
     frame_->EmitPop(Operand(kScratchRegister, 0));
-    frame_->Drop(StackHandlerConstants::kSize / kPointerSize - 1);
+    frame_->Drop(StackHandlerConstants::kSize / kStackPointerSize - 1);
     if (has_unlinks) {
       exit.Jump();
     }
@@ -2130,7 +2136,7 @@
       ASSERT(StackHandlerConstants::kNextOffset == 0);
       __ movq(kScratchRegister, handler_address);
       frame_->EmitPop(Operand(kScratchRegister, 0));
-      frame_->Drop(StackHandlerConstants::kSize / kPointerSize - 1);
+      frame_->Drop(StackHandlerConstants::kSize / kStackPointerSize - 1);
 
       if (i == kReturnShadowIndex) {
         if (!function_return_is_shadowed_) frame_->PrepareForReturn();
@@ -2217,7 +2223,7 @@
     ASSERT(StackHandlerConstants::kNextOffset == 0);
     __ movq(kScratchRegister, handler_address);
     frame_->EmitPop(Operand(kScratchRegister, 0));
-    frame_->Drop(StackHandlerConstants::kSize / kPointerSize - 1);
+    frame_->Drop(StackHandlerConstants::kSize / kStackPointerSize - 1);
 
     // Fake a top of stack value (unneeded when FALLING) and set the
     // state in ecx, then jump around the unlink blocks if any.
@@ -2258,7 +2264,7 @@
       ASSERT(StackHandlerConstants::kNextOffset == 0);
       __ movq(kScratchRegister, handler_address);
       frame_->EmitPop(Operand(kScratchRegister, 0));
-      frame_->Drop(StackHandlerConstants::kSize / kPointerSize - 1);
+      frame_->Drop(StackHandlerConstants::kSize / kStackPointerSize - 1);
 
       if (i == kReturnShadowIndex) {
         // If this target shadowed the function return, materialize
@@ -2641,7 +2647,7 @@
 
   // Generate code to set the elements in the array that are not
   // literals.
-  for (int i = 0; i < node->values()->length(); i++) {
+  for (int i = 0; i < length; i++) {
     Expression* value = node->values()->at(i);
 
     // If value is a literal the property value is already set in the
@@ -3294,6 +3300,11 @@
         JumpTarget smi_label;
         JumpTarget continue_label;
         Result operand = frame_->Pop();
+#ifdef NACL
+        // In case of NaCl, if the operand is not a Smi the result will be
+        // also a heap number
+        TypeInfo operand_info = operand.type_info();
+#endif
         operand.ToRegister();
 
         Condition is_smi = masm_->CheckSmi(operand.reg());
@@ -3308,7 +3319,18 @@
         frame_->Spill(answer.reg());
         __ SmiNot(answer.reg(), answer.reg());
         continue_label.Bind(&answer);
+
+#ifdef NACL
+        // In case of NaCl, if the operand is not a Smi the result will be
+        // also a heap number
+        if (operand_info.IsSmi()) {
+          answer.set_type_info(TypeInfo::Smi());
+        } else {
+          answer.set_type_info(TypeInfo::Number());
+        }
+#else
         answer.set_type_info(TypeInfo::Smi());
+#endif
         frame_->Push(&answer);
         break;
       }
@@ -3855,8 +3877,17 @@
     default:
       UNREACHABLE();
   }
-  Load(left);
-  Load(right);
+
+  if (left->IsTrivial()) {
+    Load(right);
+    Result right_result = frame_->Pop();
+    frame_->Push(left);
+    frame_->Push(&right_result);
+  } else {
+    Load(left);
+    Load(right);
+  }
+
   Comparison(node, cc, strict, destination());
 }
 
@@ -4603,7 +4634,7 @@
 
     Label slowcase;
     Label done;
-    __ movq(r8, Operand(rsp, kPointerSize * 2));
+    __ movq(r8, Operand(rsp, kStackPointerSize * 2));
     __ JumpIfNotSmi(r8, &slowcase);
     __ SmiToInteger32(rbx, r8);
     __ cmpl(rbx, Immediate(kMaxInlineLength));
@@ -4615,7 +4646,7 @@
     // JSArray:   [Map][empty properties][Elements][Length-smi][index][input]
     // Elements:  [Map][Length][..elements..]
     __ AllocateInNewSpace(JSRegExpResult::kSize + FixedArray::kHeaderSize,
-                          times_pointer_size,
+                          times_heap_pointer_size,
                           rbx,  // In: Number of elements.
                           rax,  // Out: Start of allocation (tagged).
                           rcx,  // Out: End of allocation.
@@ -4643,7 +4674,7 @@
     // Set input, index and length fields from arguments.
     __ pop(FieldOperand(rax, JSRegExpResult::kInputOffset));
     __ pop(FieldOperand(rax, JSRegExpResult::kIndexOffset));
-    __ lea(rsp, Operand(rsp, kPointerSize));
+    __ lea(rsp, Operand(rsp, kStackPointerSize));
     __ movq(FieldOperand(rax, JSArray::kLengthOffset), r8);
 
     // Fill out the elements FixedArray.
@@ -4666,11 +4697,12 @@
     // rcx: Start of elements in FixedArray.
     // rdx: the hole.
     Label loop;
+// PMARCH this was a bug should report this
+    __ bind(&loop);
     __ testl(rbx, rbx);
-    __ bind(&loop);
     __ j(less_equal, &done);  // Jump if ecx is negative or zero.
     __ subl(rbx, Immediate(1));
-    __ movq(Operand(rcx, rbx, times_pointer_size, 0), rdx);
+    __ movq(Operand(rcx, rbx, times_heap_pointer_size, 0), rdx);
     __ jmp(&loop);
 
     __ bind(&slowcase);
@@ -4710,7 +4742,7 @@
                             Register index,
                             int additional_offset = 0) {
   int offset = FixedArray::kHeaderSize + additional_offset * kPointerSize;
-  return FieldOperand(array, index, times_pointer_size, offset);
+  return FieldOperand(array, index, times_heap_pointer_size, offset);
 }
 
 
@@ -4768,7 +4800,7 @@
 
   // Find a place to put new cached value into.
   Label add_new_entry, update_cache;
-  __ movq(rcx, Operand(rsp, kPointerSize));  // restore the cache
+  __ movq(rcx, Operand(rsp, kStackPointerSize));  // restore the cache
   // Possible optimization: cache size is constant for the given cache
   // so technically we could use a constant here.  However, if we have
   // cache miss this optimization would hardly matter much.
@@ -4869,11 +4901,11 @@
   // tmp.reg() now holds finger offset as a smi.
   __ SmiToInteger32(tmp.reg(), FieldOperand(cache.reg(), kFingerOffset));
   __ cmpq(key.reg(), FieldOperand(cache.reg(),
-                                  tmp.reg(), times_pointer_size,
+                                  tmp.reg(), times_heap_pointer_size,
                                   FixedArray::kHeaderSize));
   deferred->Branch(not_equal);
   __ movq(tmp.reg(), FieldOperand(cache.reg(),
-                                  tmp.reg(), times_pointer_size,
+                                  tmp.reg(), times_heap_pointer_size,
                                   FixedArray::kHeaderSize + kPointerSize));
 
   deferred->BindExit();
@@ -4969,12 +5001,12 @@
   __ SmiToInteger32(index1.reg(), index1.reg());
   __ lea(index1.reg(), FieldOperand(tmp1.reg(),
                                     index1.reg(),
-                                    times_pointer_size,
+                                    times_heap_pointer_size,
                                     FixedArray::kHeaderSize));
   __ SmiToInteger32(index2.reg(), index2.reg());
   __ lea(index2.reg(), FieldOperand(tmp1.reg(),
                                     index2.reg(),
-                                    times_pointer_size,
+                                    times_heap_pointer_size,
                                     FixedArray::kHeaderSize));
 
   // Swap elements.
@@ -5336,9 +5368,8 @@
     dest->false_target()->Branch(equal);
     Condition is_smi = masm_->CheckSmi(value.reg());
     dest->true_target()->Branch(is_smi);
-    __ fldz();
-    __ fld_d(FieldOperand(value.reg(), HeapNumber::kValueOffset));
-    __ FCmp();
+    __ xorpd(xmm0, xmm0);
+    __ ucomisd(xmm0, FieldOperand(value.reg(), HeapNumber::kValueOffset));
     value.Unuse();
     dest->Split(not_zero);
   } else {
@@ -6584,7 +6615,11 @@
                                   const Result& left) {
   // Set TypeInfo of result according to the operation performed.
   // We rely on the fact that smis have a 32 bit payload on x64.
+#ifdef NACL
+  STATIC_ASSERT(kSmiValueSize == 31);
+#else
   STATIC_ASSERT(kSmiValueSize == 32);
+#endif  
   switch (op) {
     case Token::COMMA:
       return right.type_info();
@@ -6595,18 +6630,63 @@
     case Token::BIT_OR:
     case Token::BIT_XOR:
     case Token::BIT_AND:
+#ifdef NACL
+      if (left.is_smi()) {
+        if (right.is_smi())
+          return TypeInfo::Smi();
+        if (right.is_constant() && right.handle()->IsSmi())
+          return TypeInfo::Smi();
+        return TypeInfo::Number();
+      } else {
+        if (right.is_constant() && right.handle()->IsSmi()) {
+          if (op == Token::BIT_OR 
+              && ((Smi::cast(*right.handle())->value() & 0xc0000000) == 0xc0000000))
+            return TypeInfo::Smi();
+          if (op == Token::BIT_AND
+              && ((Smi::cast(*right.handle())->value() & 0xc0000000) == 0x0))
+            return TypeInfo::Smi();
+        }
+        return TypeInfo::Number(); 
+      }
+#else
       // Result is always a smi.
       return TypeInfo::Smi();
+#endif
     case Token::SAR:
+#ifdef NACL
+      if (left.is_smi())
+        return TypeInfo::Smi();
+      else if (right.is_constant()
+               && right.handle()->IsSmi() 
+               && (Smi::cast(*right.handle())->value() & 0x1F) > 1)
+          return TypeInfo::Smi();
+      return TypeInfo::Number();
+#endif
     case Token::SHL:
+#ifdef NACL
+      return TypeInfo::Number();
+#else
       // Result is always a smi.
       return TypeInfo::Smi();
+#endif
     case Token::SHR:
+#ifdef NACL
+      if (left.is_smi()) {
+        if (right.is_constant() && right.handle()->IsSmi()
+                     && (Smi::cast(*right.handle())->value() & 0x1F) == 0)
+          return TypeInfo::Number();
+        return TypeInfo::Smi();
+      } else if (right.is_constant() && right.handle()->IsSmi()
+                 && (Smi::cast(*right.handle())->value() & 0x1F) >= 2)
+          return TypeInfo::Smi();
+      return TypeInfo::Number();
+#else
       // Result of x >>> y is always a smi if masked y >= 1, otherwise a number.
       return (right.is_constant() && right.handle()->IsSmi()
                      && (Smi::cast(*right.handle())->value() & 0x1F) >= 1)
           ? TypeInfo::Smi()
           : TypeInfo::Number();
+#endif
     case Token::ADD:
       if (operands_type.IsNumber()) {
         return TypeInfo::Number();
@@ -6976,7 +7056,7 @@
                                   deferred);
 
         __ Move(answer.reg(), smi_value);
-        __ SmiShiftLeft(answer.reg(), answer.reg(), operand->reg());
+        __ SmiShiftLeft(answer.reg(), answer.reg(), operand->reg(), deferred->entry_label());
         operand->Unuse();
 
         deferred->BindExit();
@@ -7012,7 +7092,7 @@
                                     deferred);
           __ SmiShiftLeftConstant(answer.reg(),
                                   operand->reg(),
-                                  shift_value);
+                                  shift_value, deferred->entry_label());
           deferred->BindExit();
           operand->Unuse();
         }
@@ -7320,7 +7400,8 @@
       case Token::SHL: {
         __ SmiShiftLeft(answer.reg(),
                         left->reg(),
-                        rcx);
+                        rcx,
+                        deferred->entry_label());
         break;
       }
       default:
@@ -7446,7 +7527,11 @@
     // Check that the receiver is a heap object.
     __ JumpIfSmi(receiver.reg(), deferred->entry_label());
 
+#ifdef NACL
+    __ bind_aligned(deferred->patch_site());
+#else
     __ bind(deferred->patch_site());
+#endif
     // This is the map check instruction that will be patched (so we can't
     // use the double underscore macro that may insert instructions).
     // Initially use an invalid map to force a failure.
@@ -7460,6 +7545,11 @@
 
     // The delta from the patch label to the load offset must be
     // statically known.
+#ifdef NACL
+    int left = LoadIC::kOffsetToLoadInstruction - masm()->SizeOfCodeGeneratedSince(deferred->patch_site());
+    ASSERT(left >= 0);
+    masm()->nops(left);
+#endif
     ASSERT(masm()->SizeOfCodeGeneratedSince(deferred->patch_site()) ==
            LoadIC::kOffsetToLoadInstruction);
     // The initial (invalid) offset has to be large enough to force
@@ -7512,7 +7602,11 @@
     // Check that the receiver has the expected map.
     // Initially, use an invalid map. The map is patched in the IC
     // initialization code.
+#ifdef NACL
+    __ bind_aligned(deferred->patch_site());
+#else
     __ bind(deferred->patch_site());
+#endif
     // Use masm-> here instead of the double underscore macro since extra
     // coverage code can interfere with the patching.  Do not use a load
     // from the root array to load null_value, since the load must be patched
@@ -7530,9 +7624,11 @@
     // is not a dictionary.
     __ movq(elements.reg(),
             FieldOperand(receiver.reg(), JSObject::kElementsOffset));
-    __ Cmp(FieldOperand(elements.reg(), HeapObject::kMapOffset),
-           Factory::fixed_array_map());
-    deferred->Branch(not_equal);
+    if (FLAG_debug_code) {
+      __ Cmp(FieldOperand(elements.reg(), HeapObject::kMapOffset),
+             Factory::fixed_array_map());
+      __ Assert(equal, "JSObject with fast elements map has slow elements");
+    }
 
     // Check that key is within bounds.
     __ SmiCompare(key.reg(),
@@ -7837,7 +7933,7 @@
   __ AllocateInNewSpace(JSFunction::kSize, rax, rbx, rcx, &gc, TAG_OBJECT);
 
   // Get the function info from the stack.
-  __ movq(rdx, Operand(rsp, 1 * kPointerSize));
+  __ movq(rdx, Operand(rsp, 1 * kStackPointerSize));
 
   // Compute the function map in the current global context and set that
   // as the map of the allocated object.
@@ -7858,7 +7954,7 @@
   __ movq(FieldOperand(rax, JSFunction::kLiteralsOffset), rbx);
 
   // Return and remove the on-stack parameter.
-  __ ret(1 * kPointerSize);
+  __ ret(1 * kStackPointerSize);
 
   // Create a new closure through the slower runtime call.
   __ bind(&gc);
@@ -7879,7 +7975,7 @@
                         rax, rbx, rcx, &gc, TAG_OBJECT);
 
   // Get the function from the stack.
-  __ movq(rcx, Operand(rsp, 1 * kPointerSize));
+  __ movq(rcx, Operand(rsp, 1 * kStackPointerSize));
 
   // Setup the object header.
   __ LoadRoot(kScratchRegister, Heap::kContextMapRootIndex);
@@ -7905,7 +8001,7 @@
 
   // Return and remove the on-stack parameter.
   __ movq(rsi, rax);
-  __ ret(1 * kPointerSize);
+  __ ret(1 * kStackPointerSize);
 
   // Need to collect. Call into runtime system.
   __ bind(&gc);
@@ -7927,8 +8023,8 @@
   // Load boilerplate object into rcx and check if we need to create a
   // boilerplate.
   Label slow_case;
-  __ movq(rcx, Operand(rsp, 3 * kPointerSize));
-  __ movq(rax, Operand(rsp, 2 * kPointerSize));
+  __ movq(rcx, Operand(rsp, 3 * kStackPointerSize));
+  __ movq(rax, Operand(rsp, 2 * kStackPointerSize));
   SmiIndex index = masm->SmiToIndex(rax, rax, kPointerSizeLog2);
   __ movq(rcx,
           FieldOperand(rcx, index.reg, index.scale, FixedArray::kHeaderSize));
@@ -7962,7 +8058,7 @@
   }
 
   // Return and remove the on-stack parameters.
-  __ ret(3 * kPointerSize);
+  __ ret(3 * kStackPointerSize);
 
   __ bind(&slow_case);
   __ TailCallRuntime(Runtime::kCreateArrayLiteralShallow, 3, 1);
@@ -7971,7 +8067,7 @@
 
 void ToBooleanStub::Generate(MacroAssembler* masm) {
   Label false_result, true_result, not_string;
-  __ movq(rax, Operand(rsp, 1 * kPointerSize));
+  __ movq(rax, Operand(rsp, 1 * kStackPointerSize));
 
   // 'null' => false.
   __ CompareRoot(rax, Heap::kNullValueRootIndex);
@@ -8000,24 +8096,22 @@
   __ jmp(&true_result);
 
   __ bind(&not_string);
-  // HeapNumber => false iff +0, -0, or NaN.
-  // These three cases set C3 when compared to zero in the FPU.
   __ CompareRoot(rdx, Heap::kHeapNumberMapRootIndex);
   __ j(not_equal, &true_result);
-  __ fldz();  // Load zero onto fp stack
-  // Load heap-number double value onto fp stack
-  __ fld_d(FieldOperand(rax, HeapNumber::kValueOffset));
-  __ FCmp();
+  // HeapNumber => false iff +0, -0, or NaN.
+  // These three cases set the zero flag when compared to zero using ucomisd.
+  __ xorpd(xmm0, xmm0);
+  __ ucomisd(xmm0, FieldOperand(rax, HeapNumber::kValueOffset));
   __ j(zero, &false_result);
   // Fall through to |true_result|.
 
   // Return 1/0 for true/false in rax.
   __ bind(&true_result);
   __ movq(rax, Immediate(1));
-  __ ret(1 * kPointerSize);
+  __ ret(1 * kStackPointerSize);
   __ bind(&false_result);
   __ xor_(rax, rax);
-  __ ret(1 * kPointerSize);
+  __ ret(1 * kStackPointerSize);
 }
 
 
@@ -8115,18 +8209,18 @@
   Label input_not_smi;
   Label loaded;
   // Test that rax is a number.
-  __ movq(rax, Operand(rsp, kPointerSize));
+  __ movq(rax, Operand(rsp, kStackPointerSize));
   __ JumpIfNotSmi(rax, &input_not_smi);
   // Input is a smi. Untag and load it onto the FPU stack.
   // Then load the bits of the double into rbx.
   __ SmiToInteger32(rax, rax);
-  __ subq(rsp, Immediate(kPointerSize));
+  __ subq(rsp, Immediate(kStackPointerSize));
   __ cvtlsi2sd(xmm1, rax);
   __ movsd(Operand(rsp, 0), xmm1);
   __ movq(rbx, xmm1);
   __ movq(rdx, xmm1);
   __ fld_d(Operand(rsp, 0));
-  __ addq(rsp, Immediate(kPointerSize));
+  __ addq(rsp, Immediate(kStackPointerSize));
   __ jmp(&loaded);
 
   __ bind(&input_not_smi);
@@ -8191,15 +8285,18 @@
 #endif
   // Find the address of the rcx'th entry in the cache, i.e., &rax[rcx*16].
   __ addl(rcx, rcx);
-  __ lea(rcx, Operand(rax, rcx, times_8, 0));
+  __ lea(rcx, Operand(rax, rcx, times_heap_pointer_size, 0));
   // Check if cache matches: Double value is stored in uint32_t[2] array.
   Label cache_miss;
   __ cmpq(rbx, Operand(rcx, 0));
   __ j(not_equal, &cache_miss);
+  // PMARCH debug disabled transient chache
+  // we need to reimplement this with respect to 32 bit operations
+  __ jmp(&cache_miss);
   // Cache hit!
   __ movq(rax, Operand(rcx, 2 * kIntSize));
   __ fstp(0);  // Clear FPU stack.
-  __ ret(kPointerSize);
+  __ ret(kStackPointerSize);
 
   __ bind(&cache_miss);
   // Update cache with new value.
@@ -8209,7 +8306,7 @@
   __ movq(Operand(rcx, 0), rbx);
   __ movq(Operand(rcx, 2 * kIntSize), rax);
   __ fstp_d(FieldOperand(rax, HeapNumber::kValueOffset));
-  __ ret(kPointerSize);
+  __ ret(kStackPointerSize);
 
   __ bind(&runtime_call_clear_stack);
   __ fstp(0);
@@ -8221,7 +8318,7 @@
   __ LoadRoot(rax, Heap::kNanValueRootIndex);
   __ movq(Operand(rcx, 0), rbx);
   __ movq(Operand(rcx, 2 * kIntSize), rax);
-  __ ret(kPointerSize);
+  __ ret(kStackPointerSize);
 }
 
 
@@ -8313,6 +8410,8 @@
   __ bind(&done);
 }
 
+#ifdef NACL
+// PMARCH make this function to work properly
 
 // Get the integer part of a heap number.
 // Overwrites the contents of rdi, rbx and rcx. Result cannot be rdi or rbx.
@@ -8324,6 +8423,74 @@
   ASSERT(!result.is(rdi) && !result.is(rbx));
   // TODO(lrn): When type info reaches here, if value is a 32-bit integer, use
   // cvttsd2si (32-bit version) directly.
+  Register exponent = rbx;
+  Register value = rdi;
+  Label done, exponent_63_plus;
+  // Get exponent and extract it
+  __ movl(exponent, FieldOperand(source, HeapNumber::kExponentOffset));
+  __ movl(value, FieldOperand(source, HeapNumber::kMantissaOffset));
+  __ movsd(xmm0, FieldOperand(source, HeapNumber::kValueOffset));  // Save copy in xmm0 in case we need it there.
+  // Clear result preemptively, in case we need to return zero.
+  __ xorl(result, result);
+  // Double to remove sign bit, shift exponent down to least significant bits.
+  // and subtract bias to get the unshifted, unbiased exponent.
+  __ leal(exponent, Operand(exponent, exponent, times_1, 0));
+  __ shr(exponent, Immediate(32 - HeapNumber::kExponentBits));
+  __ subl(exponent, Immediate(HeapNumber::kExponentBias));
+  // Check whether the exponent is too big for a 63 bit unsigned integer.
+  __ cmpl(exponent, Immediate(63));
+  __ j(above_equal, &exponent_63_plus);
+  // Handle exponent range 0..62.
+  __ cvttsd2siq(result, xmm0);
+  __ jmp(&done);
+
+  __ bind(&exponent_63_plus);
+  // Exponent negative or 63+.
+  __ cmpl(exponent, Immediate(83));
+  // If exponent negative or above 83, number contains no significant bits in
+  // the range 0..2^31, so result is zero, and rcx already holds zero.
+  __ j(above, &done);
+
+  // Exponent in rage 63..83.
+  // Mantissa * 2^exponent contains bits in the range 2^0..2^31, namely
+  // the least significant exponent-52 bits.
+
+  // Negate low bits of mantissa if value is negative.
+  __ addl(value, value);  // Move sign bit to carry.
+  __ sbbl(result, result);  // And convert carry to -1 in result register.
+  // if scratch2 is negative, do (scratch2-1)^-1, otherwise (scratch2-0)^0.
+  __ addl(value, result);
+  // Do xor in opposite directions depending on where we want the result
+  // (depending on whether result is rcx or not).
+
+  if (result.is(rcx)) {
+    __ xorl(value, result);
+    // Left shift mantissa by (exponent - mantissabits - 1) to save the
+    // bits that have positional values below 2^32 (the extra -1 comes from the
+    // doubling done above to move the sign bit into the carry flag).
+    __ leal(rcx, Operand(exponent, -HeapNumber::kMantissaBits - 1));
+    __ shll_cl(value);
+    __ movl(result, value);
+  } else {
+    // As the then-branch, but move double-value to result before shifting.
+    __ xorl(result, value);
+    __ leal(rcx, Operand(exponent, -HeapNumber::kMantissaBits - 1));
+    __ shll_cl(result);
+  }
+
+  __ bind(&done);
+}
+#else
+// Get the integer part of a heap number.
+// Overwrites the contents of rdi, rbx and rcx. Result cannot be rdi or rbx.
+void IntegerConvert(MacroAssembler* masm,
+                    Register result,
+                    Register source) {
+  // Result may be rcx. If result and source are the same register, source will
+  // be overwritten.
+  ASSERT(!result.is(rdi) && !result.is(rbx));
+  // TODO(lrn): When type info reaches here, if value is a 32-bit integer, use
+  // cvttsd2si (32-bit version) directly.
   Register double_exponent = rbx;
   Register double_value = rdi;
   Label done, exponent_63_plus;
@@ -8380,8 +8547,8 @@
 
   __ bind(&done);
 }
+#endif // NACL
 
-
 void GenericUnaryOpStub::Generate(MacroAssembler* masm) {
   Label slow, done;
 
@@ -8408,31 +8575,62 @@
     __ CompareRoot(rdx, Heap::kHeapNumberMapRootIndex);
     __ j(not_equal, &slow);
     // Operand is a float, negate its value by flipping sign bit.
+#ifdef NACL
+    __ movq(rdx, FieldOperand(rax, HeapNumber::kExponentOffset));
+#else
     __ movq(rdx, FieldOperand(rax, HeapNumber::kValueOffset));
+#endif
     __ movq(kScratchRegister, Immediate(0x01));
+#ifdef NACL
+    __ shl(kScratchRegister, Immediate(31));
+#else
     __ shl(kScratchRegister, Immediate(63));
+#endif
     __ xor_(rdx, kScratchRegister);  // Flip sign.
     // rdx is value to store.
     if (overwrite_) {
+#ifdef NACL
+      __ movl(FieldOperand(rax, HeapNumber::kExponentOffset), rdx);
+#else
       __ movq(FieldOperand(rax, HeapNumber::kValueOffset), rdx);
+#endif
     } else {
       __ AllocateHeapNumber(rcx, rbx, &slow);
       // rcx: allocated 'empty' number
+#ifdef NACL
+      __ movl(FieldOperand(rcx, HeapNumber::kExponentOffset), rdx);
+      __ movl(rdx, FieldOperand(rax, HeapNumber::kMantissaOffset));
+      __ movl(FieldOperand(rcx, HeapNumber::kMantissaOffset), rdx);
+#else
       __ movq(FieldOperand(rcx, HeapNumber::kValueOffset), rdx);
+#endif
       __ movq(rax, rcx);
     }
   } else if (op_ == Token::BIT_NOT) {
+    Label do_float;
     // Check if the operand is a heap number.
     __ movq(rdx, FieldOperand(rax, HeapObject::kMapOffset));
     __ CompareRoot(rdx, Heap::kHeapNumberMapRootIndex);
     __ j(not_equal, &slow);
 
     // Convert the heap number in rax to an untagged integer in rcx.
-    IntegerConvert(masm, rax, rax);
+    IntegerConvert(masm, rdx, rax);
 
     // Do the bitwise operation and smi tag the result.
-    __ notl(rax);
-    __ Integer32ToSmi(rax, rax);
+    __ notl(rdx);
+    masm->JumpIfNotValidSmiValue(rdx, &do_float);
+    __ Integer32ToSmi(rax, rdx);
+    __ jmp(&done);
+
+    __ bind(&do_float);
+    __ cvtlsi2sd(xmm0, rdx);
+    if (overwrite_) {
+      __ movsd(FieldOperand(rax, HeapNumber::kValueOffset), xmm0);
+    } else {
+      __ AllocateHeapNumber(rcx, rbx, &slow);
+      __ movsd(FieldOperand(rcx, HeapNumber::kValueOffset), xmm0);
+      __ movl(rax, rcx);
+    }
   }
 
   // Return from the stub.
@@ -8476,10 +8674,10 @@
   //  esp[24]: subject string
   //  esp[32]: JSRegExp object
 
-  static const int kLastMatchInfoOffset = 1 * kPointerSize;
-  static const int kPreviousIndexOffset = 2 * kPointerSize;
-  static const int kSubjectOffset = 3 * kPointerSize;
-  static const int kJSRegExpOffset = 4 * kPointerSize;
+  static const int kLastMatchInfoOffset = 1 * kStackPointerSize;
+  static const int kPreviousIndexOffset = 2 * kStackPointerSize;
+  static const int kSubjectOffset = 3 * kStackPointerSize;
+  static const int kJSRegExpOffset = 4 * kStackPointerSize;
 
   Label runtime;
 
@@ -8631,7 +8829,11 @@
   // r12: code
   // Load used arguments before starting to push arguments for call to native
   // RegExp code to avoid handling changing stack height.
+#ifdef NACL
+  __ SmiToInteger32(rbx, Operand(rsp, kPreviousIndexOffset));
+#else
   __ SmiToInteger64(rbx, Operand(rsp, kPreviousIndexOffset));
+#endif
 
   // rax: subject string
   // rbx: previous index
@@ -8649,7 +8851,7 @@
       masm->ArgumentStackSlotsForCFunctionCall(kRegExpExecuteArguments);
 
   // Argument 7: Indicate that this is a direct call from JavaScript.
-  __ movq(Operand(rsp, (argument_slots_on_stack - 1) * kPointerSize),
+  __ movq(Operand(rsp, (argument_slots_on_stack - 1) * kStackPointerSize),
           Immediate(1));
 
   // Argument 6: Start (high end) of backtracking stack memory area.
@@ -8659,14 +8861,14 @@
   __ addq(r9, Operand(kScratchRegister, 0));
   // Argument 6 passed in r9 on Linux and on the stack on Windows.
 #ifdef _WIN64
-  __ movq(Operand(rsp, (argument_slots_on_stack - 2) * kPointerSize), r9);
+  __ movq(Operand(rsp, (argument_slots_on_stack - 2) * kStackPointerSize), r9);
 #endif
 
   // Argument 5: static offsets vector buffer.
   __ movq(r8, ExternalReference::address_of_static_offsets_vector());
   // Argument 5 passed in r8 on Linux and on the stack on Windows.
 #ifdef _WIN64
-  __ movq(Operand(rsp, (argument_slots_on_stack - 3) * kPointerSize), r8);
+  __ movq(Operand(rsp, (argument_slots_on_stack - 3) * kStackPointerSize), r8);
 #endif
 
   // First four arguments are passed in registers on both Linux and Windows.
@@ -8711,6 +8913,9 @@
 
   // Locate the code entry and call it.
   __ addq(r12, Immediate(Code::kHeaderSize - kHeapObjectTag));
+#ifdef  NACL
+  NACL_PATCH_INSTRUCTION_START(r12);
+#endif
   __ CallCFunction(r12, kRegExpExecuteArguments);
 
   // rsi is caller save, as it is used to pass parameter.
@@ -8737,7 +8942,7 @@
   __ bind(&failure);
   // For failure and exception return null.
   __ Move(rax, Factory::null_value());
-  __ ret(4 * kPointerSize);
+  __ ret(4 * kStackPointerSize);
 
   // Load RegExp data.
   __ bind(&success);
@@ -8787,7 +8992,7 @@
   // Store the smi value in the last match info.
   __ movq(FieldOperand(rbx,
                        rdx,
-                       times_pointer_size,
+                       times_heap_pointer_size,
                        RegExpImpl::kFirstCaptureOffset),
           rdi);
   __ jmp(&next_capture);
@@ -8795,7 +9000,7 @@
 
   // Return last match info.
   __ movq(rax, Operand(rsp, kLastMatchInfoOffset));
-  __ ret(4 * kPointerSize);
+  __ ret(4 * kStackPointerSize);
 
   // Do the runtime call to execute the regexp.
   __ bind(&runtime);
@@ -8898,11 +9103,11 @@
 void NumberToStringStub::Generate(MacroAssembler* masm) {
   Label runtime;
 
-  __ movq(rbx, Operand(rsp, kPointerSize));
+  __ movq(rbx, Operand(rsp, kStackPointerSize));
 
   // Generate code to lookup number in the number string cache.
   GenerateLookupNumberStringCache(masm, rbx, rax, r8, r9, false, &runtime);
-  __ ret(1 * kPointerSize);
+  __ ret(1 * kStackPointerSize);
 
   __ bind(&runtime);
   // Handle number to string in the runtime system if not found in the cache.
@@ -8951,48 +9156,31 @@
     // Test for NaN. Sadly, we can't just compare to Factory::nan_value(),
     // so we do the second best thing - test it ourselves.
     // Note: if cc_ != equal, never_nan_nan_ is not used.
+    __ Set(rax, EQUAL);
     if (never_nan_nan_ && (cc_ == equal)) {
-      __ Set(rax, EQUAL);
       __ ret(0);
     } else {
-      Label return_equal;
       Label heap_number;
       // If it's not a heap number, then return equal.
       __ Cmp(FieldOperand(rdx, HeapObject::kMapOffset),
              Factory::heap_number_map());
       __ j(equal, &heap_number);
-      __ bind(&return_equal);
-      __ Set(rax, EQUAL);
       __ ret(0);
 
       __ bind(&heap_number);
-      // It is a heap number, so return non-equal if it's NaN and equal if
-      // it's not NaN.
-      // The representation of NaN values has all exponent bits (52..62) set,
-      // and not all mantissa bits (0..51) clear.
-      // We only allow QNaNs, which have bit 51 set (which also rules out
-      // the value being Infinity).
+      // It is a heap number, so return  equal if it's not NaN.
+      // For NaN, return 1 for every condition except greater and
+      // greater-equal.  Return -1 for them, so the comparison yields
+      // false for all conditions except not-equal.
 
-      // Value is a QNaN if value & kQuietNaNMask == kQuietNaNMask, i.e.,
-      // all bits in the mask are set. We only need to check the word
-      // that contains the exponent and high bit of the mantissa.
-      ASSERT_NE(0, (kQuietNaNHighBitsMask << 1) & 0x80000000u);
-      __ movl(rdx, FieldOperand(rdx, HeapNumber::kExponentOffset));
-      __ xorl(rax, rax);
-      __ addl(rdx, rdx);  // Shift value and mask so mask applies to top bits.
-      __ cmpl(rdx, Immediate(kQuietNaNHighBitsMask << 1));
-      if (cc_ == equal) {
-        __ setcc(above_equal, rax);
-        __ ret(0);
-      } else {
-        Label nan;
-        __ j(above_equal, &nan);
-        __ Set(rax, EQUAL);
-        __ ret(0);
-        __ bind(&nan);
-        __ Set(rax, NegativeComparisonResult(cc_));
-        __ ret(0);
+      __ movsd(xmm0, FieldOperand(rdx, HeapNumber::kValueOffset));
+      __ ucomisd(xmm0, xmm0);
+      __ setcc(parity_even, rax);
+      // rax is 0 for equal non-NaN heapnumbers, 1 for NaNs.
+      if (cc_ == greater_equal || cc_ == greater) {
+        __ neg(rax);
       }
+      __ ret(0);
     }
 
     __ bind(&not_identical);
@@ -9077,7 +9265,7 @@
     __ cmovq(above, rax, rcx);
     __ movq(rcx, Immediate(-1));
     __ cmovq(below, rax, rcx);
-    __ ret(2 * kPointerSize);  // rax, rdx were pushed
+    __ ret(2 * kStackPointerSize);  // rax, rdx were pushed
 
     // If one of the numbers was NaN, then the result is always false.
     // The cc is never not-equal.
@@ -9088,7 +9276,7 @@
     } else {
       __ Set(rax, -1);
     }
-    __ ret(2 * kPointerSize);  // rax, rdx were pushed
+    __ ret(2 * kStackPointerSize);  // rax, rdx were pushed
 
     // The number comparison code did not provide a valid result.
     __ bind(&non_number_comparison);
@@ -9103,7 +9291,7 @@
     // We've already checked for object identity, so if both operands
     // are symbols they aren't equal. Register eax (not rax) already holds a
     // non-zero value, which indicates not equal, so just return.
-    __ ret(2 * kPointerSize);
+    __ ret(2 * kStackPointerSize);
   }
 
   __ bind(&check_for_strings);
@@ -9203,7 +9391,7 @@
 
   // Get the object - go slow case if it's a smi.
   Label slow;
-  __ movq(rax, Operand(rsp, 2 * kPointerSize));
+  __ movq(rax, Operand(rsp, 2 * kStackPointerSize));
   __ JumpIfSmi(rax, &slow);
 
   // Check that the left hand is a JS object. Leave its map in rax.
@@ -9213,7 +9401,7 @@
   __ j(above, &slow);
 
   // Get the prototype of the function.
-  __ movq(rdx, Operand(rsp, 1 * kPointerSize));
+  __ movq(rdx, Operand(rsp, 1 * kStackPointerSize));
   // rdx is function, rax is map.
 
   // Look up the function and the map in the instanceof cache.
@@ -9223,7 +9411,7 @@
   __ CompareRoot(rax, Heap::kInstanceofCacheMapRootIndex);
   __ j(not_equal, &miss);
   __ LoadRoot(rax, Heap::kInstanceofCacheAnswerRootIndex);
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 
   __ bind(&miss);
   __ TryGetFunctionPrototype(rdx, rbx, &slow);
@@ -9263,12 +9451,12 @@
   // Store bitwise zero in the cache.  This is a Smi in GC terms.
   ASSERT_EQ(0, kSmiTag);
   __ StoreRoot(rax, Heap::kInstanceofCacheAnswerRootIndex);
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 
   __ bind(&is_not_instance);
   // We have to store a non-zero value in the cache.
   __ StoreRoot(kScratchRegister, Heap::kInstanceofCacheAnswerRootIndex);
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 
   // Slow-case: Go through the JavaScript implementation.
   __ bind(&slow);
@@ -9285,7 +9473,7 @@
   // The displacement is used for skipping the return address and the
   // frame pointer on the stack. It is the offset of the last
   // parameter (if any) relative to the frame pointer.
-  static const int kDisplacement = 2 * kPointerSize;
+  static const int kDisplacement = 2 * kStackPointerSize;
 
   // Check if the calling frame is an arguments adaptor frame.
   Label adaptor_frame, try_allocate, runtime;
@@ -9295,7 +9483,7 @@
   __ j(equal, &adaptor_frame);
 
   // Get the length from the frame.
-  __ SmiToInteger32(rcx, Operand(rsp, 1 * kPointerSize));
+  __ SmiToInteger32(rcx, Operand(rsp, 1 * kStackPointerSize));
   __ jmp(&try_allocate);
 
   // Patch the arguments.length and the parameters pointer.
@@ -9304,11 +9492,11 @@
                     Operand(rdx,
                             ArgumentsAdaptorFrameConstants::kLengthOffset));
   // Space on stack must already hold a smi.
-  __ Integer32ToSmiField(Operand(rsp, 1 * kPointerSize), rcx);
+  __ Integer32ToSmiField(Operand(rsp, 1 * kStackPointerSize), rcx);
   // Do not clobber the length index for the indexing operation since
   // it is used compute the size for allocation later.
-  __ lea(rdx, Operand(rdx, rcx, times_pointer_size, kDisplacement));
-  __ movq(Operand(rsp, 2 * kPointerSize), rdx);
+  __ lea(rdx, Operand(rdx, rcx, times_stack_pointer_size, kDisplacement));
+  __ movq(Operand(rsp, 2 * kStackPointerSize), rdx);
 
   // Try the new space allocation. Start out with computing the size of
   // the arguments object and the elements array.
@@ -9316,7 +9504,7 @@
   __ bind(&try_allocate);
   __ testl(rcx, rcx);
   __ j(zero, &add_arguments_object);
-  __ leal(rcx, Operand(rcx, times_pointer_size, FixedArray::kHeaderSize));
+  __ leal(rcx, Operand(rcx, times_heap_pointer_size, FixedArray::kHeaderSize));
   __ bind(&add_arguments_object);
   __ addl(rcx, Immediate(Heap::kArgumentsObjectSize));
 
@@ -9340,12 +9528,12 @@
 
   // Setup the callee in-object property.
   ASSERT(Heap::arguments_callee_index == 0);
-  __ movq(kScratchRegister, Operand(rsp, 3 * kPointerSize));
+  __ movq(kScratchRegister, Operand(rsp, 3 * kStackPointerSize));
   __ movq(FieldOperand(rax, JSObject::kHeaderSize), kScratchRegister);
 
   // Get the length (smi tagged) and set that as an in-object property too.
   ASSERT(Heap::arguments_length_index == 1);
-  __ movq(rcx, Operand(rsp, 1 * kPointerSize));
+  __ movq(rcx, Operand(rsp, 1 * kStackPointerSize));
   __ movq(FieldOperand(rax, JSObject::kHeaderSize + kPointerSize), rcx);
 
   // If there are no actual arguments, we're done.
@@ -9354,7 +9542,7 @@
   __ j(zero, &done);
 
   // Get the parameters pointer from the stack and untag the length.
-  __ movq(rdx, Operand(rsp, 2 * kPointerSize));
+  __ movq(rdx, Operand(rsp, 2 * kStackPointerSize));
 
   // Setup the elements pointer in the allocated arguments object and
   // initialize the header in the elements fixed array.
@@ -9368,16 +9556,16 @@
   // Copy the fixed array slots.
   Label loop;
   __ bind(&loop);
-  __ movq(kScratchRegister, Operand(rdx, -1 * kPointerSize));  // Skip receiver.
+  __ movq(kScratchRegister, Operand(rdx, -1 * kStackPointerSize));  // Skip receiver.
   __ movq(FieldOperand(rdi, FixedArray::kHeaderSize), kScratchRegister);
   __ addq(rdi, Immediate(kPointerSize));
-  __ subq(rdx, Immediate(kPointerSize));
+  __ subq(rdx, Immediate(kStackPointerSize));
   __ decl(rcx);
   __ j(not_zero, &loop);
 
   // Return and remove the on-stack parameters.
   __ bind(&done);
-  __ ret(3 * kPointerSize);
+  __ ret(3 * kStackPointerSize);
 
   // Do the runtime call to allocate the arguments object.
   __ bind(&runtime);
@@ -9391,7 +9579,7 @@
   // The displacement is used for skipping the frame pointer on the
   // stack. It is the offset of the last parameter (if any) relative
   // to the frame pointer.
-  static const int kDisplacement = 1 * kPointerSize;
+  static const int kDisplacement = 1 * kStackPointerSize;
 
   // Check that the key is a smi.
   Label slow;
@@ -9411,9 +9599,9 @@
   __ j(above_equal, &slow);
 
   // Read the argument from the stack and return it.
-  SmiIndex index = masm->SmiToIndex(rax, rax, kPointerSizeLog2);
+  SmiIndex index = masm->SmiToIndex(rax, rax, kStackPointerSizeLog2);
   __ lea(rbx, Operand(rbp, index.reg, index.scale, 0));
-  index = masm->SmiToNegativeIndex(rdx, rdx, kPointerSizeLog2);
+  index = masm->SmiToNegativeIndex(rdx, rdx, kStackPointerSizeLog2);
   __ movq(rax, Operand(rbx, index.reg, index.scale, kDisplacement));
   __ Ret();
 
@@ -9426,9 +9614,9 @@
   __ j(above_equal, &slow);
 
   // Read the argument from the stack and return it.
-  index = masm->SmiToIndex(rax, rcx, kPointerSizeLog2);
+  index = masm->SmiToIndex(rax, rcx, kStackPointerSizeLog2);
   __ lea(rbx, Operand(rbx, index.reg, index.scale, 0));
-  index = masm->SmiToNegativeIndex(rdx, rdx, kPointerSizeLog2);
+  index = masm->SmiToNegativeIndex(rdx, rdx, kStackPointerSizeLog2);
   __ movq(rax, Operand(rbx, index.reg, index.scale, kDisplacement));
   __ Ret();
 
@@ -9445,9 +9633,9 @@
 void CEntryStub::GenerateThrowTOS(MacroAssembler* masm) {
   // Check that stack should contain next handler, frame pointer, state and
   // return address in that order.
-  ASSERT_EQ(StackHandlerConstants::kFPOffset + kPointerSize,
+  ASSERT_EQ(StackHandlerConstants::kFPOffset + kStackPointerSize,
             StackHandlerConstants::kStateOffset);
-  ASSERT_EQ(StackHandlerConstants::kStateOffset + kPointerSize,
+  ASSERT_EQ(StackHandlerConstants::kStateOffset + kStackPointerSize,
             StackHandlerConstants::kPCOffset);
 
   ExternalReference handler_address(Top::k_handler_address);
@@ -9482,8 +9670,8 @@
   // rbx: pointer to C function  (C callee-saved).
   // rbp: frame pointer  (restored after C call).
   // rsp: stack pointer  (restored after C call).
-  // r14: number of arguments including receiver (C callee-saved).
-  // r15: pointer to the first argument (C callee-saved).
+  // r12: number of arguments including receiver (C callee-saved).
+  // r14: pointer to the first argument (C callee-saved).
   //      This pointer is reused in LeaveExitFrame(), so it is stored in a
   //      callee-saved register.
 
@@ -9523,24 +9711,24 @@
 #ifdef _WIN64
   // Windows 64-bit ABI passes arguments in rcx, rdx, r8, r9
   // Store Arguments object on stack, below the 4 WIN64 ABI parameter slots.
-  __ movq(Operand(rsp, 4 * kPointerSize), r14);  // argc.
-  __ movq(Operand(rsp, 5 * kPointerSize), r15);  // argv.
+  __ movq(Operand(rsp, 4 * kStackPointerSize), r12);  // argc.
+  __ movq(Operand(rsp, 5 * kStackPointerSize), r14);  // argv.
   if (result_size_ < 2) {
     // Pass a pointer to the Arguments object as the first argument.
     // Return result in single register (rax).
-    __ lea(rcx, Operand(rsp, 4 * kPointerSize));
+    __ lea(rcx, Operand(rsp, 4 * kStackPointerSize));
   } else {
     ASSERT_EQ(2, result_size_);
     // Pass a pointer to the result location as the first argument.
-    __ lea(rcx, Operand(rsp, 6 * kPointerSize));
+    __ lea(rcx, Operand(rsp, 6 * kStackPointerSize));
     // Pass a pointer to the Arguments object as the second argument.
-    __ lea(rdx, Operand(rsp, 4 * kPointerSize));
+    __ lea(rdx, Operand(rsp, 4 * kStackPointerSize));
   }
 
 #else  // _WIN64
   // GCC passes arguments in rdi, rsi, rdx, rcx, r8, r9.
-  __ movq(rdi, r14);  // argc.
-  __ movq(rsi, r15);  // argv.
+  __ movq(rdi, r12);  // argc.
+  __ movq(rsi, r14);  // argv.
 #endif
   __ call(rbx);
   // Result is in rax - do not destroy this register!
@@ -9560,8 +9748,8 @@
     // Read result values stored on stack. Result is stored
     // above the four argument mirror slots and the two
     // Arguments object slots.
-    __ movq(rax, Operand(rsp, 6 * kPointerSize));
-    __ movq(rdx, Operand(rsp, 7 * kPointerSize));
+    __ movq(rax, Operand(rsp, 6 * kStackPointerSize));
+    __ movq(rdx, Operand(rsp, 7 * kStackPointerSize));
   }
 #endif
   __ lea(rcx, Operand(rax, 1));
@@ -9648,14 +9836,14 @@
   __ xor_(rsi, rsi);
 
   // Restore registers from handler.
-  ASSERT_EQ(StackHandlerConstants::kNextOffset + kPointerSize,
+  ASSERT_EQ(StackHandlerConstants::kNextOffset + kStackPointerSize,
             StackHandlerConstants::kFPOffset);
   __ pop(rbp);  // FP
-  ASSERT_EQ(StackHandlerConstants::kFPOffset + kPointerSize,
+  ASSERT_EQ(StackHandlerConstants::kFPOffset + kStackPointerSize,
             StackHandlerConstants::kStateOffset);
   __ pop(rdx);  // State
 
-  ASSERT_EQ(StackHandlerConstants::kStateOffset + kPointerSize,
+  ASSERT_EQ(StackHandlerConstants::kStateOffset + kStackPointerSize,
             StackHandlerConstants::kPCOffset);
   __ ret(0);
 }
@@ -9670,7 +9858,7 @@
     // Get the receiver from the stack.
     // +1 ~ return address
     Label receiver_is_value, receiver_is_js_object;
-    __ movq(rax, Operand(rsp, (argc_ + 1) * kPointerSize));
+    __ movq(rax, Operand(rsp, (argc_ + 1) * kStackPointerSize));
 
     // Check if receiver is a smi (which is a number value).
     __ JumpIfSmi(rax, &receiver_is_value);
@@ -9685,14 +9873,14 @@
     __ push(rax);
     __ InvokeBuiltin(Builtins::TO_OBJECT, CALL_FUNCTION);
     __ LeaveInternalFrame();
-    __ movq(Operand(rsp, (argc_ + 1) * kPointerSize), rax);
+    __ movq(Operand(rsp, (argc_ + 1) * kStackPointerSize), rax);
 
     __ bind(&receiver_is_js_object);
   }
 
   // Get the function to call from the stack.
   // +2 ~ receiver, return address
-  __ movq(rdi, Operand(rsp, (argc_ + 2) * kPointerSize));
+  __ movq(rdi, Operand(rsp, (argc_ + 2) * kStackPointerSize));
 
   // Check that the function really is a JavaScript function.
   __ JumpIfSmi(rdi, &slow);
@@ -9708,7 +9896,7 @@
   __ bind(&slow);
   // CALL_NON_FUNCTION expects the non-function callee as receiver (instead
   // of the original receiver from the call site).
-  __ movq(Operand(rsp, (argc_ + 1) * kPointerSize), rdi);
+  __ movq(Operand(rsp, (argc_ + 1) * kStackPointerSize), rdi);
   __ Set(rax, argc_);
   __ Set(rbx, 0);
   __ GetBuiltinEntry(rdx, Builtins::CALL_NON_FUNCTION);
@@ -9741,9 +9929,10 @@
   // rbx: pointer to builtin function  (C callee-saved).
   // rbp: frame pointer of exit frame  (restored after C call).
   // rsp: stack pointer (restored after C call).
-  // r14: number of arguments including receiver (C callee-saved).
-  // r15: argv pointer (C callee-saved).
+  // r12: number of arguments including receiver (C callee-saved).
+  // r14: argv pointer (C callee-saved).
 
+
   Label throw_normal_exception;
   Label throw_termination_exception;
   Label throw_out_of_memory_exception;
@@ -9808,7 +9997,13 @@
   __ push(r12);
   __ push(r13);
   __ push(r14);
+#ifdef NACL
+  // NACL_CHANGE(pmarch) NaCl does not allow to mess with r15
+  // we push r14 twice to preserve stack alignment
+  __ push(r14);
+#else
   __ push(r15);
+#endif
   __ push(rdi);
   __ push(rsi);
   __ push(rbx);
@@ -9865,13 +10060,16 @@
     __ load_rax(entry);
   }
   __ lea(kScratchRegister, FieldOperand(rax, Code::kHeaderSize));
+#ifdef NACL
+  NACL_PATCH_INSTRUCTION_START(kScratchRegister);
+#endif
   __ call(kScratchRegister);
 
   // Unlink this frame from the handler chain.
   __ movq(kScratchRegister, ExternalReference(Top::k_handler_address));
   __ pop(Operand(kScratchRegister, 0));
   // Pop next_sp.
-  __ addq(rsp, Immediate(StackHandlerConstants::kSize - kPointerSize));
+  __ addq(rsp, Immediate(StackHandlerConstants::kSize - kStackPointerSize));
 
 #ifdef ENABLE_LOGGING_AND_PROFILING
   // If current EBP value is the same as js_entry_sp value, it means that
@@ -9892,11 +10090,17 @@
   __ pop(rbx);
   __ pop(rsi);
   __ pop(rdi);
+#ifdef NACL
+  __ pop(r14);
+  // NACL_CHANGE(pmarch) NaCl does not allow to mess with r15
+  // push r14 twice to preserve stack alignment
+#else
   __ pop(r15);
+#endif
   __ pop(r14);
   __ pop(r13);
   __ pop(r12);
-  __ addq(rsp, Immediate(2 * kPointerSize));  // remove markers
+  __ addq(rsp, Immediate(2 * kStackPointerSize));  // remove markers
 
   // Restore frame pointer and return.
   __ pop(rbp);
@@ -10255,8 +10459,8 @@
     }
   }
   if (!HasArgsInRegisters()) {
-    __ movq(right, Operand(rsp, 1 * kPointerSize));
-    __ movq(left, Operand(rsp, 2 * kPointerSize));
+    __ movq(right, Operand(rsp, 1 * kStackPointerSize));
+    __ movq(left, Operand(rsp, 2 * kStackPointerSize));
   }
 
   Label not_smis;
@@ -10343,7 +10547,7 @@
           __ SmiShiftLogicalRight(left, left, right, slow);
           break;
         case Token::SHL:
-          __ SmiShiftLeft(left, left, right);
+          __ SmiShiftLeft(left, left, right, &use_fp_on_smis);
           break;
         default:
           UNREACHABLE();
@@ -10363,6 +10567,7 @@
   // operations on known smis (e.g., if the result of the operation
   // overflowed the smi range).
   switch (op_) {
+    case Token::SHL:
     case Token::ADD:
     case Token::SUB:
     case Token::MUL:
@@ -10375,12 +10580,19 @@
       }
       // left is rdx, right is rax.
       __ AllocateHeapNumber(rbx, rcx, slow);
-      FloatingPointHelper::LoadSSE2SmiOperands(masm);
+
+      if (op_ == Token::SHL) 
+         // rdx contains a result which is not a Smi
+         __ cvtlsi2sd(xmm0, rdx);
+      else
+        FloatingPointHelper::LoadSSE2SmiOperands(masm);
+
       switch (op_) {
         case Token::ADD: __ addsd(xmm0, xmm1); break;
         case Token::SUB: __ subsd(xmm0, xmm1); break;
         case Token::MUL: __ mulsd(xmm0, xmm1); break;
         case Token::DIV: __ divsd(xmm0, xmm1); break;
+        case Token::SHL: break;
         default: UNREACHABLE();
       }
       __ movsd(FieldOperand(rbx, HeapNumber::kValueOffset), xmm0);
@@ -10446,7 +10658,9 @@
         Label not_floats;
         // rax: y
         // rdx: x
-        ASSERT(!static_operands_type_.IsSmi());
+        // PMARCH disabled as it causes assertion in crypto.js benchmark when
+        // running in shell compiled under debug mode (in vanilla v8)
+        // ASSERT(!static_operands_type_.IsSmi());
         if (static_operands_type_.IsNumber()) {
           if (FLAG_debug_code) {
             // Assert at runtime that inputs are only numbers.
@@ -10520,9 +10734,9 @@
       case Token::BIT_AND:
       case Token::BIT_XOR:
       case Token::SAR:
-      case Token::SHL:
+      case Token::SHL: 
       case Token::SHR: {
-        Label skip_allocation, non_smi_shr_result;
+        Label skip_allocation, non_smi_result;
         Register heap_number_map = r9;
         __ LoadRoot(heap_number_map, Heap::kHeapNumberMapRootIndex);
         if (static_operands_type_.IsNumber()) {
@@ -10538,23 +10752,29 @@
                                               heap_number_map);
         }
         switch (op_) {
-          case Token::BIT_OR:  __ orl(rax, rcx); break;
+          case Token::BIT_OR: __ orl(rax, rcx); break;
           case Token::BIT_AND: __ andl(rax, rcx); break;
           case Token::BIT_XOR: __ xorl(rax, rcx); break;
           case Token::SAR: __ sarl_cl(rax); break;
           case Token::SHL: __ shll_cl(rax); break;
           case Token::SHR: {
-            __ shrl_cl(rax);
+             __ shrl_cl(rax);
             // Check if result is negative. This can only happen for a shift
             // by zero.
             __ testl(rax, rax);
-            __ j(negative, &non_smi_shr_result);
+            __ j(negative, &non_smi_result);
             break;
           }
           default: UNREACHABLE();
         }
 
+        masm->JumpIfNotValidSmiValue(rax, &non_smi_result);
+
+#ifdef NACL
+        STATIC_ASSERT(kSmiValueSize == 31);
+#else
         STATIC_ASSERT(kSmiValueSize == 32);
+#endif        
         // Tag smi result and return.
         __ Integer32ToSmi(rax, rax);
         GenerateReturn(masm);
@@ -10563,48 +10783,65 @@
         // returned immediately as a smi.
         // We might need to allocate a HeapNumber if we shift a negative
         // number right by zero (i.e., convert to UInt32).
-        if (op_ == Token::SHR) {
-          ASSERT(non_smi_shr_result.is_linked());
-          __ bind(&non_smi_shr_result);
-          // Allocate a heap number if needed.
-          __ movl(rbx, rax);  // rbx holds result value (uint32 value as int64).
-          switch (mode_) {
-            case OVERWRITE_LEFT:
-            case OVERWRITE_RIGHT:
-              // If the operand was an object, we skip the
-              // allocation of a heap number.
-              __ movq(rax, Operand(rsp, mode_ == OVERWRITE_RIGHT ?
-                                   1 * kPointerSize : 2 * kPointerSize));
-              __ JumpIfNotSmi(rax, &skip_allocation);
-              // Fall through!
-            case NO_OVERWRITE:
-              // Allocate heap number in new space.
-              // Not using AllocateHeapNumber macro in order to reuse
-              // already loaded heap_number_map.
-              __ AllocateInNewSpace(HeapNumber::kSize,
-                                    rax,
-                                    rcx,
-                                    no_reg,
-                                    &call_runtime,
-                                    TAG_OBJECT);
-              // Set the map.
-              if (FLAG_debug_code) {
-                __ AbortIfNotRootValue(heap_number_map,
-                                       Heap::kHeapNumberMapRootIndex,
-                                       "HeapNumberMap register clobbered.");
-              }
-              __ movq(FieldOperand(rax, HeapObject::kMapOffset),
-                      heap_number_map);
-              __ bind(&skip_allocation);
-              break;
-            default: UNREACHABLE();
+        switch(op_) {
+          case Token::BIT_OR:
+          case Token::BIT_AND:
+          case Token::BIT_XOR:
+          case Token::SAR:
+          case Token::SHL: 
+          case Token::SHR: {
+            ASSERT(non_smi_result.is_linked());
+            __ bind(&non_smi_result);
+            // Allocate a heap number if needed.
+            __ movl(rbx, rax);  // rbx holds result value (uint32 value as int64).
+            switch (mode_) {
+              case OVERWRITE_LEFT:
+              case OVERWRITE_RIGHT:
+                // If the operand was an object, we skip the
+                // allocation of a heap number.
+                __ movq(rax, Operand(rsp, mode_ == OVERWRITE_RIGHT ?
+                                     1 * kStackPointerSize : 2 * kStackPointerSize));
+                __ JumpIfNotSmi(rax, &skip_allocation);
+                // Fall through!
+              case NO_OVERWRITE:
+                // Allocate heap number in new space.
+                // Not using AllocateHeapNumber macro in order to reuse
+                // already loaded heap_number_map.
+                __ AllocateInNewSpace(HeapNumber::kSize,
+                                      rax,
+                                      rcx,
+                                      no_reg,
+                                      &call_runtime,
+                                      TAG_OBJECT);
+                // Set the map.
+                if (FLAG_debug_code) {
+                  __ AbortIfNotRootValue(heap_number_map,
+                                         Heap::kHeapNumberMapRootIndex,
+                                         "HeapNumberMap register clobbered.");
+                }
+                __ movq(FieldOperand(rax, HeapObject::kMapOffset),
+                        heap_number_map);
+                __ bind(&skip_allocation);
+                break;
+              default: UNREACHABLE();
+            }
+            // Store the result in the HeapNumber and return.
+#ifdef NACL          
+            if (op_ == Token::SHR)
+              // when we perform shr we convert any negative results into positive
+              // unsiged integers (this is the case of x >>> 0)
+              __ cvtqsi2sd(xmm0, rbx);
+            else
+              __ cvtlsi2sd(xmm0, rbx);
+#else
+            __ cvtqsi2sd(xmm0, rbx);
+#endif
+            __ movsd(FieldOperand(rax, HeapNumber::kValueOffset), xmm0);
+            GenerateReturn(masm);
+            break;
           }
-          // Store the result in the HeapNumber and return.
-          __ cvtqsi2sd(xmm0, rbx);
-          __ movsd(FieldOperand(rax, HeapNumber::kValueOffset), xmm0);
-          GenerateReturn(masm);
+          default: UNREACHABLE(); break;
         }
-
         break;
       }
       default: UNREACHABLE(); break;
@@ -10666,7 +10903,7 @@
 
       // Replace second argument on stack and tailcall string add stub to make
       // the result.
-      __ movq(Operand(rsp, 1 * kPointerSize), rbx);
+      __ movq(Operand(rsp, 1 * kStackPointerSize), rbx);
       __ TailCallStub(&string_add_stub);
 
       // Only first argument is a string.
@@ -10726,8 +10963,8 @@
 
 void GenericBinaryOpStub::GenerateLoadArguments(MacroAssembler* masm) {
   ASSERT(!HasArgsInRegisters());
-  __ movq(rax, Operand(rsp, 1 * kPointerSize));
-  __ movq(rdx, Operand(rsp, 2 * kPointerSize));
+  __ movq(rax, Operand(rsp, 1 * kStackPointerSize));
+  __ movq(rdx, Operand(rsp, 2 * kStackPointerSize));
 }
 
 
@@ -10735,7 +10972,7 @@
   // If arguments are not passed in registers remove them from the stack before
   // returning.
   if (!HasArgsInRegisters()) {
-    __ ret(2 * kPointerSize);  // Remove both operands
+    __ ret(2 * kStackPointerSize);  // Remove both operands
   } else {
     __ ret(0);
   }
@@ -11062,8 +11299,8 @@
   Label string_add_runtime;
 
   // Load the two arguments.
-  __ movq(rax, Operand(rsp, 2 * kPointerSize));  // First argument.
-  __ movq(rdx, Operand(rsp, 1 * kPointerSize));  // Second argument.
+  __ movq(rax, Operand(rsp, 2 * kStackPointerSize));  // First argument.
+  __ movq(rdx, Operand(rsp, 1 * kStackPointerSize));  // Second argument.
 
   // Make sure that both arguments are strings if not known in advance.
   if (string_check_) {
@@ -11090,7 +11327,7 @@
   __ j(not_zero, &second_not_zero_length);
   // Second string is empty, result is first string which is already in rax.
   __ IncrementCounter(&Counters::string_add_native, 1);
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
   __ bind(&second_not_zero_length);
   __ movq(rbx, FieldOperand(rax, String::kLengthOffset));
   __ SmiTest(rbx);
@@ -11098,7 +11335,7 @@
   // First string is empty, result is second string which is in rdx.
   __ movq(rax, rdx);
   __ IncrementCounter(&Counters::string_add_native, 1);
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 
   // Both strings are non-empty.
   // rax: first string
@@ -11121,7 +11358,9 @@
   __ movzxbl(r9, FieldOperand(r9, Map::kInstanceTypeOffset));
 
   // Look at the length of the result of adding the two strings.
+#ifndef NACL
   ASSERT(String::kMaxLength <= Smi::kMaxValue / 2);
+#endif
   __ SmiAdd(rbx, rbx, rcx, NULL);
   // Use the runtime system when adding two one character strings, as it
   // contains optimizations for this specific case using the symbol table.
@@ -11139,12 +11378,26 @@
   // Try to lookup two character string in symbol table. If it is not found
   // just allocate a new one.
   Label make_two_character_string, make_flat_ascii_string;
+#ifdef NACL
+  // we need an extra scratch register to replace r11 which is reserved as a
+  // scratch register for sandboxing memory access operations
+  __ push(rdx);
   StringHelper::GenerateTwoCharacterSymbolTableProbe(
-      masm, rbx, rcx, r14, r12, rdi, r15, &make_two_character_string);
+      masm, rbx, rcx, r14, r12, rdi, rdx, &make_two_character_string);
+  __ pop(rdx);
   __ IncrementCounter(&Counters::string_add_native, 1);
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 
   __ bind(&make_two_character_string);
+  __ pop(rdx);
+#else
+  StringHelper::GenerateTwoCharacterSymbolTableProbe(
+      masm, rbx, rcx, r14, r12, rdi, r11, &make_two_character_string);
+  __ IncrementCounter(&Counters::string_add_native, 1);
+  __ ret(2 * kStackPointerSize);
+
+  __ bind(&make_two_character_string);
+#endif
   __ Set(rbx, 2);
   __ jmp(&make_flat_ascii_string);
 
@@ -11182,7 +11435,7 @@
   __ movq(FieldOperand(rcx, ConsString::kSecondOffset), rdx);
   __ movq(rax, rcx);
   __ IncrementCounter(&Counters::string_add_native, 1);
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
   __ bind(&non_ascii);
   // At least one of the strings is two-byte. Check whether it happens
   // to contain only ascii characters.
@@ -11232,7 +11485,11 @@
 
   __ bind(&make_flat_ascii_string);
   // Both strings are ascii strings. As they are short they are both flat.
-  __ AllocateAsciiString(rcx, rbx, rdi, r14, r15, &string_add_runtime);
+#ifdef NACL
+  __ AllocateAsciiString(rcx, rbx, rdi, r14, r12, &string_add_runtime);
+#else
+  __ AllocateAsciiString(rcx, rbx, rdi, r14, r11, &string_add_runtime);
+#endif
   // rcx: result string
   __ movq(rbx, rcx);
   // Locate first character of result.
@@ -11256,7 +11513,7 @@
   StringHelper::GenerateCopyCharacters(masm, rcx, rdx, rdi, true);
   __ movq(rax, rbx);
   __ IncrementCounter(&Counters::string_add_native, 1);
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 
   // Handle creating a flat two byte result.
   // rax: first string - known to be two byte
@@ -11269,7 +11526,11 @@
   __ j(not_zero, &string_add_runtime);
   // Both strings are two byte strings. As they are short they are both
   // flat.
-  __ AllocateTwoByteString(rcx, rbx, rdi, r14, r15, &string_add_runtime);
+#ifdef NACL
+  __ AllocateTwoByteString(rcx, rbx, rdi, r14, r12, &string_add_runtime);
+#else
+  __ AllocateTwoByteString(rcx, rbx, rdi, r14, r11, &string_add_runtime);
+#endif
   // rcx: result string
   __ movq(rbx, rcx);
   // Locate first character of result.
@@ -11293,7 +11554,7 @@
   StringHelper::GenerateCopyCharacters(masm, rcx, rdx, rdi, false);
   __ movq(rax, rbx);
   __ IncrementCounter(&Counters::string_add_native, 1);
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 
   // Just jump to runtime to add the two strings.
   __ bind(&string_add_runtime);
@@ -11357,8 +11618,13 @@
 
   // Copy from edi to esi using rep movs instruction.
   __ movl(kScratchRegister, count);
+#ifdef NACL
+  __ shrl(count, Immediate(2));  // Number of doublewords to copy.
+  __ repmovsl();
+#else
   __ shr(count, Immediate(3));  // Number of doublewords to copy.
   __ repmovsq();
+#endif
 
   // Find number of bytes left.
   __ movl(count, kScratchRegister);
@@ -11457,7 +11723,7 @@
     __ movq(candidate,
             FieldOperand(symbol_table,
                          scratch,
-                         times_pointer_size,
+                         times_heap_pointer_size,
                          SymbolTable::kElementsStartOffset));
 
     // If entry is undefined no string with this hash can be found.
@@ -11498,6 +11764,8 @@
   }
 }
 
+// TODO(pmarch) Modify hash function with respect that we use 32bit registers
+// so it can be more afficient
 
 void StringHelper::GenerateHashInit(MacroAssembler* masm,
                                     Register hash,
@@ -11536,6 +11804,7 @@
                                        Register scratch) {
   // hash += hash << 3;
   __ leal(hash, Operand(hash, hash, times_8, 0));
+
   // hash ^= hash >> 11;
   __ movl(scratch, hash);
   __ sarl(scratch, Immediate(11));
@@ -11561,10 +11830,10 @@
   //  rsp[16]: from
   //  rsp[24]: string
 
-  const int kToOffset = 1 * kPointerSize;
-  const int kFromOffset = kToOffset + kPointerSize;
-  const int kStringOffset = kFromOffset + kPointerSize;
-  const int kArgumentsSize = (kStringOffset + kPointerSize) - kToOffset;
+  const int kToOffset = 1 * kStackPointerSize;
+  const int kFromOffset = kToOffset + kStackPointerSize;
+  const int kStringOffset = kFromOffset + kStackPointerSize;
+  const int kArgumentsSize = (kStringOffset + kStackPointerSize) - kToOffset;
 
   // Make sure first argument is a string.
   __ movq(rax, Operand(rsp, kStringOffset));
@@ -11583,7 +11852,9 @@
   __ JumpIfNotBothPositiveSmi(rcx, rdx, &runtime);
 
   __ SmiSub(rcx, rcx, rdx, NULL);  // Overflow doesn't happen.
-  __ j(negative, &runtime);
+  __ cmpq(FieldOperand(rax, String::kLengthOffset), rcx);
+  Label return_rax;
+  __ j(equal, &return_rax);
   // Special handling of sub-strings of length 1 and 2. One character strings
   // are handled in the runtime system (looked up in the single character
   // cache). Two character strings are looked for in the symbol cache.
@@ -11609,7 +11880,7 @@
   Label make_two_character_string;
   StringHelper::GenerateTwoCharacterSymbolTableProbe(
       masm, rbx, rcx, rax, rdx, rdi, r14, &make_two_character_string);
-  __ ret(3 * kPointerSize);
+  __ ret(3 * kStackPointerSize);
 
   __ bind(&make_two_character_string);
   // Setup registers for allocating the two character string.
@@ -11686,6 +11957,8 @@
   // rsi: character of sub string start
   StringHelper::GenerateCopyCharactersREP(masm, rdi, rsi, rcx, false);
   __ movq(rsi, rdx);  // Restore esi.
+
+  __ bind(&return_rax);
   __ IncrementCounter(&Counters::sub_string_native, 1);
   __ ret(kArgumentsSize);
 
@@ -11767,7 +12040,7 @@
 
   // Result is EQUAL.
   __ Move(rax, Smi::FromInt(EQUAL));
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 
   Label result_greater;
   __ bind(&result_not_equal);
@@ -11776,12 +12049,12 @@
 
   // Result is LESS.
   __ Move(rax, Smi::FromInt(LESS));
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 
   // Result is GREATER.
   __ bind(&result_greater);
   __ Move(rax, Smi::FromInt(GREATER));
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 }
 
 
@@ -11793,8 +12066,8 @@
   //  rsp[8]: right string
   //  rsp[16]: left string
 
-  __ movq(rdx, Operand(rsp, 2 * kPointerSize));  // left
-  __ movq(rax, Operand(rsp, 1 * kPointerSize));  // right
+  __ movq(rdx, Operand(rsp, 2 * kStackPointerSize));  // left
+  __ movq(rax, Operand(rsp, 1 * kStackPointerSize));  // right
 
   // Check for identity.
   Label not_same;
@@ -11802,7 +12075,7 @@
   __ j(not_equal, &not_same);
   __ Move(rax, Smi::FromInt(EQUAL));
   __ IncrementCounter(&Counters::string_compare_native, 1);
-  __ ret(2 * kPointerSize);
+  __ ret(2 * kStackPointerSize);
 
   __ bind(&not_same);
 
@@ -11844,10 +12117,10 @@
 
   // Compute x mod y.
   // Load y and x (use argument backing store as temporary storage).
-  __ movsd(Operand(rsp, kPointerSize * 2), xmm1);
-  __ movsd(Operand(rsp, kPointerSize), xmm0);
-  __ fld_d(Operand(rsp, kPointerSize * 2));
-  __ fld_d(Operand(rsp, kPointerSize));
+  __ movsd(Operand(rsp, kStackPointerSize * 2), xmm1);
+  __ movsd(Operand(rsp, kStackPointerSize), xmm0);
+  __ fld_d(Operand(rsp, kStackPointerSize * 2));
+  __ fld_d(Operand(rsp, kStackPointerSize));
 
   // Clear exception flags before operation.
   {
@@ -11883,14 +12156,14 @@
   __ fstp(0);  // Drop result in st(0).
   int64_t kNaNValue = V8_INT64_C(0x7ff8000000000000);
   __ movq(rcx, kNaNValue, RelocInfo::NONE);
-  __ movq(Operand(rsp, kPointerSize), rcx);
-  __ movsd(xmm0, Operand(rsp, kPointerSize));
+  __ movq(Operand(rsp, kStackPointerSize), rcx);
+  __ movsd(xmm0, Operand(rsp, kStackPointerSize));
   __ jmp(&return_result);
 
   // If result is valid, return that.
   __ bind(&valid_result);
-  __ fstp_d(Operand(rsp, kPointerSize));
-  __ movsd(xmm0, Operand(rsp, kPointerSize));
+  __ fstp_d(Operand(rsp, kStackPointerSize));
+  __ movsd(xmm0, Operand(rsp, kStackPointerSize));
 
   // Clean up FPU stack and exceptions and return xmm0
   __ bind(&return_result);
Index: src/x64/regexp-macro-assembler-x64.h
===================================================================
--- src/x64/regexp-macro-assembler-x64.h	(revision 4925)
+++ src/x64/regexp-macro-assembler-x64.h	(working copy)
@@ -42,6 +42,9 @@
   virtual void AdvanceRegister(int reg, int by);
   virtual void Backtrack();
   virtual void Bind(Label* label);
+#ifdef NACL
+  virtual void Bind_Aligned(Label* label);
+#endif
   virtual void CheckAtStart(Label* on_at_start);
   virtual void CheckCharacter(uint32_t c, Label* on_equal);
   virtual void CheckCharacterAfterAnd(uint32_t c,
@@ -99,6 +102,10 @@
   virtual void ClearRegisters(int reg_from, int reg_to);
   virtual void WriteStackPointerToRegister(int reg);
 
+#ifdef NACL
+  void AddInstructionStartToRegister(Register reg);
+#endif
+
   static Result Match(Handle<Code> regexp,
                       Handle<String> subject,
                       int* offsets_vector,
@@ -124,8 +131,8 @@
   // Offsets from rbp of function parameters and stored registers.
   static const int kFramePointer = 0;
   // Above the frame pointer - function parameters and return address.
-  static const int kReturn_eip = kFramePointer + kPointerSize;
-  static const int kFrameAlign = kReturn_eip + kPointerSize;
+  static const int kReturn_eip = kFramePointer + kStackPointerSize;
+  static const int kFrameAlign = kReturn_eip + kStackPointerSize;
 
 #ifdef _WIN64
   // Parameters (first four passed as registers, but with room on stack).
@@ -134,48 +141,48 @@
   // use this space to store the register passed parameters.
   static const int kInputString = kFrameAlign;
   // StartIndex is passed as 32 bit int.
-  static const int kStartIndex = kInputString + kPointerSize;
-  static const int kInputStart = kStartIndex + kPointerSize;
-  static const int kInputEnd = kInputStart + kPointerSize;
-  static const int kRegisterOutput = kInputEnd + kPointerSize;
-  static const int kStackHighEnd = kRegisterOutput + kPointerSize;
+  static const int kStartIndex = kInputString + kStackPointerSize;
+  static const int kInputStart = kStartIndex + kStackPointerSize;
+  static const int kInputEnd = kInputStart + kStackPointerSize;
+  static const int kRegisterOutput = kInputEnd + kStackPointerSize;
+  static const int kStackHighEnd = kRegisterOutput + kStackPointerSize;
   // DirectCall is passed as 32 bit int (values 0 or 1).
-  static const int kDirectCall = kStackHighEnd + kPointerSize;
+  static const int kDirectCall = kStackHighEnd + kStackPointerSize;
 #else
   // In AMD64 ABI Calling Convention, the first six integer parameters
   // are passed as registers, and caller must allocate space on the stack
   // if it wants them stored. We push the parameters after the frame pointer.
-  static const int kInputString = kFramePointer - kPointerSize;
-  static const int kStartIndex = kInputString - kPointerSize;
-  static const int kInputStart = kStartIndex - kPointerSize;
-  static const int kInputEnd = kInputStart - kPointerSize;
-  static const int kRegisterOutput = kInputEnd - kPointerSize;
-  static const int kStackHighEnd = kRegisterOutput - kPointerSize;
+  static const int kInputString = kFramePointer - kStackPointerSize;
+  static const int kStartIndex = kInputString - kStackPointerSize;
+  static const int kInputStart = kStartIndex - kStackPointerSize;
+  static const int kInputEnd = kInputStart - kStackPointerSize;
+  static const int kRegisterOutput = kInputEnd - kStackPointerSize;
+  static const int kStackHighEnd = kRegisterOutput - kStackPointerSize;
   static const int kDirectCall = kFrameAlign;
 #endif
 
 #ifdef _WIN64
   // Microsoft calling convention has three callee-saved registers
   // (that we are using). We push these after the frame pointer.
-  static const int kBackup_rsi = kFramePointer - kPointerSize;
-  static const int kBackup_rdi = kBackup_rsi - kPointerSize;
-  static const int kBackup_rbx = kBackup_rdi - kPointerSize;
+  static const int kBackup_rsi = kFramePointer - kStackPointerSize;
+  static const int kBackup_rdi = kBackup_rsi - kStackPointerSize;
+  static const int kBackup_rbx = kBackup_rdi - kStackPointerSize;
   static const int kLastCalleeSaveRegister = kBackup_rbx;
 #else
   // AMD64 Calling Convention has only one callee-save register that
   // we use. We push this after the frame pointer (and after the
   // parameters).
-  static const int kBackup_rbx = kStackHighEnd - kPointerSize;
+  static const int kBackup_rbx = kStackHighEnd - kStackPointerSize;
   static const int kLastCalleeSaveRegister = kBackup_rbx;
 #endif
 
   // When adding local variables remember to push space for them in
   // the frame in GetCode.
   static const int kInputStartMinusOne =
-      kLastCalleeSaveRegister - kPointerSize;
+      kLastCalleeSaveRegister - kStackPointerSize;
 
   // First register address. Following registers are below it on the stack.
-  static const int kRegisterZero = kInputStartMinusOne - kPointerSize;
+  static const int kRegisterZero = kInputStartMinusOne - kStackPointerSize;
 
   // Initial size of code buffer.
   static const size_t kRegExpCodeSize = 1024;
Index: src/x64/nacl-sandbox-x64.cc
===================================================================
--- src/x64/nacl-sandbox-x64.cc	(revision 0)
+++ src/x64/nacl-sandbox-x64.cc	(revision 0)
@@ -0,0 +1,362 @@
+#if defined(V8_TARGET_ARCH_X64) && defined(NACL)
+
+#include "v8.h"
+#include "assembler.h"
+#include "assembler-x64.h"
+#include "assembler-x64-inl.h"
+#include "nacl-sandbox-x64.h"
+
+namespace v8 {
+namespace internal {
+
+
+void Sandbox::start_sandbox() {
+#ifdef DEBUG
+  if (!(flags_ & NoRegUseCheck))
+    check_register_use();
+#endif
+  pre_sandbox();
+  if (size_ != NoSize)
+    size_ += get_instruction_size();
+  ensure_space();
+  preceding_sandbox();
+}
+
+
+void Sandbox::finish_sandbox() {
+  following_sandbox();
+  verify_ensured_space();
+}
+
+
+// this is to unwrap operands
+// [base + index * scale + disp] ==> [$r15 + index*1 + disp]
+void Sandbox::pre_sandbox() {
+  
+  if (flags_ & RepMovsSandbox) {
+    // repmovs copies data from [rsi] to [rdi]
+    // correcting rsi value with r15 can be done out of repmovs boundle
+    emit_repmovs_presandbox();
+  }
+  
+  switch (mode_) {
+    case (SandboxNone):
+      return;
+    case (SandboxLeft):
+      if (left_target_.is_operand() && 
+          op_needs_unwrapping(left_target_.get_operand()))
+        unwrap_operand(left_target_.get_operand());
+      return;
+    case (SandboxRight):
+      if (right_target_.is_operand() && 
+          op_needs_unwrapping(right_target_.get_operand()))
+        unwrap_operand(right_target_.get_operand());
+      return;
+    case (SandboxBoth):
+      if (left_target_.is_operand() && 
+          op_needs_unwrapping(left_target_.get_operand()))
+        unwrap_operand(left_target_.get_operand());
+      if (right_target_.is_operand() && 
+          op_needs_unwrapping(right_target_.get_operand()))
+        unwrap_operand(right_target_.get_operand());
+      return;
+    default:
+      ASSERT(0);
+  }
+}
+
+
+void Sandbox::unwrap_operand(Operand* op) {
+  Register base = op->get_base();
+  Register index = op->get_index();
+
+  if (!base.is_valid()) {
+    // [index*scale + disp]
+    ASSERT(index.is_valid());
+    op->copyfrom(Operand(r15, index, op->get_scale(), op->get_disp()));
+    return;
+  }
+
+  ASSERT(!is_trusted_reg(base));
+
+  if (!index.is_valid()) {
+    // [base + disp]
+    op->copyfrom(Operand(r15, base, times_1, op->get_disp()));
+    return;
+  }
+
+  // [base + index*scale + disp]
+  ASSERT(index.is_valid());
+
+  assembler_->leal(scratch_reg, *op);
+  op->copyfrom(Operand(r15, scratch_reg, times_1, 0));
+}
+
+
+int Sandbox::get_instruction_size() {
+  int size = 0;
+
+  if (flags_ & RepMovsSandbox) {
+    // when sandboxing repmovs instruction, we need to use 2 movl and 2 addq
+    // instructions
+    return sizeof_movl(rdi) + sizeof_lea();
+  }
+
+  if (left_target_.is_operand())
+    // operand size
+    size += left_target_.get_operand()->get_size();
+ 
+  if (right_target_.is_operand())
+    // operand size
+    size += right_target_.get_operand()->get_size();
+
+  // size of sandboxing code preceeding or following target instruction
+  switch (mode_) {
+    case (SandboxNone):
+      break;
+    case (SandboxLeft):
+      size += get_target_sandbox_size(left_target_);
+      break;
+    case (SandboxRight):
+      size += get_target_sandbox_size(right_target_);
+      break;
+    case (SandboxBoth):
+      size += get_target_sandbox_size(left_target_);
+      size += get_target_sandbox_size(right_target_);
+      break;
+    default:
+      ASSERT(0); 
+  }
+
+  // optional rex32
+  if (flags_ & OptionalRex32)
+    size += optional_rex_32(left_target_, right_target_);
+
+  return size;
+}
+
+
+int Sandbox::optional_rex_32(SandboxTarget left, SandboxTarget right) {
+  int b = 0;
+  Register left_reg = left.get_register();
+  Register right_reg = right.get_register();
+  Operand* left_op = left.get_operand();
+  Operand* right_op = right.get_operand();
+  XMMRegister left_xmm = left.get_xmmregister();
+  XMMRegister right_xmm = right.get_xmmregister();
+
+  if (left_reg.is_valid())
+    b |= left_reg.high_bit();
+  if (right_reg.is_valid())
+    b |= right_reg.high_bit();
+  if (left_op != NULL)
+    b |= left_op->rex_;
+  if (right_op != NULL)
+    b |= right_op->rex_;
+  if (left_xmm.is_valid())
+    b |= left_xmm.code() & 0x8;
+  if (right_xmm.is_valid())
+    b |= right_xmm.code() & 0x8;
+
+  return b ? 1 : 0;
+}
+
+
+int Sandbox::get_target_sandbox_size(SandboxTarget target) {
+  ASSERT(!target.is_empty());
+
+  if (target.is_register()) {
+    if ((flags_ & CallSandbox) ||
+        (flags_ & JumpSandbox)) {
+      // sandboxing call instruction requires andl and andq instructions
+      return sizeof_andl(target.get_register(), Immediate(CALL_MASK))
+          + sizeof_addq();
+    }
+
+    if (reg_needs_sandbox(target.get_register())) {
+      // sandboxing rsp/rbp register involves addq instruction which is placed
+      // after the sandboxed instruction
+      //
+      // PMARCH debug: the later is needed if movq instructions are enabled
+      return sizeof_addq();// + sizeof_movl(target.get_register());
+    }
+  }
+
+  if (target.is_operand() && op_needs_sandbox(target.get_operand())) {
+    // sandboxing of an operand requires movl instuction preceeding the sandbox
+    // instruction
+    return sizeof_movl(target.get_operand()->get_index());
+  }
+
+  return 0;
+}
+
+
+void Sandbox::ensure_space() {
+  if (size_ == NoSize)
+    return;
+
+  ASSERT(size_ <= NACL_CHUNK);
+  int left = NACL_CHUNK - (assembler_->pc_offset() & (NACL_CHUNK-1));
+
+  // if there is not enough space in the corrunt chunk, go to next one
+  if (left < size_)
+    assembler_->next_nacl_boundle();
+
+  if (flags_ & CallSandbox) {
+    // when sandboxing a call instruction, we need to make sure that the
+    // target instuction is placed at the end of a boundle
+    left = NACL_CHUNK - (assembler_->pc_offset() & (NACL_CHUNK-1));
+    assembler_->nops(left - size_);
+  }
+  last_pc_ = assembler_->pc_offset();
+
+  if (assembler_->buffer_overflow()) assembler_->GrowBuffer();
+
+#ifdef DEBUG 
+  space_before_ = assembler_->available_space();
+#endif
+}
+
+
+void Sandbox::verify_ensured_space() {
+#ifdef DEBUG
+  int bytes_generated = space_before_ - assembler_->available_space();
+  ASSERT(bytes_generated < assembler_->kGap);
+#endif
+  
+  if (size_ == NoSize)
+    return;
+
+  int written_bytes = assembler_->pc_offset() - last_pc_;
+
+  if (size_ != written_bytes) {
+#ifdef DEBUG
+    // PMARCH make it look nice
+        fprintf(stderr, "Instruction block size failed in %s:%s:%d\n", fname_, file_, line_);
+        fprintf(stderr, "\tdeclared size %d, acctual size %d\n", size_, written_bytes);
+#endif
+        ASSERT(bytes_generated == size_);
+  }
+}
+
+#ifdef DEBUG
+void Sandbox::check_reg(Register reg) {
+  if (!(!reg.is(r15) &&
+     !reg.is(rbp) &&
+     !reg.is(rsp)))
+     fprintf(stderr, "Instruction uses reserved or trusted registers %s:%s:%d\n", fname_, file_, line_);
+  ASSERT(!reg.is(r15) &&
+         !reg.is(rbp) &&
+         !reg.is(rsp) &&
+         !reg.is(scratch_reg));
+}
+
+void Sandbox::check_op(Operand* op) {
+  if (op == NULL) return;
+  check_reg(op->get_index());
+  ASSERT(!op->get_base().is(scratch_reg));
+}
+
+void Sandbox::check_register_use() {
+  switch (mode_) {
+    case (SandboxNone):
+      check_reg(left_target_.get_register());
+//      check_reg(right_target_.get_register());
+      break;
+    case (SandboxLeft):
+//      check_reg(right_target_.get_register());
+      break;
+    case (SandboxRight):
+      check_reg(left_target_.get_register());
+      break;
+    case (SandboxBoth):
+      break;
+    default:
+      ASSERT(0);
+  }
+
+  check_op(left_target_.get_operand());
+  check_op(left_target_.get_operand());
+  check_op(right_target_.get_operand());
+  check_op(right_target_.get_operand());
+}
+#endif
+
+void Sandbox::preceding_sandbox() {
+
+  if (flags_ & RepMovsSandbox) {
+    emit_repmovs_sandbox();
+    return;
+  }
+  
+  Operand* left = left_target_.get_operand();
+  Operand* right = right_target_.get_operand();
+
+  switch (mode_) {
+    case (SandboxNone):
+      return;
+    case (SandboxLeft):
+      ASSERT(!left_target_.is_xmmregister());
+      if (left != NULL && op_needs_sandbox(left))
+        emit_operand_sandbox(left);
+      if (((flags_ & CallSandbox) ||
+           (flags_ & JumpSandbox)) && 
+          left_target_.is_register())
+        emit_call_sandbox(left_target_.get_register());
+      break;
+    case (SandboxRight):
+      ASSERT(!right_target_.is_xmmregister());
+      if (right != NULL && op_needs_sandbox(right))
+        emit_operand_sandbox(right);
+      break;
+    case (SandboxBoth):
+      ASSERT(!left_target_.is_xmmregister());
+      ASSERT(!right_target_.is_xmmregister());
+      if (left != NULL && op_needs_sandbox(left))
+        emit_operand_sandbox(left);
+      if (right != NULL && op_needs_sandbox(right))
+        emit_operand_sandbox(right);
+      break;
+    default:
+      ASSERT(0);
+  }
+}
+
+
+void Sandbox::following_sandbox() {
+  Register left = left_target_.get_register();
+  Register right = right_target_.get_register();
+
+  switch (mode_) {
+    case (SandboxNone):
+      return;
+    case (SandboxLeft):
+      ASSERT(!left_target_.is_xmmregister());
+      if (left.is_valid() && reg_needs_sandbox(left))
+        emit_register_sandbox(left);
+      break;
+    case (SandboxRight):
+      ASSERT(!right_target_.is_xmmregister());
+      if (right.is_valid() && reg_needs_sandbox(right))
+        emit_register_sandbox(right);
+      break;
+    case (SandboxBoth):
+      ASSERT(!left_target_.is_xmmregister());
+      ASSERT(!right_target_.is_xmmregister());
+      if (left.is_valid() && reg_needs_sandbox(left))
+        emit_register_sandbox(left);
+      if (right.is_valid() && reg_needs_sandbox(right))
+        emit_register_sandbox(right);
+      break;
+    default:
+      ASSERT(0);
+  }
+}
+
+
+} }  // v8::internal
+
+#endif  // V8_TARGET_ARCH_X64 && NACL
+
+
Index: src/x64/nacl-sandbox-x64.h
===================================================================
--- src/x64/nacl-sandbox-x64.h	(revision 0)
+++ src/x64/nacl-sandbox-x64.h	(revision 0)
@@ -0,0 +1,264 @@
+#ifndef V8_X64_NACL_SANDBOX_X64_H_
+#define V8_X64_NACL_SANDBOX_X64_H_
+
+namespace v8 {
+namespace internal {
+
+#ifdef DEBUG
+#define SANDBOX_DEBUG (char *)&__func__, (char *)&__FILE__, __LINE__,
+#else
+#define SANDBOX_DEBUG
+#endif
+
+#define CALL_MASK 0xffffffe0
+
+class SandboxTarget BASE_EMBEDDED {
+ public:
+
+  SandboxTarget(Register reg)
+      : reg_(reg), op_(NULL), xmmreg_(no_xmm_reg) {
+  }
+    
+  SandboxTarget(const Operand &op)
+      : reg_(no_reg),
+        op_(reinterpret_cast<Operand*>(&const_cast<Operand&>(op))),
+        xmmreg_(no_xmm_reg) {
+  }
+
+  SandboxTarget(XMMRegister reg)
+      : reg_(no_reg), op_(NULL), xmmreg_(reg) {
+  }
+
+  Operand* get_operand() {
+    return op_;
+  }
+
+  Register get_register() {
+    return reg_;
+  }
+
+  XMMRegister get_xmmregister() {
+    return xmmreg_;
+  }
+
+  bool is_register() {
+    return reg_.is_valid() ? true : false;
+  }
+
+  bool is_operand() {
+    return op_ == NULL ? false : true;
+  }
+
+  bool is_xmmregister() {
+    return xmmreg_.is_valid() ? true : false;
+  }
+
+  bool is_empty() {
+    return is_register() || is_operand() || is_xmmregister() ? false : true;
+  }
+
+ private:
+
+  Register reg_;
+  Operand *op_;
+  XMMRegister xmmreg_;
+};
+
+
+static const SandboxTarget no_target = SandboxTarget(no_reg);
+
+
+class Sandbox BASE_EMBEDDED {
+ public:
+
+  enum SandboxMode {
+    SandboxNone = 0,
+    SandboxLeft,
+    SandboxRight,
+    SandboxBoth
+  };
+  
+  static const int NoFlags = 0;
+  static const int OptionalRex32  = 1;
+  static const int NoRegUseCheck  = 1<<1;
+  static const int CallSandbox    = 1<<2;
+  static const int JumpSandbox    = 1<<3;
+  static const int RepMovsSandbox = 1<<4;
+  static const int NoSize = -1;
+
+  explicit Sandbox(
+#ifdef DEBUG
+                   char *fname,
+                   char *file,
+                   int line,
+#endif
+                   Assembler *assembler, 
+                   int size = NoSize,
+                   int flags = NoFlags,
+                   SandboxMode mode = SandboxNone,
+                   SandboxTarget left_target = no_target,
+                   SandboxTarget right_target = no_target) : 
+#ifdef DEBUG
+                fname_(fname), 
+                file_(file), 
+                line_(line),
+#endif
+                assembler_(assembler),
+                size_(size),
+                flags_(flags),
+                mode_(mode), 
+                left_target_(left_target),
+                right_target_(right_target) {
+    start_sandbox();
+  }
+
+  ~Sandbox() {
+    finish_sandbox();
+  }
+
+ private:
+ 
+  void start_sandbox();
+  void finish_sandbox();
+  // unwraps operands if needed
+  void pre_sandbox();
+  void unwrap_operand(Operand* op);
+  // computes sandboxing size and terarget instruction size
+  // for reserving this space in NaCl bundle
+  int get_instruction_size();
+  // size of sandboxing code for a target 
+  int get_target_sandbox_size(SandboxTarget target);
+  // checks if optional rex is needed
+  int optional_rex_32(SandboxTarget left, SandboxTarget right);
+  // reserve space in a NaCl boundle for target 
+  // instruction and sandboxing code
+  void ensure_space();
+  // checks whether we emitted the declared amount of bytes, 
+  // for sandbox and target instruction
+  void verify_ensured_space();
+  // sandboxes operands before the target instruction
+  void preceding_sandbox();
+  // sandboxes registers after the target instruction
+  void following_sandbox();
+
+  inline void emit_register_sandbox(Register reg) {
+    ASSERT(is_trusted_reg(reg));
+    ASSERT(!reg.is(r15));
+    ASSERT(!reg.is(scratch_reg));
+// PMARCH
+// this is only neccessary when  movq instructions are enabler, otherwise we can
+// get rid of movl
+//    assembler_->no_sbx_movl(reg, reg);
+    assembler_->no_sbx_addq(reg, r15);
+  }
+
+  inline void emit_operand_sandbox(Operand *op) {
+    ASSERT(is_trusted_reg(op->get_base()) && !is_trusted_reg(op->get_index()));
+    assembler_->no_sbx_movl(op->get_index(), op->get_index());
+  }
+
+  inline void emit_call_sandbox(Register reg) {
+    ASSERT(reg.is_valid() && !is_trusted_reg(reg));
+    assembler_->no_sbx_andl(reg, Immediate(CALL_MASK));
+    assembler_->no_sbx_addq(reg, r15);
+  }
+
+  inline void emit_repmovs_presandbox() {
+    assembler_->movl(rsi, rsi);
+    assembler_->lea(rsi, Operand(r15, rsi, times_1, 0));
+  }
+
+  inline void emit_repmovs_sandbox() {
+    /* repmovs expects sandbox with lea for some reason */
+    assembler_->no_sbx_movl(rdi, rdi);
+    assembler_->no_sbx_lea(rdi, Operand(r15, rdi, times_1, 0));
+#if 0
+    assembler_->no_sbx_movl(rsi, rsi);
+    assembler_->no_sbx_addq(rsi, r15);
+    assembler_->no_sbx_movl(rdi, rdi);
+    assembler_->no_sbx_addq(rdi, r15);
+#endif
+  }
+
+  inline bool op_needs_unwrapping(Operand* op) {
+    if (is_trusted_reg(op->get_base()))
+      return false;
+    return true;
+  }
+
+  inline int sizeof_addq() {
+    static const int addq = 3;
+    return addq;
+  }
+
+  inline int sizeof_lea() {
+    // lea used to sandbox rep movs instructions
+    // and we always sandbox rdi register, therefore
+    // the size is constant
+    static const int lea = 4;
+    return lea;
+  }
+
+  inline int sizeof_movl(Register reg) {
+    static const int movl = 2;
+    // operand sandboxing involves movl(index, index)
+    return movl + (reg.high_bit() ? 1 : 0);
+  }
+
+  inline int sizeof_andl(Register reg, Immediate imm) {
+    int size = reg.high_bit() ? 1 : 0;
+
+    if (imm.get_size_8_32() == 1)
+      return size + 3;
+    else if (reg.is(rax))
+      return size + 1 + sizeof(uint32_t);
+    else
+      return size + 2 + sizeof(uint32_t);
+  }
+
+  inline bool is_trusted_reg(Register reg) {
+    if (reg.is(rsp) || reg.is(rbp) || reg.is(r15))
+      return true;
+    return false;
+  }
+
+  inline bool reg_needs_sandbox(Register reg) {
+    ASSERT(!reg.is(r15)); // r15 is never a subject to sandbox
+    return is_trusted_reg(reg);
+  }
+
+  inline bool op_needs_sandbox(Operand* op) {
+    // opernad must be unwrapped by now
+    ASSERT(is_trusted_reg(op->get_base()));
+    if (op->get_index().is_valid())
+      return true;
+    else
+      return false;
+  }
+
+#ifdef DEBUG
+  void check_register_use();
+  void check_reg(Register reg);
+  void check_op(Operand* op);
+#endif
+
+#ifdef DEBUG
+  char *fname_;
+  char *file_;
+  int line_;
+  int space_before_;
+#endif
+
+  Assembler *assembler_;
+  int size_;
+  int flags_;
+  SandboxMode mode_;
+  SandboxTarget left_target_;
+  SandboxTarget right_target_;
+  int last_pc_;
+};
+
+
+} }  // namespace v8::internal
+
+#endif  // V8_X64_NACL_SANDBOX_X64_H_
Index: src/virtual-frame-light-inl.h
===================================================================
--- src/virtual-frame-light-inl.h	(revision 4925)
+++ src/virtual-frame-light-inl.h	(working copy)
@@ -145,7 +145,7 @@
 int VirtualFrame::fp_relative(int index) {
   ASSERT(index < element_count());
   ASSERT(frame_pointer() < element_count());  // FP is on the frame.
-  return (frame_pointer() - index) * kPointerSize;
+  return (frame_pointer() - index) * kStackPointerSize;
 }
 
 
@@ -162,7 +162,7 @@
 MemOperand VirtualFrame::LocalAt(int index) {
   ASSERT(0 <= index);
   ASSERT(index < local_count());
-  return MemOperand(fp, kLocal0Offset - index * kPointerSize);
+  return MemOperand(fp, kLocal0Offset - index * kStackPointerSize);
 }
 
 } }  // namespace v8::internal
Index: src/naclcode.cc
===================================================================
--- src/naclcode.cc	(revision 0)
+++ src/naclcode.cc	(revision 0)
@@ -0,0 +1,354 @@
+// Copyright 2006-2010 the V8 project authors. All rights reserved.
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are
+// met:
+//
+//     * Redistributions of source code must retain the above copyright
+//       notice, this list of conditions and the following disclaimer.
+//     * Redistributions in binary form must reproduce the above
+//       copyright notice, this list of conditions and the following
+//       disclaimer in the documentation and/or other materials provided
+//       with the distribution.
+//     * Neither the name of Google Inc. nor the names of its
+//       contributors may be used to endorse or promote products derived
+//       from this software without specific prior written permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+// "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+#include "v8.h"
+#include "naclcode.h"
+#include "utils.h"
+#include "platform.h"
+#include <sys/mman.h>
+#include <map>
+#include <list>
+#include <vector>
+
+#if 0
+class MiniTimer {
+public:
+  MiniTimer(const char* name) : name_(name) {
+    total_ = -rdtsc();
+  }
+  ~MiniTimer() {
+    total_ += rdtsc();
+    printf("TIMER: %s %.1f%% of time (%.0f/%.0f) \n", name_, 100.0*v_/(double)total_, (double)v_, (double)total_);
+  }
+  static int64_t rdtsc() {
+    union {
+      uint64_t v;
+      struct { uint32_t lo, hi; } b;
+    } u;
+    asm volatile ("rdtsc" : "=a" (u.b.lo), "=d"(u.b.hi));
+    return u.v;
+  }
+
+  void start() { v_ -= rdtsc(); }
+  void stop() { v_ += rdtsc(); }
+
+  const char* name_;
+  int64_t v_;
+  int64_t total_;
+};
+#else
+class MiniTimer {
+public:
+  MiniTimer(const char*){}
+  void start() {}
+  void stop() {}
+};
+#endif
+
+static MiniTimer TSYSCALL("NaCl Syscall");
+static MiniTimer TPATCH("NaCl Patching");
+
+#ifdef NACLPOSIX
+#undef NACL
+#endif
+
+#ifdef NACL
+#include <sys/nacl_syscalls.h>
+
+#ifndef MAP_NORESERVE
+#define MAP_NORESERVE 0
+#endif
+
+// evil constants taken from dynamic_load_test.c
+#define DYNAMIC_CODE_SEGMENT_START 0x600000
+#define DYNAMIC_CODE_SEGMENT_END 0x2000000
+
+#endif //NACL
+
+namespace v8 {
+namespace internal { 
+
+  /*
+#ifdef NACL
+inline void memmove32(uint8_t* dst, uint8_t* src) {
+  //use sse registers
+  asm ( "movdqu (%0),   %%xmm0\n"
+        "movdqu 0x10(%0), %%xmm1\n"
+        "movdqu %%xmm0, (%1)\n"
+        "movdqu %%xmm1, 0x10(%1)\n"
+       : : "r"(src), "r"(dst)
+       : "xmm0", "xmm1", "memory" );
+}
+inline void memmove16(uint8_t* dst, uint8_t* src) {
+  //use sse registers
+  asm ( "movdqu (%0),   %%xmm0\n"
+        "movdqu %%xmm0, (%1)\n"
+       : : "r"(src), "r"(dst)
+       : "xmm0", "memory" );
+}
+#endif
+inline void memmove4(uint8_t* dst, uint8_t* src) {
+  *reinterpret_cast<uint32_t*>(dst) = *reinterpret_cast<uint32_t*>(src);
+}*/
+  
+const size_t kMaxCodeObjs = 128*1024;
+NaClCode::Tag kNaclDebugTag  = reinterpret_cast<NaClCode::Tag>(0xbad4ac1);
+
+#ifdef NACL
+const size_t kCodeBufSize = DYNAMIC_CODE_SEGMENT_END - DYNAMIC_CODE_SEGMENT_START;
+#else
+const size_t kCodeBufSize = 128*1024*1024;
+#endif
+
+#ifdef NACLSIM
+const size_t kCodeAlignment = 4096;
+int kCodeHeapProt = PROT_NONE;
+#else
+static const size_t kCodeAlignment = 32;
+static const int kCodeHeapProt = PROT_EXEC|PROT_READ|PROT_WRITE;
+#endif
+
+#if defined(NACL)
+#define NACLDELETE
+#endif
+
+class NaClCodeHeap {
+  struct SearchEntry {
+    uint8_t* inst_;
+    NaClCode::Tag code_;
+    SearchEntry(uint8_t* i = 0, NaClCode::Tag c = 0) : inst_(i), code_(c) {}
+  };
+public:
+  NaClCodeHeap(){
+#ifdef NACL
+    codeheap_ = reinterpret_cast<uint8_t*>(DYNAMIC_CODE_SEGMENT_START);
+    end_      = reinterpret_cast<uint8_t*>(DYNAMIC_CODE_SEGMENT_END);
+    freeptr_  = static_cast<uint8_t*>(codeheap_);
+    n_ = 0;
+#else
+    //use a big code buffer to mimic what we have to do in nacl
+    codeheap_ = mmap(0,
+                     kCodeBufSize,
+                     kCodeHeapProt,
+                     MAP_PRIVATE|MAP_ANONYMOUS|MAP_NORESERVE,
+                     -1,
+                     0);
+    freeptr_ = static_cast<uint8_t*>(codeheap_);
+    end_ = freeptr_ + kCodeBufSize;
+    n_ = 0;
+#endif//NACL
+  }
+
+  uint8_t* alloc(size_t bytes, NaClCode::Tag code) {
+#ifdef NACLDELETE
+    SearchEntryMap::iterator rb = freemap_.find(bytes);
+    if(rb != freemap_.end() && !rb->second.empty()) {
+      //reuse a recently deleted spot
+      SearchEntry* i = rb->second.back();
+      rb->second.pop_back();
+      i->code_ = code;
+      return i->inst_;
+    }
+#endif
+    //linear allocate a block of memory 
+    uint8_t* rv = freeptr_;
+    freeptr_+=bytes;
+    ASSERT(freeptr_<=end_);
+
+    //install search entry
+    ASSERT((unsigned int)n_<kMaxCodeObjs);
+    db_[n_++] = SearchEntry(rv, code);
+
+    return rv;
+  }
+
+
+  void dealloc(uint8_t* ptr, size_t bytes) {
+#ifdef NACLDELETE
+    /*
+     * We use an extremely simple allocation scheme for code reuse...
+     */
+    SearchEntry* entry = search(ptr, 0, n_-1);
+    ASSERT(entry->inst_ == ptr);
+    entry->code_ = NULL;
+    int rv = nacl_dyncode_delete(ptr, bytes);
+    CHECK(rv==0);
+    freemap_[bytes].push_back(entry);
+#endif
+  }
+
+  //binary search the code heap
+  NaClCode::Tag search(uint8_t* p) {
+    if(!contains(p)){
+      return NULL;
+    }
+    return search(p, 0, n_-1)->code_;
+  }
+  
+  bool contains(uint8_t* p) {
+    return !(p<codeheap_ || p>=freeptr_ || n_==0);
+  }
+
+
+private:
+  //binary search the code heap
+  SearchEntry* search(uint8_t* p, int begin, int end) {
+    if(begin==end){
+      return db_+begin;
+    }
+    int mid=(begin+end+1)/2;
+    if(db_[mid].inst_ <= p){
+      return search(p, mid, end);
+    }else{
+      return search(p, begin, mid-1);
+    }
+  }
+
+private:
+  void* codeheap_;
+  uint8_t* freeptr_;
+  const uint8_t* end_;
+  int n_;
+  SearchEntry db_[kMaxCodeObjs];
+
+#ifdef NACLDELETE
+  typedef std::vector<SearchEntry*> SearchEntryList;
+  typedef std::map<size_t, SearchEntryList> SearchEntryMap;
+  SearchEntryMap freemap_;
+#endif
+
+} theNaClHeap;
+
+
+uint8_t* NaClCode::Allocate(size_t bytes, NaClCode::Tag backpointer) {
+#ifdef NACL_USE_BACKPOINTER
+  const int extra = kCodeAlignment;
+#else
+  const int extra = 0;
+#endif
+
+  //extra block for backpointer
+  uint8_t* rv = theNaClHeap.alloc(RoundUp(bytes, kCodeAlignment)+extra,
+                                  backpointer);
+
+#ifdef NACL_USE_BACKPOINTER
+  //install backpointer
+  rv+=kCodeAlignment;
+  reinterpret_cast<NaClCode::Tag*>(rv)[-1] = backpointer;
+#ifdef DEBUG
+  reinterpret_cast<NaClCode::Tag*>(rv)[-2] = kNaclDebugTag;
+#endif
+#endif
+
+  return rv;
+}
+
+void NaClCode::Deallocate(uint8_t* ptr, size_t bytes) {
+  theNaClHeap.dealloc(ptr, RoundUp(bytes, kCodeAlignment));
+}
+
+#ifdef DEBUG
+void CodeDump(const char *name, uint8_t *src, size_t bytes) {
+  FILE *f;
+  uint8_t *p = src;
+  uint8_t *q = src + bytes;
+  f = fopen(name, "a");
+  if (f == NULL) return;
+
+  while (p < q) {
+    fwrite(p, 1, 1, f);
+    p++;
+  }
+
+  fclose(f);
+}
+#endif
+  
+void NaClCode::Install(uint8_t* dst, uint8_t* src, size_t bytes){
+  ASSERT(IsProtectedCode(dst) && !IsProtectedCode(src));
+
+#if defined(NACL)
+  TSYSCALL.start();
+  int rc = nacl_dyncode_create(dst, src, bytes);
+  TSYSCALL.stop();
+  if(rc != 0) {
+    fprintf(stderr, "V8/NaCl - Dynamic Creation Failure:\n\tsrc: %p\n\tdst: %p\n\tsize: %d\n\tdebugoffset: +0x%x\n\tcode object: %8p\n",
+        src, dst, bytes, src-dst, (void*)Search(dst));
+    OS::DebugBreak();
+  }
+#elif defined(NACLSIM)
+  CHECK(mprotect(dst, RoundUp(bytes, kCodeAlignment), PROT_READ|PROT_WRITE)==0);
+  memmove(dst, src, bytes);
+  CHECK(mprotect(dst, RoundUp(bytes, kCodeAlignment), PROT_READ|PROT_EXEC)==0);
+#else
+  memmove(dst, src, bytes);
+#endif
+}
+  
+NaClCode::Tag NaClCode::GetBackpointer(uint8_t* inst) {
+#ifdef NACL_USE_BACKPOINTER
+  ASSERT(reinterpret_cast<NaClCode::Tag*>(inst)[-2] == kNaclDebugTag);
+  return reinterpret_cast<NaClCode::Tag*>(inst)[-1];
+#else
+  return Search(inst);
+#endif
+}
+  
+NaClCode::Tag NaClCode::Search(uint8_t* inst) {
+  return theNaClHeap.search(inst);
+}
+  
+bool NaClCode::IsProtectedCode(uint8_t* dst) {
+  return theNaClHeap.contains(dst);
+}
+
+// PMARCH  
+#define NACL_CHUNK 32
+ 
+void NaClCode::Modify(uint8_t* dst, uint8_t* src, size_t bytes) {
+#ifdef NACL
+  ASSERT(NaClCode::IsProtectedCode(dst));
+
+  int rc = nacl_dyncode_modify(dst, src, bytes);
+  TPATCH.stop();
+  if(rc != 0) {
+    fprintf(stderr, "V8/NaCl - Dynamic Replacement Failure:\n\tsrc: %p\n\tdst: %p\n\tsize: %d\n\tdebugoffset: +0x%x\n\tcode object: %8p\n",
+        src, dst, bytes, src-dst, (void*)Search(dst));
+    OS::DebugBreak();
+  }
+#else
+  memcpy(dst, src, bytes);
+#endif
+}
+
+}} //v8::internal
+
+
+
+
+
+
Index: src/objects-inl.h
===================================================================
--- src/objects-inl.h	(revision 4925)
+++ src/objects-inl.h	(working copy)
@@ -39,6 +39,7 @@
 #include "contexts.h"
 #include "conversions-inl.h"
 #include "property.h"
+#include "naclcode.h"
 
 namespace v8 {
 namespace internal {
@@ -897,7 +898,7 @@
   bool in_range = (value >= kMinValue) && (value <= kMaxValue);
 #endif
 
-#ifdef V8_TARGET_ARCH_X64
+#if defined(V8_TARGET_ARCH_X64) && !defined(NACL)
   // To be representable as a long smi, the value must be a 32-bit integer.
   bool result = (value == static_cast<int32_t>(value));
 #else
@@ -1166,6 +1167,8 @@
 
 
 void JSObject::set_elements(HeapObject* value, WriteBarrierMode mode) {
+  ASSERT(map()->has_fast_elements() ==
+         (value->map() == Heap::fixed_array_map()));
   // In the assert below Dictionary is covered under FixedArray.
   ASSERT(value->IsFixedArray() || value->IsPixelArray() ||
          value->IsExternalArray());
@@ -1181,11 +1184,21 @@
 
 
 void JSObject::initialize_elements() {
+  ASSERT(map()->has_fast_elements());
   ASSERT(!Heap::InNewSpace(Heap::empty_fixed_array()));
   WRITE_FIELD(this, kElementsOffset, Heap::empty_fixed_array());
 }
 
 
+Object* JSObject::ResetElements() {
+  Object* obj = map()->GetFastElementsMap();
+  if (obj->IsFailure()) return obj;
+  set_map(Map::cast(obj));
+  initialize_elements();
+  return this;
+}
+
+
 ACCESSORS(Oddball, to_string, String, kToStringOffset)
 ACCESSORS(Oddball, to_number, Object, kToNumberOffset)
 
@@ -1676,7 +1689,7 @@
 
 void String::set_hash_field(uint32_t value) {
   WRITE_UINT32_FIELD(this, kHashFieldOffset, value);
-#if V8_HOST_ARCH_64_BIT
+#if defined(V8_HOST_ARCH_64_BIT) && !defined(NACL)
   WRITE_UINT32_FIELD(this, kHashFieldOffset + kIntSize, 0);
 #endif
 }
@@ -2311,15 +2324,20 @@
   return static_cast<Flags>(bits);
 }
 
-
 Code* Code::GetCodeFromTargetAddress(Address address) {
+#if NACL        
+  //NACL_CHANGE:
+  return NaClCode::GetBackpointer(address);
+#else
   HeapObject* code = HeapObject::FromAddress(address - Code::kHeaderSize);
+
   // GetCodeFromTargetAddress might be called when marking objects during mark
   // sweep. reinterpret_cast is therefore used instead of the more appropriate
   // Code::cast. Code::cast does not work when the object's map is
   // marked.
   Code* result = reinterpret_cast<Code*>(code);
   return result;
+#endif
 }
 
 
@@ -2335,6 +2353,26 @@
 }
 
 
+Object* Map::GetFastElementsMap() {
+  if (has_fast_elements()) return this;
+  Object* obj = CopyDropTransitions();
+  if (obj->IsFailure()) return obj;
+  Map* new_map = Map::cast(obj);
+  new_map->set_has_fast_elements(true);
+  return new_map;
+}
+
+
+Object* Map::GetSlowElementsMap() {
+  if (!has_fast_elements()) return this;
+  Object* obj = CopyDropTransitions();
+  if (obj->IsFailure()) return obj;
+  Map* new_map = Map::cast(obj);
+  new_map->set_has_fast_elements(false);
+  return new_map;
+}
+
+
 ACCESSORS(Map, instance_descriptors, DescriptorArray,
           kInstanceDescriptorsOffset)
 ACCESSORS(Map, code_cache, Object, kCodeCacheOffset)
@@ -2745,19 +2783,46 @@
 INT_ACCESSORS(Code, relocation_size, kRelocationSizeOffset)
 INT_ACCESSORS(Code, sinfo_size, kSInfoSizeOffset)
 
+#ifdef NACL
+//NACL_CHANGE: add external_instructions field:
+byte* Code::external_instructions() {
+  intptr_t ptr = READ_INTPTR_FIELD(this, kExternalInstructionsOffset);
+  return reinterpret_cast<byte*>(ptr);
+}
 
+//NACL_CHANGE: add external_instructions field:
+void Code::set_external_instructions(byte* value) { 
+  intptr_t ptr = reinterpret_cast<intptr_t>(value);
+  WRITE_INTPTR_FIELD(this, kExternalInstructionsOffset, ptr);
+}
+#endif
+
 byte* Code::instruction_start()  {
+#ifdef NACL        
+//NACL_CHANGE: split instructions to separate object
+  return external_instructions();
+#else  
   return FIELD_ADDR(this, kHeaderSize);
+#endif  
 }
 
-
 int Code::body_size() {
+#ifdef NACL        
+//NACL_CHANGE: dont include instructions in body size
+  return RoundUp(relocation_size(), kObjectAlignment);
+#else  
   return RoundUp(instruction_size() + relocation_size(), kObjectAlignment);
+#endif  
 }
 
 
 byte* Code::relocation_start() {
+#ifdef NACL        
+  //NACL_CHANGE: no longer place this after instructions
+  return FIELD_ADDR(this, kHeaderSize);
+#else  
   return FIELD_ADDR(this, kHeaderSize + instruction_size());
+#endif 
 }
 
 
@@ -2838,11 +2903,14 @@
   if (array->IsFixedArray()) {
     // FAST_ELEMENTS or DICTIONARY_ELEMENTS are both stored in a FixedArray.
     if (array->map() == Heap::fixed_array_map()) {
+      ASSERT(map()->has_fast_elements());
       return FAST_ELEMENTS;
     }
     ASSERT(array->IsDictionary());
+    ASSERT(!map()->has_fast_elements());
     return DICTIONARY_ELEMENTS;
   }
+  ASSERT(!map()->has_fast_elements());
   if (array->IsExternalArray()) {
     switch (array->map()->instance_type()) {
       case EXTERNAL_BYTE_ARRAY_TYPE:
Index: src/assembler.cc
===================================================================
--- src/assembler.cc	(revision 4925)
+++ src/assembler.cc	(working copy)
@@ -477,7 +477,9 @@
            *target_reference_address());
   } else if (IsCodeTarget(rmode_)) {
     Code* code = Code::GetCodeFromTargetAddress(target_address());
-    PrintF(" (%s)  (%p)", Code::Kind2String(code->kind()), target_address());
+    if(code!=0) {
+      PrintF(" (%s)  (%p)", Code::Kind2String(code->kind()), target_address());
+    }
   } else if (IsPosition(rmode_)) {
     PrintF("  (%d)", data());
   }
@@ -506,9 +508,14 @@
       ASSERT(addr != NULL);
       // Check that we can find the right code object.
       Code* code = Code::GetCodeFromTargetAddress(addr);
+#ifndef NACL
+      //NACL_CHANGE: heap search is broken for code (which is not in heap anymore)
+      //             so dont test it
       Object* found = Heap::FindCodeObject(addr);
       ASSERT(found->IsCode());
       ASSERT(code->address() == HeapObject::cast(found)->address());
+#endif      
+      USE(code);
       break;
     }
     case RUNTIME_ENTRY:
Index: src/mark-compact.cc
===================================================================
--- src/mark-compact.cc	(revision 4925)
+++ src/mark-compact.cc	(working copy)
@@ -2312,13 +2312,20 @@
 
 
 void MarkCompactCollector::ReportDeleteIfNeeded(HeapObject* obj) {
-#ifdef ENABLE_LOGGING_AND_PROFILING
+#if defined(ENABLE_LOGGING_AND_PROFILING) || defined(NACL)
   if (obj->IsCode()) {
+#ifdef NACL
+    Code::cast(obj)->NaClOnDelete();
+#endif
+#if !defined(ENABLE_LOGGING_AND_PROFILING)
+  }
+#else
     PROFILE(CodeDeleteEvent(obj->address()));
   } else if (obj->IsJSFunction()) {
     PROFILE(FunctionDeleteEvent(obj->address()));
   }
 #endif
+#endif
 }
 
 } }  // namespace v8::internal
Index: src/log.cc
===================================================================
--- src/log.cc	(revision 4925)
+++ src/log.cc	(working copy)
@@ -309,10 +309,10 @@
 
 void Profiler::Run() {
   TickSample sample;
-  bool overflow = Logger::profiler_->Remove(&sample);
+  bool overflow = Remove(&sample);
   while (running_) {
     LOG(TickEvent(&sample, overflow));
-    overflow = Logger::profiler_->Remove(&sample);
+    overflow = Remove(&sample);
   }
 }
 
@@ -1150,7 +1150,7 @@
 
 int Logger::GetActiveProfilerModules() {
   int result = PROFILER_MODULE_NONE;
-  if (!profiler_->paused()) {
+  if (profiler_ != NULL && !profiler_->paused()) {
     result |= PROFILER_MODULE_CPU;
   }
   if (FLAG_log_gc) {
@@ -1162,7 +1162,7 @@
 
 void Logger::PauseProfiler(int flags, int tag) {
   if (!Log::IsEnabled()) return;
-  if (flags & PROFILER_MODULE_CPU) {
+  if (profiler_ != NULL && (flags & PROFILER_MODULE_CPU)) {
     // It is OK to have negative nesting.
     if (--cpu_profiler_nesting_ == 0) {
       profiler_->pause();
@@ -1193,7 +1193,7 @@
   if (tag != 0) {
     UncheckedIntEvent("open-tag", tag);
   }
-  if (flags & PROFILER_MODULE_CPU) {
+  if (profiler_ != NULL && (flags & PROFILER_MODULE_CPU)) {
     if (cpu_profiler_nesting_++ == 0) {
       ++logging_nesting_;
       if (FLAG_prof_lazy) {
@@ -1475,6 +1475,7 @@
     }
   }
 
+
   ASSERT(VMState::is_outermost_external());
 
   ticker_ = new Ticker(kSamplingIntervalMs);
Index: src/platform-posix.cc
===================================================================
--- src/platform-posix.cc	(revision 4925)
+++ src/platform-posix.cc	(working copy)
@@ -33,14 +33,19 @@
 #include <errno.h>
 #include <time.h>
 
+#if !defined(NACL) || defined(NACLPOSIX)
 #include <sys/socket.h>
+#endif // !defined(NACL)
+
 #include <sys/resource.h>
 #include <sys/time.h>
 #include <sys/types.h>
 
+#if !defined(NACL) || defined(NACLPOSIX)
 #include <arpa/inet.h>
 #include <netinet/in.h>
 #include <netdb.h>
+#endif // !defined(NACL)
 
 #if defined(ANDROID)
 #define LOG_TAG "v8"
@@ -51,6 +56,10 @@
 
 #include "platform.h"
 
+#if defined(NACLPOSIX)
+#undef NACL //disable NACL exception in this file
+#endif
+
 namespace v8 {
 namespace internal {
 
@@ -72,6 +81,8 @@
 // POSIX date/time support.
 //
 
+#ifndef NACL
+
 int OS::GetUserTime(uint32_t* secs,  uint32_t* usecs) {
   struct rusage usage;
 
@@ -81,7 +92,9 @@
   return 0;
 }
 
+#endif // !defined(NACL)
 
+
 double OS::TimeCurrentMillis() {
   struct timeval tv;
   if (gettimeofday(&tv, NULL) < 0) return 0.0;
@@ -125,6 +138,7 @@
   va_start(args, format);
   VPrint(format, args);
   va_end(args);
+  fflush(stdout);
 }
 
 
@@ -133,6 +147,7 @@
   LOG_PRI_VA(ANDROID_LOG_INFO, LOG_TAG, format, args);
 #else
   vprintf(format, args);
+  fflush(stdout);
 #endif
 }
 
@@ -150,10 +165,13 @@
   LOG_PRI_VA(ANDROID_LOG_ERROR, LOG_TAG, format, args);
 #else
   vfprintf(stderr, format, args);
+  fflush(stdout);
 #endif
 }
 
 
+#ifndef NACL
+
 int OS::SNPrintF(Vector<char> str, const char* format, ...) {
   va_list args;
   va_start(args, format);
@@ -163,6 +181,7 @@
 }
 
 
+
 int OS::VSNPrintF(Vector<char> str,
                   const char* format,
                   va_list args) {
@@ -175,7 +194,9 @@
   }
 }
 
+#endif // !defined(NACL)
 
+
 // ----------------------------------------------------------------------------
 // POSIX string support.
 //
@@ -194,6 +215,8 @@
 // POSIX socket support.
 //
 
+#ifndef NACL
+
 class POSIXSocket : public Socket {
  public:
   explicit POSIXSocket() {
@@ -358,5 +381,6 @@
   return new POSIXSocket();
 }
 
+#endif // !defined(NACL)
 
 } }  // namespace v8::internal
Index: src/dtoa-config.c
===================================================================
--- src/dtoa-config.c	(revision 4925)
+++ src/dtoa-config.c	(working copy)
@@ -39,7 +39,7 @@
 
 #if !(defined(__APPLE__) && defined(__MACH__)) && \
     !defined(WIN32) && !defined(__FreeBSD__) && !defined(__OpenBSD__) && \
-    !defined(__sun)
+    !defined(__sun) && !defined(NACL)
 #include <endian.h>
 #endif
 #include <math.h>
Index: src/arm/assembler-arm.h
===================================================================
--- src/arm/assembler-arm.h	(revision 4925)
+++ src/arm/assembler-arm.h	(working copy)
@@ -1110,6 +1110,7 @@
   void EndBlockConstPool() {
     const_pool_blocked_nesting_--;
   }
+  bool is_const_pool_blocked() const { return const_pool_blocked_nesting_ > 0; }
 
  private:
   // Code buffer:
Index: src/arm/codegen-arm.cc
===================================================================
--- src/arm/codegen-arm.cc	(revision 4925)
+++ src/arm/codegen-arm.cc	(working copy)
@@ -157,6 +157,7 @@
       state_(NULL),
       loop_nesting_(0),
       type_info_(NULL),
+      function_return_(JumpTarget::BIDIRECTIONAL),
       function_return_is_shadowed_(false) {
 }
 
@@ -218,7 +219,7 @@
       // for stack overflow.
       frame_->AllocateStackSlots();
 
-      VirtualFrame::SpilledScope spilled_scope(frame_);
+      frame_->AssertIsSpilled();
       int heap_slots = scope()->num_heap_slots() - Context::MIN_CONTEXT_SLOTS;
       if (heap_slots > 0) {
         // Allocate local context.
@@ -257,6 +258,7 @@
         // order: such a parameter is copied repeatedly into the same
         // context location and thus the last value is what is seen inside
         // the function.
+        frame_->AssertIsSpilled();
         for (int i = 0; i < scope()->num_parameters(); i++) {
           Variable* par = scope()->parameter(i);
           Slot* slot = par->slot();
@@ -282,8 +284,7 @@
 
       // Initialize ThisFunction reference if present.
       if (scope()->is_function_scope() && scope()->function() != NULL) {
-        __ mov(ip, Operand(Factory::the_hole_value()));
-        frame_->EmitPush(ip);
+        frame_->EmitPushRoot(Heap::kTheHoleValueRootIndex);
         StoreToSlot(scope()->function()->slot(), NOT_CONST_INIT);
       }
     } else {
@@ -510,7 +511,6 @@
         has_valid_frame() &&
         !has_cc() &&
         frame_->height() == original_height) {
-      frame_->SpillAll();
       true_target->Jump();
     }
   }
@@ -535,22 +535,18 @@
 
   if (has_cc()) {
     // Convert cc_reg_ into a boolean value.
-    VirtualFrame::SpilledScope scope(frame_);
     JumpTarget loaded;
     JumpTarget materialize_true;
     materialize_true.Branch(cc_reg_);
-    __ LoadRoot(r0, Heap::kFalseValueRootIndex);
-    frame_->EmitPush(r0);
+    frame_->EmitPushRoot(Heap::kFalseValueRootIndex);
     loaded.Jump();
     materialize_true.Bind();
-    __ LoadRoot(r0, Heap::kTrueValueRootIndex);
-    frame_->EmitPush(r0);
+    frame_->EmitPushRoot(Heap::kTrueValueRootIndex);
     loaded.Bind();
     cc_reg_ = al;
   }
 
   if (true_target.is_linked() || false_target.is_linked()) {
-    VirtualFrame::SpilledScope scope(frame_);
     // We have at least one condition value that has been "translated"
     // into a branch, thus it needs to be loaded explicitly.
     JumpTarget loaded;
@@ -561,8 +557,7 @@
     // Load "true" if necessary.
     if (true_target.is_linked()) {
       true_target.Bind();
-      __ LoadRoot(r0, Heap::kTrueValueRootIndex);
-      frame_->EmitPush(r0);
+      frame_->EmitPushRoot(Heap::kTrueValueRootIndex);
     }
     // If both "true" and "false" need to be loaded jump across the code for
     // "false".
@@ -572,8 +567,7 @@
     // Load "false" if necessary.
     if (false_target.is_linked()) {
       false_target.Bind();
-      __ LoadRoot(r0, Heap::kFalseValueRootIndex);
-      frame_->EmitPush(r0);
+      frame_->EmitPushRoot(Heap::kFalseValueRootIndex);
     }
     // A value is loaded on all paths reaching this point.
     loaded.Bind();
@@ -592,11 +586,11 @@
 
 
 void CodeGenerator::LoadGlobalReceiver(Register scratch) {
-  VirtualFrame::SpilledScope spilled_scope(frame_);
-  __ ldr(scratch, ContextOperand(cp, Context::GLOBAL_INDEX));
-  __ ldr(scratch,
-         FieldMemOperand(scratch, GlobalObject::kGlobalReceiverOffset));
-  frame_->EmitPush(scratch);
+  Register reg = frame_->GetTOSRegister();
+  __ ldr(reg, ContextOperand(cp, Context::GLOBAL_INDEX));
+  __ ldr(reg,
+         FieldMemOperand(reg, GlobalObject::kGlobalReceiverOffset));
+  frame_->EmitPush(reg);
 }
 
 
@@ -613,8 +607,6 @@
 
 
 void CodeGenerator::StoreArgumentsObject(bool initial) {
-  VirtualFrame::SpilledScope spilled_scope(frame_);
-
   ArgumentsAllocationMode mode = ArgumentsMode();
   ASSERT(mode != NO_ARGUMENTS_ALLOCATION);
 
@@ -623,9 +615,9 @@
     // When using lazy arguments allocation, we store the hole value
     // as a sentinel indicating that the arguments object hasn't been
     // allocated yet.
-    __ LoadRoot(ip, Heap::kTheHoleValueRootIndex);
-    frame_->EmitPush(ip);
+    frame_->EmitPushRoot(Heap::kTheHoleValueRootIndex);
   } else {
+    frame_->SpillAll();
     ArgumentsAccessStub stub(ArgumentsAccessStub::NEW_OBJECT);
     __ ldr(r2, frame_->Function());
     // The receiver is below the arguments, the return address, and the
@@ -649,9 +641,9 @@
     // already been written to. This can happen if the a function
     // has a local variable named 'arguments'.
     LoadFromSlot(scope()->arguments()->var()->slot(), NOT_INSIDE_TYPEOF);
-    frame_->EmitPop(r0);
+    Register arguments = frame_->PopToRegister();
     __ LoadRoot(ip, Heap::kTheHoleValueRootIndex);
-    __ cmp(r0, ip);
+    __ cmp(arguments, ip);
     done.Branch(ne);
   }
   StoreToSlot(arguments->slot(), NOT_CONST_INIT);
@@ -754,36 +746,35 @@
 // may jump to 'false_target' in case the register converts to 'false'.
 void CodeGenerator::ToBoolean(JumpTarget* true_target,
                               JumpTarget* false_target) {
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   // Note: The generated code snippet does not change stack variables.
   //       Only the condition code should be set.
-  frame_->EmitPop(r0);
+  Register tos = frame_->PopToRegister();
 
   // Fast case checks
 
   // Check if the value is 'false'.
   __ LoadRoot(ip, Heap::kFalseValueRootIndex);
-  __ cmp(r0, ip);
+  __ cmp(tos, ip);
   false_target->Branch(eq);
 
   // Check if the value is 'true'.
   __ LoadRoot(ip, Heap::kTrueValueRootIndex);
-  __ cmp(r0, ip);
+  __ cmp(tos, ip);
   true_target->Branch(eq);
 
   // Check if the value is 'undefined'.
   __ LoadRoot(ip, Heap::kUndefinedValueRootIndex);
-  __ cmp(r0, ip);
+  __ cmp(tos, ip);
   false_target->Branch(eq);
 
   // Check if the value is a smi.
-  __ cmp(r0, Operand(Smi::FromInt(0)));
+  __ cmp(tos, Operand(Smi::FromInt(0)));
   false_target->Branch(eq);
-  __ tst(r0, Operand(kSmiTagMask));
+  __ tst(tos, Operand(kSmiTagMask));
   true_target->Branch(eq);
 
   // Slow case: call the runtime.
-  frame_->EmitPush(r0);
+  frame_->EmitPush(tos);
   frame_->CallRuntime(Runtime::kToBool, 1);
   // Convert the result (r0) to a condition code.
   __ LoadRoot(ip, Heap::kFalseValueRootIndex);
@@ -935,7 +926,15 @@
 };
 
 
+
+// On entry the non-constant side of the binary operation is in tos_register_
+// and the constant smi side is nowhere.  The tos_register_ is not used by the
+// virtual frame.  On exit the answer is in the tos_register_ and the virtual
+// frame is unchanged.
 void DeferredInlineSmiOperation::Generate() {
+  VirtualFrame copied_frame(*frame_state()->frame());
+  copied_frame.SpillAll();
+
   Register lhs = r1;
   Register rhs = r0;
   switch (op_) {
@@ -969,45 +968,20 @@
     case Token::MOD:
     case Token::BIT_OR:
     case Token::BIT_XOR:
-    case Token::BIT_AND: {
-      if (reversed_) {
-        if (tos_register_.is(r0)) {
-          __ mov(r1, Operand(Smi::FromInt(value_)));
-        } else {
-          ASSERT(tos_register_.is(r1));
-          __ mov(r0, Operand(Smi::FromInt(value_)));
-          lhs = r0;
-          rhs = r1;
-        }
-      } else {
-        if (tos_register_.is(r1)) {
-          __ mov(r0, Operand(Smi::FromInt(value_)));
-        } else {
-          ASSERT(tos_register_.is(r0));
-          __ mov(r1, Operand(Smi::FromInt(value_)));
-          lhs = r0;
-          rhs = r1;
-        }
-      }
-      break;
-    }
-
+    case Token::BIT_AND:
     case Token::SHL:
     case Token::SHR:
     case Token::SAR: {
-      if (!reversed_) {
-        if (tos_register_.is(r1)) {
-          __ mov(r0, Operand(Smi::FromInt(value_)));
-        } else {
-          ASSERT(tos_register_.is(r0));
-          __ mov(r1, Operand(Smi::FromInt(value_)));
-          lhs = r0;
-          rhs = r1;
-        }
+      if (tos_register_.is(r1)) {
+        __ mov(r0, Operand(Smi::FromInt(value_)));
       } else {
-        ASSERT(op_ == Token::SHL);
+        ASSERT(tos_register_.is(r0));
         __ mov(r1, Operand(Smi::FromInt(value_)));
       }
+      if (reversed_ == tos_register_.is(r1)) {
+          lhs = r0;
+          rhs = r1;
+      }
       break;
     }
 
@@ -1019,11 +993,17 @@
 
   GenericBinaryOpStub stub(op_, overwrite_mode_, lhs, rhs, value_);
   __ CallStub(&stub);
+
   // The generic stub returns its value in r0, but that's not
   // necessarily what we want.  We want whatever the inlined code
   // expected, which is that the answer is in the same register as
   // the operand was.
   __ Move(tos_register_, r0);
+
+  // The tos register was not in use for the virtual frame that we
+  // came into this function with, so we can merge back to that frame
+  // without trashing it.
+  copied_frame.MergeTo(frame_state()->frame());
 }
 
 
@@ -1124,12 +1104,6 @@
 
   // We move the top of stack to a register (normally no move is invoved).
   Register tos = frame_->PopToRegister();
-  // All other registers are spilled.  The deferred code expects one argument
-  // in a register and all other values are flushed to the stack.  The
-  // answer is returned in the same register that the top of stack argument was
-  // in.
-  frame_->SpillAll();
-
   switch (op) {
     case Token::ADD: {
       DeferredCode* deferred =
@@ -1448,8 +1422,6 @@
 void CodeGenerator::CallWithArguments(ZoneList<Expression*>* args,
                                       CallFunctionFlags flags,
                                       int position) {
-  frame_->AssertIsSpilled();
-
   // Push the arguments ("left-to-right") on the stack.
   int arg_count = args->length();
   for (int i = 0; i < arg_count; i++) {
@@ -1482,7 +1454,6 @@
   // stack, as receiver and arguments, and calls x.
   // In the implementation comments, we call x the applicand
   // and y the receiver.
-  VirtualFrame::SpilledScope spilled_scope(frame_);
 
   ASSERT(ArgumentsMode() == LAZY_ARGUMENTS_ALLOCATION);
   ASSERT(arguments->IsArguments());
@@ -1500,6 +1471,15 @@
   Load(receiver);
   LoadFromSlot(scope()->arguments()->var()->slot(), NOT_INSIDE_TYPEOF);
 
+  // At this point the top two stack elements are probably in registers
+  // since they were just loaded.  Ensure they are in regs and get the
+  // regs.
+  Register receiver_reg = frame_->Peek2();
+  Register arguments_reg = frame_->Peek();
+
+  // From now on the frame is spilled.
+  frame_->SpillAll();
+
   // Emit the source position information after having loaded the
   // receiver and the arguments.
   CodeForSourcePosition(position);
@@ -1513,32 +1493,30 @@
   // already. If so, just use that instead of copying the arguments
   // from the stack. This also deals with cases where a local variable
   // named 'arguments' has been introduced.
-  __ ldr(r0, MemOperand(sp, 0));
-
-  Label slow, done;
+  JumpTarget slow;
+  Label done;
   __ LoadRoot(ip, Heap::kTheHoleValueRootIndex);
-  __ cmp(ip, r0);
-  __ b(ne, &slow);
+  __ cmp(ip, arguments_reg);
+  slow.Branch(ne);
 
   Label build_args;
   // Get rid of the arguments object probe.
   frame_->Drop();
   // Stack now has 3 elements on it.
   // Contents of stack at this point:
-  //   sp[0]: receiver
+  //   sp[0]: receiver - in the receiver_reg register.
   //   sp[1]: applicand.apply
   //   sp[2]: applicand.
 
   // Check that the receiver really is a JavaScript object.
-  __ ldr(r0, MemOperand(sp, 0));
-  __ BranchOnSmi(r0, &build_args);
+  __ BranchOnSmi(receiver_reg, &build_args);
   // We allow all JSObjects including JSFunctions.  As long as
   // JS_FUNCTION_TYPE is the last instance type and it is right
   // after LAST_JS_OBJECT_TYPE, we do not have to check the upper
   // bound.
   ASSERT(LAST_TYPE == JS_FUNCTION_TYPE);
   ASSERT(JS_FUNCTION_TYPE == LAST_JS_OBJECT_TYPE + 1);
-  __ CompareObjectType(r0, r1, r2, FIRST_JS_OBJECT_TYPE);
+  __ CompareObjectType(receiver_reg, r2, r3, FIRST_JS_OBJECT_TYPE);
   __ b(lt, &build_args);
 
   // Check that applicand.apply is Function.prototype.apply.
@@ -1627,7 +1605,7 @@
   StoreArgumentsObject(false);
 
   // Stack and frame now have 4 elements.
-  __ bind(&slow);
+  slow.Bind();
 
   // Generic computation of x.apply(y, args) with no special optimization.
   // Flip applicand.apply and applicand on the stack, so
@@ -1652,7 +1630,6 @@
 
 
 void CodeGenerator::Branch(bool if_true, JumpTarget* target) {
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   ASSERT(has_cc());
   Condition cc = if_true ? cc_reg_ : NegateCondition(cc_reg_);
   target->Branch(cc);
@@ -1661,7 +1638,7 @@
 
 
 void CodeGenerator::CheckStack() {
-  VirtualFrame::SpilledScope spilled_scope(frame_);
+  frame_->SpillAll();
   Comment cmnt(masm_, "[ check stack");
   __ LoadRoot(ip, Heap::kStackLimitRootIndex);
   // Put the lr setup instruction in the delay slot.  kInstrSize is added to
@@ -1683,7 +1660,6 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   for (int i = 0; frame_ != NULL && i < statements->length(); i++) {
     Visit(statements->at(i));
   }
@@ -1695,7 +1671,6 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ Block");
   CodeForStatementPosition(node);
   node->break_target()->SetExpectedHeight();
@@ -1713,7 +1688,6 @@
   frame_->EmitPush(Operand(pairs));
   frame_->EmitPush(Operand(Smi::FromInt(is_eval() ? 1 : 0)));
 
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   frame_->CallRuntime(Runtime::kDeclareGlobals, 3);
   // The result is discarded.
 }
@@ -1754,7 +1728,6 @@
       frame_->EmitPush(Operand(0));
     }
 
-    VirtualFrame::SpilledScope spilled_scope(frame_);
     frame_->CallRuntime(Runtime::kDeclareContextSlot, 4);
     // Ignore the return value (declarations are statements).
 
@@ -1899,7 +1872,6 @@
 
 
 void CodeGenerator::VisitContinueStatement(ContinueStatement* node) {
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ ContinueStatement");
   CodeForStatementPosition(node);
   node->target()->continue_target()->Jump();
@@ -1907,7 +1879,6 @@
 
 
 void CodeGenerator::VisitBreakStatement(BreakStatement* node) {
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ BreakStatement");
   CodeForStatementPosition(node);
   node->target()->break_target()->Jump();
@@ -1915,7 +1886,7 @@
 
 
 void CodeGenerator::VisitReturnStatement(ReturnStatement* node) {
-  VirtualFrame::SpilledScope spilled_scope(frame_);
+  frame_->SpillAll();
   Comment cmnt(masm_, "[ ReturnStatement");
 
   CodeForStatementPosition(node);
@@ -1926,7 +1897,7 @@
   } else {
     // Pop the result from the frame and prepare the frame for
     // returning thus making it easier to merge.
-    frame_->EmitPop(r0);
+    frame_->PopToR0();
     frame_->PrepareForReturn();
     if (function_return_.is_bound()) {
       // If the function return label is already bound we reuse the
@@ -1986,7 +1957,6 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ WithEnterStatement");
   CodeForStatementPosition(node);
   Load(node->expression());
@@ -2012,7 +1982,6 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ WithExitStatement");
   CodeForStatementPosition(node);
   // Pop context.
@@ -2027,7 +1996,6 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ SwitchStatement");
   CodeForStatementPosition(node);
   node->break_target()->SetExpectedHeight();
@@ -2055,8 +2023,7 @@
     next_test.Bind();
     next_test.Unuse();
     // Duplicate TOS.
-    __ ldr(r0, frame_->Top());
-    frame_->EmitPush(r0);
+    frame_->Dup();
     Comparison(eq, NULL, clause->label(), true);
     Branch(false, &next_test);
 
@@ -2094,7 +2061,7 @@
     default_entry.Bind();
     VisitStatements(default_clause->statements());
     // If control flow can fall out of the default and there is a case after
-    // it, jup to that case's body.
+    // it, jump to that case's body.
     if (frame_ != NULL && default_exit.is_bound()) {
       default_exit.Jump();
     }
@@ -2116,7 +2083,6 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ DoWhileStatement");
   CodeForStatementPosition(node);
   node->break_target()->SetExpectedHeight();
@@ -2191,7 +2157,6 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ WhileStatement");
   CodeForStatementPosition(node);
 
@@ -2209,7 +2174,7 @@
   node->continue_target()->Bind();
 
   if (info == DONT_KNOW) {
-    JumpTarget body;
+    JumpTarget body(JumpTarget::BIDIRECTIONAL);
     LoadCondition(node->cond(), &body, node->break_target(), true);
     if (has_valid_frame()) {
       // A NULL frame indicates that control did not fall out of the
@@ -2242,7 +2207,6 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ ForStatement");
   CodeForStatementPosition(node);
   if (node->init() != NULL) {
@@ -2931,7 +2895,6 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ Conditional");
   JumpTarget then;
   JumpTarget else_;
@@ -2972,10 +2935,8 @@
                                     &done);
 
     slow.Bind();
-    VirtualFrame::SpilledScope spilled_scope(frame_);
     frame_->EmitPush(cp);
-    __ mov(r0, Operand(slot->var()->name()));
-    frame_->EmitPush(r0);
+    frame_->EmitPush(Operand(slot->var()->name()));
 
     if (typeof_state == INSIDE_TYPEOF) {
       frame_->CallRuntime(Runtime::kLoadContextSlotNoReferenceError, 2);
@@ -2990,16 +2951,17 @@
     Register scratch = VirtualFrame::scratch0();
     TypeInfo info = type_info(slot);
     frame_->EmitPush(SlotOperand(slot, scratch), info);
+
     if (slot->var()->mode() == Variable::CONST) {
       // Const slots may contain 'the hole' value (the constant hasn't been
       // initialized yet) which needs to be converted into the 'undefined'
       // value.
       Comment cmnt(masm_, "[ Unhole const");
-      frame_->EmitPop(scratch);
+      Register tos = frame_->PopToRegister();
       __ LoadRoot(ip, Heap::kTheHoleValueRootIndex);
-      __ cmp(scratch, ip);
-      __ LoadRoot(scratch, Heap::kUndefinedValueRootIndex, eq);
-      frame_->EmitPush(scratch);
+      __ cmp(tos, ip);
+      __ LoadRoot(tos, Heap::kUndefinedValueRootIndex, eq);
+      frame_->EmitPush(tos);
     }
   }
 }
@@ -3007,6 +2969,7 @@
 
 void CodeGenerator::LoadFromSlotCheckForArguments(Slot* slot,
                                                   TypeofState state) {
+  VirtualFrame::RegisterAllocationScope scope(this);
   LoadFromSlot(slot, state);
 
   // Bail out quickly if we're not using lazy arguments allocation.
@@ -3015,17 +2978,15 @@
   // ... or if the slot isn't a non-parameter arguments slot.
   if (slot->type() == Slot::PARAMETER || !slot->is_arguments()) return;
 
-  VirtualFrame::SpilledScope spilled_scope(frame_);
-
-  // Load the loaded value from the stack into r0 but leave it on the
+  // Load the loaded value from the stack into a register but leave it on the
   // stack.
-  __ ldr(r0, MemOperand(sp, 0));
+  Register tos = frame_->Peek();
 
   // If the loaded value is the sentinel that indicates that we
   // haven't loaded the arguments object yet, we need to do it now.
   JumpTarget exit;
   __ LoadRoot(ip, Heap::kTheHoleValueRootIndex);
-  __ cmp(r0, ip);
+  __ cmp(tos, ip);
   exit.Branch(ne);
   frame_->Drop();
   StoreArgumentsObject(false);
@@ -3035,14 +2996,13 @@
 
 void CodeGenerator::StoreToSlot(Slot* slot, InitState init_state) {
   ASSERT(slot != NULL);
+  VirtualFrame::RegisterAllocationScope scope(this);
   if (slot->type() == Slot::LOOKUP) {
-    VirtualFrame::SpilledScope spilled_scope(frame_);
     ASSERT(slot->var()->is_dynamic());
 
     // For now, just do a runtime call.
     frame_->EmitPush(cp);
-    __ mov(r0, Operand(slot->var()->name()));
-    frame_->EmitPush(r0);
+    frame_->EmitPush(Operand(slot->var()->name()));
 
     if (init_state == CONST_INIT) {
       // Same as the case for a normal store, but ignores attribute
@@ -3071,7 +3031,7 @@
   } else {
     ASSERT(!slot->var()->is_dynamic());
     Register scratch = VirtualFrame::scratch0();
-    VirtualFrame::RegisterAllocationScope scope(this);
+    Register scratch2 = VirtualFrame::scratch1();
 
     // The frame must be spilled when branching to this target.
     JumpTarget exit;
@@ -3085,7 +3045,6 @@
       __ ldr(scratch, SlotOperand(slot, scratch));
       __ LoadRoot(ip, Heap::kTheHoleValueRootIndex);
       __ cmp(scratch, ip);
-      frame_->SpillAll();
       exit.Branch(ne);
     }
 
@@ -3104,18 +3063,18 @@
       // Skip write barrier if the written value is a smi.
       __ tst(tos, Operand(kSmiTagMask));
       // We don't use tos any more after here.
-      VirtualFrame::SpilledScope spilled_scope(frame_);
       exit.Branch(eq);
       // scratch is loaded with context when calling SlotOperand above.
       int offset = FixedArray::kHeaderSize + slot->index() * kPointerSize;
-      // r1 could be identical with tos, but that doesn't matter.
-      __ RecordWrite(scratch, Operand(offset), r3, r1);
+      // We need an extra register.  Until we have a way to do that in the
+      // virtual frame we will cheat and ask for a free TOS register.
+      Register scratch3 = frame_->GetTOSRegister();
+      __ RecordWrite(scratch, Operand(offset), scratch2, scratch3);
     }
     // If we definitely did not jump over the assignment, we do not need
     // to bind the exit label.  Doing so can defeat peephole
     // optimization.
     if (init_state == CONST_INIT || slot->type() == Slot::CONTEXT) {
-      frame_->SpillAll();
       exit.Bind();
     }
   }
@@ -3289,42 +3248,51 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ RexExp Literal");
 
+  Register tmp = VirtualFrame::scratch0();
+  // Free up a TOS register that can be used to push the literal.
+  Register literal = frame_->GetTOSRegister();
+
   // Retrieve the literal array and check the allocated entry.
 
   // Load the function of this activation.
-  __ ldr(r1, frame_->Function());
+  __ ldr(tmp, frame_->Function());
 
   // Load the literals array of the function.
-  __ ldr(r1, FieldMemOperand(r1, JSFunction::kLiteralsOffset));
+  __ ldr(tmp, FieldMemOperand(tmp, JSFunction::kLiteralsOffset));
 
   // Load the literal at the ast saved index.
   int literal_offset =
       FixedArray::kHeaderSize + node->literal_index() * kPointerSize;
-  __ ldr(r2, FieldMemOperand(r1, literal_offset));
+  __ ldr(literal, FieldMemOperand(tmp, literal_offset));
 
   JumpTarget done;
   __ LoadRoot(ip, Heap::kUndefinedValueRootIndex);
-  __ cmp(r2, ip);
+  __ cmp(literal, ip);
+  // This branch locks the virtual frame at the done label to match the
+  // one we have here, where the literal register is not on the stack and
+  // nothing is spilled.
   done.Branch(ne);
 
-  // If the entry is undefined we call the runtime system to computed
+  // If the entry is undefined we call the runtime system to compute
   // the literal.
-  frame_->EmitPush(r1);  // literal array  (0)
-  __ mov(r0, Operand(Smi::FromInt(node->literal_index())));
-  frame_->EmitPush(r0);  // literal index  (1)
-  __ mov(r0, Operand(node->pattern()));  // RegExp pattern (2)
-  frame_->EmitPush(r0);
-  __ mov(r0, Operand(node->flags()));  // RegExp flags   (3)
-  frame_->EmitPush(r0);
+  // literal array  (0)
+  frame_->EmitPush(tmp);
+  // literal index  (1)
+  frame_->EmitPush(Operand(Smi::FromInt(node->literal_index())));
+  // RegExp pattern (2)
+  frame_->EmitPush(Operand(node->pattern()));
+  // RegExp flags   (3)
+  frame_->EmitPush(Operand(node->flags()));
   frame_->CallRuntime(Runtime::kMaterializeRegExpLiteral, 4);
-  __ mov(r2, Operand(r0));
+  __ Move(literal, r0);
 
+  // This call to bind will get us back to the virtual frame we had before
+  // where things are not spilled and the literal register is not on the stack.
   done.Bind();
   // Push the literal.
-  frame_->EmitPush(r2);
+  frame_->EmitPush(literal);
   ASSERT_EQ(original_height + 1, frame_->height());
 }
 
@@ -3333,20 +3301,20 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ ObjectLiteral");
 
+  Register literal = frame_->GetTOSRegister();
   // Load the function of this activation.
-  __ ldr(r3, frame_->Function());
+  __ ldr(literal, frame_->Function());
   // Literal array.
-  __ ldr(r3, FieldMemOperand(r3, JSFunction::kLiteralsOffset));
+  __ ldr(literal, FieldMemOperand(literal, JSFunction::kLiteralsOffset));
+  frame_->EmitPush(literal);
   // Literal index.
-  __ mov(r2, Operand(Smi::FromInt(node->literal_index())));
+  frame_->EmitPush(Operand(Smi::FromInt(node->literal_index())));
   // Constant properties.
-  __ mov(r1, Operand(node->constant_properties()));
+  frame_->EmitPush(Operand(node->constant_properties()));
   // Should the object literal have fast elements?
-  __ mov(r0, Operand(Smi::FromInt(node->fast_elements() ? 1 : 0)));
-  frame_->EmitPushMultiple(4, r3.bit() | r2.bit() | r1.bit() | r0.bit());
+  frame_->EmitPush(Operand(Smi::FromInt(node->fast_elements() ? 1 : 0)));
   if (node->depth() > 1) {
     frame_->CallRuntime(Runtime::kCreateObjectLiteral, 4);
   } else {
@@ -3369,37 +3337,33 @@
         if (key->handle()->IsSymbol()) {
           Handle<Code> ic(Builtins::builtin(Builtins::StoreIC_Initialize));
           Load(value);
-          frame_->EmitPop(r0);
+          frame_->PopToR0();
+          // Fetch the object literal.
+          frame_->SpillAllButCopyTOSToR1();
           __ mov(r2, Operand(key->handle()));
-          __ ldr(r1, frame_->Top());  // Load the receiver.
           frame_->CallCodeObject(ic, RelocInfo::CODE_TARGET, 0);
           break;
         }
         // else fall through
       case ObjectLiteral::Property::PROTOTYPE: {
-        __ ldr(r0, frame_->Top());
-        frame_->EmitPush(r0);  // dup the result
+        frame_->Dup();
         Load(key);
         Load(value);
         frame_->CallRuntime(Runtime::kSetProperty, 3);
         break;
       }
       case ObjectLiteral::Property::SETTER: {
-        __ ldr(r0, frame_->Top());
-        frame_->EmitPush(r0);
+        frame_->Dup();
         Load(key);
-        __ mov(r0, Operand(Smi::FromInt(1)));
-        frame_->EmitPush(r0);
+        frame_->EmitPush(Operand(Smi::FromInt(1)));
         Load(value);
         frame_->CallRuntime(Runtime::kDefineAccessor, 4);
         break;
       }
       case ObjectLiteral::Property::GETTER: {
-        __ ldr(r0, frame_->Top());
-        frame_->EmitPush(r0);
+        frame_->Dup();
         Load(key);
-        __ mov(r0, Operand(Smi::FromInt(0)));
-        frame_->EmitPush(r0);
+        frame_->EmitPush(Operand(Smi::FromInt(0)));
         Load(value);
         frame_->CallRuntime(Runtime::kDefineAccessor, 4);
         break;
@@ -3414,16 +3378,16 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ ArrayLiteral");
 
+  Register tos = frame_->GetTOSRegister();
   // Load the function of this activation.
-  __ ldr(r2, frame_->Function());
+  __ ldr(tos, frame_->Function());
   // Load the literals array of the function.
-  __ ldr(r2, FieldMemOperand(r2, JSFunction::kLiteralsOffset));
-  __ mov(r1, Operand(Smi::FromInt(node->literal_index())));
-  __ mov(r0, Operand(node->constant_elements()));
-  frame_->EmitPushMultiple(3, r2.bit() | r1.bit() | r0.bit());
+  __ ldr(tos, FieldMemOperand(tos, JSFunction::kLiteralsOffset));
+  frame_->EmitPush(tos);
+  frame_->EmitPush(Operand(Smi::FromInt(node->literal_index())));
+  frame_->EmitPush(Operand(node->constant_elements()));
   int length = node->values()->length();
   if (node->depth() > 1) {
     frame_->CallRuntime(Runtime::kCreateArrayLiteral, 3);
@@ -3450,10 +3414,10 @@
 
     // The property must be set by generated code.
     Load(value);
-    frame_->EmitPop(r0);
-
+    frame_->PopToR0();
     // Fetch the object literal.
-    __ ldr(r1, frame_->Top());
+    frame_->SpillAllButCopyTOSToR1();
+
     // Get the elements array.
     __ ldr(r1, FieldMemOperand(r1, JSObject::kElementsOffset));
 
@@ -3863,7 +3827,6 @@
   // ------------------------------------------------------------------------
 
   if (var != NULL && var->is_possibly_eval()) {
-    VirtualFrame::SpilledScope spilled_scope(frame_);
     // ----------------------------------
     // JavaScript example: 'eval(arg)'  // eval is not known to be shadowed
     // ----------------------------------
@@ -3877,8 +3840,7 @@
     Load(function);
 
     // Allocate a frame slot for the receiver.
-    __ LoadRoot(r2, Heap::kUndefinedValueRootIndex);
-    frame_->EmitPush(r2);
+    frame_->EmitPushRoot(Heap::kUndefinedValueRootIndex);
 
     // Load the arguments.
     int arg_count = args->length();
@@ -3886,6 +3848,8 @@
       Load(args->at(i));
     }
 
+    VirtualFrame::SpilledScope spilled_scope(frame_);
+
     // If we know that eval can only be shadowed by eval-introduced
     // variables we attempt to load the global eval function directly
     // in generated code. If we succeed, there is no need to perform a
@@ -5201,7 +5165,6 @@
 #ifdef DEBUG
   int original_height = frame_->height();
 #endif
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   Comment cmnt(masm_, "[ UnaryOperation");
 
   Token::Value op = node->op();
@@ -5273,8 +5236,7 @@
         break;
 
       case Token::SUB: {
-        VirtualFrame::SpilledScope spilled(frame_);
-        frame_->EmitPop(r0);
+        frame_->PopToR0();
         GenericUnaryOpStub stub(Token::SUB, overwrite);
         frame_->CallStub(&stub, 0);
         frame_->EmitPush(r0);  // r0 has result
@@ -5282,23 +5244,28 @@
       }
 
       case Token::BIT_NOT: {
-        // smi check
-        VirtualFrame::SpilledScope spilled(frame_);
-        frame_->EmitPop(r0);
-        JumpTarget smi_label;
+        Register tos = frame_->PopToRegister();
+        JumpTarget not_smi_label;
         JumpTarget continue_label;
-        __ tst(r0, Operand(kSmiTagMask));
-        smi_label.Branch(eq);
+        // Smi check.
+        __ tst(tos, Operand(kSmiTagMask));
+        not_smi_label.Branch(ne);
 
+        __ mvn(tos, Operand(tos));
+        __ bic(tos, tos, Operand(kSmiTagMask));  // Bit-clear inverted smi-tag.
+        frame_->EmitPush(tos);
+        // The fast case is the first to jump to the continue label, so it gets
+        // to decide the virtual frame layout.
+        continue_label.Jump();
+
+        not_smi_label.Bind();
+        frame_->SpillAll();
+        __ Move(r0, tos);
         GenericUnaryOpStub stub(Token::BIT_NOT, overwrite);
         frame_->CallStub(&stub, 0);
-        continue_label.Jump();
+        frame_->EmitPush(r0);
 
-        smi_label.Bind();
-        __ mvn(r0, Operand(r0));
-        __ bic(r0, r0, Operand(kSmiTagMask));  // bit-clear inverted smi-tag
         continue_label.Bind();
-        frame_->EmitPush(r0);  // r0 has result
         break;
       }
 
@@ -5308,16 +5275,16 @@
         break;
 
       case Token::ADD: {
-        VirtualFrame::SpilledScope spilled(frame_);
-        frame_->EmitPop(r0);
+        Register tos = frame_->Peek();
         // Smi check.
         JumpTarget continue_label;
-        __ tst(r0, Operand(kSmiTagMask));
+        __ tst(tos, Operand(kSmiTagMask));
         continue_label.Branch(eq);
-        frame_->EmitPush(r0);
+
         frame_->InvokeBuiltin(Builtins::TO_NUMBER, CALL_JS, 1);
+        frame_->EmitPush(r0);
+
         continue_label.Bind();
-        frame_->EmitPush(r0);  // r0 has result
         break;
       }
       default:
@@ -5335,6 +5302,7 @@
   int original_height = frame_->height();
 #endif
   Comment cmnt(masm_, "[ CountOperation");
+  VirtualFrame::RegisterAllocationScope scope(this);
 
   bool is_postfix = node->is_postfix();
   bool is_increment = node->op() == Token::INC;
@@ -5478,7 +5446,6 @@
   // after evaluating the left hand side (due to the shortcut
   // semantics), but the compiler must (statically) know if the result
   // of compiling the binary operation is materialized or not.
-  VirtualFrame::SpilledScope spilled_scope(frame_);
   if (node->op() == Token::AND) {
     JumpTarget is_true;
     LoadCondition(node->left(), &is_true, false_target(), false);
@@ -5663,8 +5630,6 @@
     if (left_is_null || right_is_null) {
       Load(left_is_null ? right : left);
       Register tos = frame_->PopToRegister();
-      // JumpTargets can't cope with register allocation yet.
-      frame_->SpillAll();
       __ LoadRoot(ip, Heap::kNullValueRootIndex);
       __ cmp(tos, ip);
 
@@ -5707,9 +5672,6 @@
     LoadTypeofExpression(operation->expression());
     Register tos = frame_->PopToRegister();
 
-    // JumpTargets can't cope with register allocation yet.
-    frame_->SpillAll();
-
     Register scratch = VirtualFrame::scratch0();
 
     if (check->Equals(Heap::number_symbol())) {
@@ -5830,7 +5792,6 @@
       break;
 
     case Token::IN: {
-      VirtualFrame::SpilledScope scope(frame_);
       Load(left);
       Load(right);
       frame_->InvokeBuiltin(Builtins::IN, CALL_JS, 2);
@@ -5839,7 +5800,6 @@
     }
 
     case Token::INSTANCEOF: {
-      VirtualFrame::SpilledScope scope(frame_);
       Load(left);
       Load(right);
       InstanceofStub stub;
@@ -5937,10 +5897,15 @@
 };
 
 
+// Takes key and register in r0 and r1 or vice versa.  Returns result
+// in r0.
 void DeferredReferenceGetKeyedValue::Generate() {
   ASSERT((key_.is(r0) && receiver_.is(r1)) ||
          (key_.is(r1) && receiver_.is(r0)));
 
+  VirtualFrame copied_frame(*frame_state()->frame());
+  copied_frame.SpillAll();
+
   Register scratch1 = VirtualFrame::scratch0();
   Register scratch2 = VirtualFrame::scratch1();
   __ DecrementCounter(&Counters::keyed_load_inline, 1, scratch1, scratch2);
@@ -5961,6 +5926,13 @@
     // keyed load has been inlined.
     __ nop(PROPERTY_ACCESS_INLINED);
 
+    // Now go back to the frame that we entered with.  This will not overwrite
+    // the receiver or key registers since they were not in use when we came
+    // in.  The instructions emitted by this merge are skipped over by the
+    // inline load patching mechanism when looking for the branch instruction
+    // that tells it where the code to patch is.
+    copied_frame.MergeTo(frame_state()->frame());
+
     // Block the constant pool for one more instruction after leaving this
     // constant pool block scope to include the branch instruction ending the
     // deferred code.
@@ -6114,7 +6086,6 @@
     bool key_is_known_smi = frame_->KnownSmiAt(0);
     Register key = frame_->PopToRegister();
     Register receiver = frame_->PopToRegister(key);
-    VirtualFrame::SpilledScope spilled(frame_);
 
     // The deferred code expects key and receiver in registers.
     DeferredReferenceGetKeyedValue* deferred =
@@ -6152,10 +6123,12 @@
       // Get the elements array from the receiver and check that it
       // is not a dictionary.
       __ ldr(scratch1, FieldMemOperand(receiver, JSObject::kElementsOffset));
-      __ ldr(scratch2, FieldMemOperand(scratch1, JSObject::kMapOffset));
-      __ LoadRoot(ip, Heap::kFixedArrayMapRootIndex);
-      __ cmp(scratch2, ip);
-      deferred->Branch(ne);
+      if (FLAG_debug_code) {
+        __ ldr(scratch2, FieldMemOperand(scratch1, JSObject::kMapOffset));
+        __ LoadRoot(ip, Heap::kFixedArrayMapRootIndex);
+        __ cmp(scratch2, ip);
+        __ Assert(eq, "JSObject with fast elements map has slow elements");
+      }
 
       // Check that key is within bounds. Use unsigned comparison to handle
       // negative keys.
@@ -6176,7 +6149,7 @@
 
       __ mov(r0, scratch1);
       // Make sure that the expected number of instructions are generated.
-      ASSERT_EQ(kInlinedKeyedLoadInstructionsAfterPatch,
+      ASSERT_EQ(GetInlinedKeyedLoadInstructionsAfterPatch(),
                 masm_->InstructionsGeneratedSince(&check_inlined_codesize));
     }
 
@@ -6204,9 +6177,9 @@
     // Load the value, key and receiver from the stack.
     Register value = frame_->PopToRegister();
     Register key = frame_->PopToRegister(value);
+    VirtualFrame::SpilledScope spilled(frame_);
     Register receiver = r2;
     frame_->EmitPop(receiver);
-    VirtualFrame::SpilledScope spilled(frame_);
 
     // The deferred code expects value, key and receiver in registers.
     DeferredReferenceSetKeyedValue* deferred =
Index: src/arm/codegen-arm.h
===================================================================
--- src/arm/codegen-arm.h	(revision 4925)
+++ src/arm/codegen-arm.h	(working copy)
@@ -276,7 +276,9 @@
   static int InlineRuntimeCallArgumentsCount(Handle<String> name);
 
   // Constants related to patching of inlined load/store.
-  static const int kInlinedKeyedLoadInstructionsAfterPatch = 17;
+  static int GetInlinedKeyedLoadInstructionsAfterPatch() {
+    return FLAG_debug_code ? 27 : 13;
+  }
   static const int kInlinedKeyedStoreInstructionsAfterPatch = 5;
 
  private:
Index: src/arm/virtual-frame-arm.cc
===================================================================
--- src/arm/virtual-frame-arm.cc	(revision 4925)
+++ src/arm/virtual-frame-arm.cc	(working copy)
@@ -482,6 +482,32 @@
 }
 
 
+void VirtualFrame::SpillAllButCopyTOSToR1() {
+  switch (top_of_stack_state_) {
+    case NO_TOS_REGISTERS:
+      __ ldr(r1, MemOperand(sp, 0));
+      break;
+    case R0_TOS:
+      __ push(r0);
+      __ mov(r1, r0);
+      break;
+    case R1_TOS:
+      __ push(r1);
+      break;
+    case R0_R1_TOS:
+      __ Push(r1, r0);
+      __ mov(r1, r0);
+      break;
+    case R1_R0_TOS:
+      __ Push(r0, r1);
+      break;
+    default:
+      UNREACHABLE();
+  }
+  top_of_stack_state_ = NO_TOS_REGISTERS;
+}
+
+
 void VirtualFrame::SpillAllButCopyTOSToR1R0() {
   switch (top_of_stack_state_) {
     case NO_TOS_REGISTERS:
@@ -524,6 +550,24 @@
 }
 
 
+Register VirtualFrame::Peek2() {
+  AssertIsNotSpilled();
+  switch (top_of_stack_state_) {
+    case NO_TOS_REGISTERS:
+    case R0_TOS:
+    case R0_R1_TOS:
+      MergeTOSTo(R0_R1_TOS);
+      return r1;
+    case R1_TOS:
+    case R1_R0_TOS:
+      MergeTOSTo(R1_R0_TOS);
+      return r0;
+  }
+  UNREACHABLE();
+  return no_reg;
+}
+
+
 void VirtualFrame::Dup() {
   if (SpilledScope::is_spilled()) {
     __ ldr(ip, MemOperand(sp, 0));
Index: src/arm/virtual-frame-arm.h
===================================================================
--- src/arm/virtual-frame-arm.h	(revision 4925)
+++ src/arm/virtual-frame-arm.h	(working copy)
@@ -189,12 +189,15 @@
     return (tos_known_smi_map_ & (~other->tos_known_smi_map_)) == 0;
   }
 
+  inline void ForgetTypeInfo() {
+    tos_known_smi_map_ = 0;
+  }
+
   // Detach a frame from its code generator, perhaps temporarily.  This
   // tells the register allocator that it is free to use frame-internal
   // registers.  Used when the code generator's frame is switched from this
   // one to NULL by an unconditional jump.
   void DetachFromCodeGenerator() {
-    AssertIsSpilled();
   }
 
   // (Re)attach a frame to its code generator.  This informs the register
@@ -202,7 +205,6 @@
   // Used when a code generator's frame is switched from NULL to this one by
   // binding a label.
   void AttachToCodeGenerator() {
-    AssertIsSpilled();
   }
 
   // Emit code for the physical JS entry and exit frame sequences.  After
@@ -330,6 +332,10 @@
   // must be copied to a scratch register before modification.
   Register Peek();
 
+  // Look at the value beneath the top of the stack.  The register returned is
+  // aliased and must be copied to a scratch register before modification.
+  Register Peek2();
+
   // Duplicate the top of stack.
   void Dup();
 
@@ -339,6 +345,9 @@
   // Flushes all registers, but it puts a copy of the top-of-stack in r0.
   void SpillAllButCopyTOSToR0();
 
+  // Flushes all registers, but it puts a copy of the top-of-stack in r1.
+  void SpillAllButCopyTOSToR1();
+
   // Flushes all registers, but it puts a copy of the top-of-stack in r1
   // and the next value on the stack in r0.
   void SpillAllButCopyTOSToR1R0();
Index: src/arm/ic-arm.cc
===================================================================
--- src/arm/ic-arm.cc	(revision 4925)
+++ src/arm/ic-arm.cc	(working copy)
@@ -47,71 +47,97 @@
 
 #define __ ACCESS_MASM(masm)
 
+
+static void GenerateGlobalInstanceTypeCheck(MacroAssembler* masm,
+                                            Register type,
+                                            Label* global_object) {
+  // Register usage:
+  //   type: holds the receiver instance type on entry.
+  __ cmp(type, Operand(JS_GLOBAL_OBJECT_TYPE));
+  __ b(eq, global_object);
+  __ cmp(type, Operand(JS_BUILTINS_OBJECT_TYPE));
+  __ b(eq, global_object);
+  __ cmp(type, Operand(JS_GLOBAL_PROXY_TYPE));
+  __ b(eq, global_object);
+}
+
+
+// Generated code falls through if the receiver is a regular non-global
+// JS object with slow properties and no interceptors.
+static void GenerateDictionaryLoadReceiverCheck(MacroAssembler* masm,
+                                                Register receiver,
+                                                Register elements,
+                                                Register t0,
+                                                Register t1,
+                                                Label* miss) {
+  // Register usage:
+  //   receiver: holds the receiver on entry and is unchanged.
+  //   elements: holds the property dictionary on fall through.
+  // Scratch registers:
+  //   t0: used to holds the receiver map.
+  //   t1: used to holds the receiver instance type, receiver bit mask and
+  //       elements map.
+
+  // Check that the receiver isn't a smi.
+  __ tst(receiver, Operand(kSmiTagMask));
+  __ b(eq, miss);
+
+  // Check that the receiver is a valid JS object.
+  __ CompareObjectType(receiver, t0, t1, FIRST_JS_OBJECT_TYPE);
+  __ b(lt, miss);
+
+  // If this assert fails, we have to check upper bound too.
+  ASSERT(LAST_TYPE == JS_FUNCTION_TYPE);
+
+  GenerateGlobalInstanceTypeCheck(masm, t1, miss);
+
+  // Check that the global object does not require access checks.
+  __ ldrb(t1, FieldMemOperand(t0, Map::kBitFieldOffset));
+  __ tst(t1, Operand((1 << Map::kIsAccessCheckNeeded) |
+                     (1 << Map::kHasNamedInterceptor)));
+  __ b(nz, miss);
+
+  __ ldr(elements, FieldMemOperand(receiver, JSObject::kPropertiesOffset));
+  __ ldr(t1, FieldMemOperand(elements, HeapObject::kMapOffset));
+  __ LoadRoot(ip, Heap::kHashTableMapRootIndex);
+  __ cmp(t1, ip);
+  __ b(nz, miss);
+}
+
+
 // Helper function used from LoadIC/CallIC GenerateNormal.
-// receiver: Receiver. It is not clobbered if a jump to the miss label is
-//           done
+//
+// elements: Property dictionary. It is not clobbered if a jump to the miss
+//           label is done.
 // name:     Property name. It is not clobbered if a jump to the miss label is
 //           done
 // result:   Register for the result. It is only updated if a jump to the miss
-//           label is not done. Can be the same as receiver or name clobbering
+//           label is not done. Can be the same as elements or name clobbering
 //           one of these in the case of not jumping to the miss label.
-// The three scratch registers need to be different from the receiver, name and
+// The two scratch registers need to be different from elements, name and
 // result.
+// The generated code assumes that the receiver has slow properties,
+// is not a global object and does not have interceptors.
 static void GenerateDictionaryLoad(MacroAssembler* masm,
                                    Label* miss,
-                                   Register receiver,
+                                   Register elements,
                                    Register name,
                                    Register result,
                                    Register scratch1,
-                                   Register scratch2,
-                                   Register scratch3,
-                                   DictionaryCheck check_dictionary) {
+                                   Register scratch2) {
   // Main use of the scratch registers.
-  // scratch1: Used to hold the property dictionary.
-  // scratch2: Used as temporary and to hold the capacity of the property
+  // scratch1: Used as temporary and to hold the capacity of the property
   //           dictionary.
-  // scratch3: Used as temporary.
+  // scratch2: Used as temporary.
 
   Label done;
 
-  // Check for the absence of an interceptor.
-  // Load the map into scratch1.
-  __ ldr(scratch1, FieldMemOperand(receiver, JSObject::kMapOffset));
-
-  // Bail out if the receiver has a named interceptor.
-  __ ldrb(scratch2, FieldMemOperand(scratch1, Map::kBitFieldOffset));
-  __ tst(scratch2, Operand(1 << Map::kHasNamedInterceptor));
-  __ b(nz, miss);
-
-  // Bail out if we have a JS global proxy object.
-  __ ldrb(scratch2, FieldMemOperand(scratch1, Map::kInstanceTypeOffset));
-  __ cmp(scratch2, Operand(JS_GLOBAL_PROXY_TYPE));
-  __ b(eq, miss);
-
-  // Possible work-around for http://crbug.com/16276.
-  // See also: http://codereview.chromium.org/155418.
-  __ cmp(scratch2, Operand(JS_GLOBAL_OBJECT_TYPE));
-  __ b(eq, miss);
-  __ cmp(scratch2, Operand(JS_BUILTINS_OBJECT_TYPE));
-  __ b(eq, miss);
-
-  // Load the properties array.
-  __ ldr(scratch1, FieldMemOperand(receiver, JSObject::kPropertiesOffset));
-
-  // Check that the properties array is a dictionary.
-  if (check_dictionary == CHECK_DICTIONARY) {
-    __ ldr(scratch2, FieldMemOperand(scratch1, HeapObject::kMapOffset));
-    __ LoadRoot(ip, Heap::kHashTableMapRootIndex);
-    __ cmp(scratch2, ip);
-    __ b(ne, miss);
-  }
-
   // Compute the capacity mask.
   const int kCapacityOffset = StringDictionary::kHeaderSize +
       StringDictionary::kCapacityIndex * kPointerSize;
-  __ ldr(scratch2, FieldMemOperand(scratch1, kCapacityOffset));
-  __ mov(scratch2, Operand(scratch2, ASR, kSmiTagSize));  // convert smi to int
-  __ sub(scratch2, scratch2, Operand(1));
+  __ ldr(scratch1, FieldMemOperand(elements, kCapacityOffset));
+  __ mov(scratch1, Operand(scratch1, ASR, kSmiTagSize));  // convert smi to int
+  __ sub(scratch1, scratch1, Operand(1));
 
   const int kElementsStartOffset = StringDictionary::kHeaderSize +
       StringDictionary::kElementsStartIndex * kPointerSize;
@@ -122,26 +148,26 @@
   static const int kProbes = 4;
   for (int i = 0; i < kProbes; i++) {
     // Compute the masked index: (hash + i + i * i) & mask.
-    __ ldr(scratch3, FieldMemOperand(name, String::kHashFieldOffset));
+    __ ldr(scratch2, FieldMemOperand(name, String::kHashFieldOffset));
     if (i > 0) {
       // Add the probe offset (i + i * i) left shifted to avoid right shifting
       // the hash in a separate instruction. The value hash + i + i * i is right
       // shifted in the following and instruction.
       ASSERT(StringDictionary::GetProbeOffset(i) <
              1 << (32 - String::kHashFieldOffset));
-      __ add(scratch3, scratch3, Operand(
+      __ add(scratch2, scratch2, Operand(
           StringDictionary::GetProbeOffset(i) << String::kHashShift));
     }
-    __ and_(scratch3, scratch2, Operand(scratch3, LSR, String::kHashShift));
+    __ and_(scratch2, scratch1, Operand(scratch2, LSR, String::kHashShift));
 
     // Scale the index by multiplying by the element size.
     ASSERT(StringDictionary::kEntrySize == 3);
-    // scratch3 = scratch3 * 3.
-    __ add(scratch3, scratch3, Operand(scratch3, LSL, 1));
+    // scratch2 = scratch2 * 3.
+    __ add(scratch2, scratch2, Operand(scratch2, LSL, 1));
 
     // Check if the key is identical to the name.
-    __ add(scratch3, scratch1, Operand(scratch3, LSL, 2));
-    __ ldr(ip, FieldMemOperand(scratch3, kElementsStartOffset));
+    __ add(scratch2, elements, Operand(scratch2, LSL, 2));
+    __ ldr(ip, FieldMemOperand(scratch2, kElementsStartOffset));
     __ cmp(name, Operand(ip));
     if (i != kProbes - 1) {
       __ b(eq, &done);
@@ -151,15 +177,15 @@
   }
 
   // Check that the value is a normal property.
-  __ bind(&done);  // scratch3 == scratch1 + 4 * index
-  __ ldr(scratch2,
-         FieldMemOperand(scratch3, kElementsStartOffset + 2 * kPointerSize));
-  __ tst(scratch2, Operand(PropertyDetails::TypeField::mask() << kSmiTagSize));
+  __ bind(&done);  // scratch2 == elements + 4 * index
+  __ ldr(scratch1,
+         FieldMemOperand(scratch2, kElementsStartOffset + 2 * kPointerSize));
+  __ tst(scratch1, Operand(PropertyDetails::TypeField::mask() << kSmiTagSize));
   __ b(ne, miss);
 
   // Get the value at the masked, scaled index and return.
   __ ldr(result,
-         FieldMemOperand(scratch3, kElementsStartOffset + 1 * kPointerSize));
+         FieldMemOperand(scratch2, kElementsStartOffset + 1 * kPointerSize));
 }
 
 
@@ -310,6 +336,7 @@
                                            Register receiver,
                                            Register scratch1,
                                            Register scratch2,
+                                           int interceptor_bit,
                                            Label* slow) {
   // Check that the object isn't a smi.
   __ BranchOnSmi(receiver, slow);
@@ -317,8 +344,9 @@
   __ ldr(scratch1, FieldMemOperand(receiver, HeapObject::kMapOffset));
   // Check bit field.
   __ ldrb(scratch2, FieldMemOperand(scratch1, Map::kBitFieldOffset));
-  __ tst(scratch2, Operand(KeyedLoadIC::kSlowCaseBitFieldMask));
-  __ b(ne, slow);
+  __ tst(scratch2,
+         Operand((1 << Map::kIsAccessCheckNeeded) | (1 << interceptor_bit)));
+  __ b(nz, slow);
   // Check that the object is some kind of JS object EXCEPT JS Value type.
   // In the case that the object is a value-wrapper object,
   // we enter the runtime system to make sure that indexing into string
@@ -502,13 +530,11 @@
 }
 
 
-static void GenerateNormalHelper(MacroAssembler* masm,
-                                 int argc,
-                                 bool is_global_object,
-                                 Label* miss,
-                                 Register scratch) {
-  // Search dictionary - put result in register r1.
-  GenerateDictionaryLoad(masm, miss, r1, r2, r1, r0, r3, r4, CHECK_DICTIONARY);
+static void GenerateFunctionTailCall(MacroAssembler* masm,
+                                     int argc,
+                                     Label* miss,
+                                     Register scratch) {
+  // r1: function
 
   // Check that the value isn't a smi.
   __ tst(r1, Operand(kSmiTagMask));
@@ -518,13 +544,6 @@
   __ CompareObjectType(r1, scratch, scratch, JS_FUNCTION_TYPE);
   __ b(ne, miss);
 
-  // Patch the receiver with the global proxy if necessary.
-  if (is_global_object) {
-    __ ldr(r0, MemOperand(sp, argc * kPointerSize));
-    __ ldr(r0, FieldMemOperand(r0, GlobalObject::kGlobalReceiverOffset));
-    __ str(r0, MemOperand(sp, argc * kPointerSize));
-  }
-
   // Invoke the function.
   ParameterCount actual(argc);
   __ InvokeFunction(r1, actual, JUMP_FUNCTION);
@@ -536,54 +555,19 @@
   //  -- r2    : name
   //  -- lr    : return address
   // -----------------------------------
-  Label miss, global_object, non_global_object;
+  Label miss;
 
   // Get the receiver of the function from the stack into r1.
   __ ldr(r1, MemOperand(sp, argc * kPointerSize));
 
-  // Check that the receiver isn't a smi.
-  __ tst(r1, Operand(kSmiTagMask));
-  __ b(eq, &miss);
+  GenerateDictionaryLoadReceiverCheck(masm, r1, r0, r3, r4, &miss);
 
-  // Check that the receiver is a valid JS object.  Put the map in r3.
-  __ CompareObjectType(r1, r3, r0, FIRST_JS_OBJECT_TYPE);
-  __ b(lt, &miss);
+  // r0: elements
+  // Search the dictionary - put result in register r1.
+  GenerateDictionaryLoad(masm, &miss, r0, r2, r1, r3, r4);
 
-  // If this assert fails, we have to check upper bound too.
-  ASSERT(LAST_TYPE == JS_FUNCTION_TYPE);
+  GenerateFunctionTailCall(masm, argc, &miss, r4);
 
-  // Check for access to global object.
-  __ cmp(r0, Operand(JS_GLOBAL_OBJECT_TYPE));
-  __ b(eq, &global_object);
-  __ cmp(r0, Operand(JS_BUILTINS_OBJECT_TYPE));
-  __ b(ne, &non_global_object);
-
-  // Accessing global object: Load and invoke.
-  __ bind(&global_object);
-  // Check that the global object does not require access checks.
-  __ ldrb(r3, FieldMemOperand(r3, Map::kBitFieldOffset));
-  __ tst(r3, Operand(1 << Map::kIsAccessCheckNeeded));
-  __ b(ne, &miss);
-  GenerateNormalHelper(masm, argc, true, &miss, r4);
-
-  // Accessing non-global object: Check for access to global proxy.
-  Label global_proxy, invoke;
-  __ bind(&non_global_object);
-  __ cmp(r0, Operand(JS_GLOBAL_PROXY_TYPE));
-  __ b(eq, &global_proxy);
-  // Check that the non-global, non-global-proxy object does not
-  // require access checks.
-  __ ldrb(r3, FieldMemOperand(r3, Map::kBitFieldOffset));
-  __ tst(r3, Operand(1 << Map::kIsAccessCheckNeeded));
-  __ b(ne, &miss);
-  __ bind(&invoke);
-  GenerateNormalHelper(masm, argc, false, &miss, r4);
-
-  // Global object access: Check access rights.
-  __ bind(&global_proxy);
-  __ CheckAccessGlobalProxy(r1, r0, &miss);
-  __ b(&invoke);
-
   __ bind(&miss);
 }
 
@@ -594,6 +578,12 @@
   //  -- lr    : return address
   // -----------------------------------
 
+  if (id == IC::kCallIC_Miss) {
+    __ IncrementCounter(&Counters::call_miss, 1, r3, r4);
+  } else {
+    __ IncrementCounter(&Counters::keyed_call_miss, 1, r3, r4);
+  }
+
   // Get the receiver of the function from the stack.
   __ ldr(r3, MemOperand(sp, argc * kPointerSize));
 
@@ -614,23 +604,26 @@
   __ LeaveInternalFrame();
 
   // Check if the receiver is a global object of some sort.
-  Label invoke, global;
-  __ ldr(r2, MemOperand(sp, argc * kPointerSize));  // receiver
-  __ tst(r2, Operand(kSmiTagMask));
-  __ b(eq, &invoke);
-  __ CompareObjectType(r2, r3, r3, JS_GLOBAL_OBJECT_TYPE);
-  __ b(eq, &global);
-  __ cmp(r3, Operand(JS_BUILTINS_OBJECT_TYPE));
-  __ b(ne, &invoke);
+  // This can happen only for regular CallIC but not KeyedCallIC.
+  if (id == IC::kCallIC_Miss) {
+    Label invoke, global;
+    __ ldr(r2, MemOperand(sp, argc * kPointerSize));  // receiver
+    __ tst(r2, Operand(kSmiTagMask));
+    __ b(eq, &invoke);
+    __ CompareObjectType(r2, r3, r3, JS_GLOBAL_OBJECT_TYPE);
+    __ b(eq, &global);
+    __ cmp(r3, Operand(JS_BUILTINS_OBJECT_TYPE));
+    __ b(ne, &invoke);
 
-  // Patch the receiver on the stack.
-  __ bind(&global);
-  __ ldr(r2, FieldMemOperand(r2, GlobalObject::kGlobalReceiverOffset));
-  __ str(r2, MemOperand(sp, argc * kPointerSize));
+    // Patch the receiver on the stack.
+    __ bind(&global);
+    __ ldr(r2, FieldMemOperand(r2, GlobalObject::kGlobalReceiverOffset));
+    __ str(r2, MemOperand(sp, argc * kPointerSize));
+    __ bind(&invoke);
+  }
 
   // Invoke the function.
   ParameterCount actual(argc);
-  __ bind(&invoke);
   __ InvokeFunction(r1, actual, JUMP_FUNCTION);
 }
 
@@ -698,7 +691,8 @@
   // Now the key is known to be a smi. This place is also jumped to from below
   // where a numeric string is converted to a smi.
 
-  GenerateKeyedLoadReceiverCheck(masm, r1, r0, r3, &slow_call);
+  GenerateKeyedLoadReceiverCheck(
+      masm, r1, r0, r3, Map::kHasIndexedInterceptor, &slow_call);
 
   GenerateFastArrayLoad(
       masm, r1, r2, r4, r3, r0, r1, &check_number_dictionary, &slow_load);
@@ -708,15 +702,8 @@
   // receiver in r1 is not used after this point.
   // r2: key
   // r1: function
+  GenerateFunctionTailCall(masm, argc, &slow_call, r0);
 
-  // Check that the value in r1 is a JSFunction.
-  __ BranchOnSmi(r1, &slow_call);
-  __ CompareObjectType(r1, r0, r0, JS_FUNCTION_TYPE);
-  __ b(ne, &slow_call);
-  // Invoke the function.
-  ParameterCount actual(argc);
-  __ InvokeFunction(r1, actual, JUMP_FUNCTION);
-
   __ bind(&check_number_dictionary);
   // r2: key
   // r3: elements map
@@ -751,16 +738,16 @@
   // If the receiver is a regular JS object with slow properties then do
   // a quick inline probe of the receiver's dictionary.
   // Otherwise do the monomorphic cache probe.
-  GenerateKeyedLoadReceiverCheck(masm, r1, r0, r3, &lookup_monomorphic_cache);
+  GenerateKeyedLoadReceiverCheck(
+      masm, r1, r0, r3, Map::kHasNamedInterceptor, &lookup_monomorphic_cache);
 
-  __ ldr(r3, FieldMemOperand(r1, JSObject::kPropertiesOffset));
-  __ ldr(r3, FieldMemOperand(r3, HeapObject::kMapOffset));
+  __ ldr(r0, FieldMemOperand(r1, JSObject::kPropertiesOffset));
+  __ ldr(r3, FieldMemOperand(r0, HeapObject::kMapOffset));
   __ LoadRoot(ip, Heap::kHashTableMapRootIndex);
   __ cmp(r3, ip);
   __ b(ne, &lookup_monomorphic_cache);
 
-  GenerateDictionaryLoad(
-      masm, &slow_load, r1, r2, r1, r0, r3, r4, DICTIONARY_CHECK_DONE);
+  GenerateDictionaryLoad(masm, &slow_load, r0, r2, r1, r3, r4);
   __ IncrementCounter(&Counters::keyed_call_generic_lookup_dict, 1, r0, r3);
   __ jmp(&do_call);
 
@@ -826,36 +813,14 @@
   //  -- r0    : receiver
   //  -- sp[0] : receiver
   // -----------------------------------
-  Label miss, probe, global;
+  Label miss;
 
-  // Check that the receiver isn't a smi.
-  __ tst(r0, Operand(kSmiTagMask));
-  __ b(eq, &miss);
+  GenerateDictionaryLoadReceiverCheck(masm, r0, r1, r3, r4, &miss);
 
-  // Check that the receiver is a valid JS object.  Put the map in r3.
-  __ CompareObjectType(r0, r3, r1, FIRST_JS_OBJECT_TYPE);
-  __ b(lt, &miss);
-  // If this assert fails, we have to check upper bound too.
-  ASSERT(LAST_TYPE == JS_FUNCTION_TYPE);
-
-  // Check for access to global object (unlikely).
-  __ cmp(r1, Operand(JS_GLOBAL_PROXY_TYPE));
-  __ b(eq, &global);
-
-  // Check for non-global object that requires access check.
-  __ ldrb(r3, FieldMemOperand(r3, Map::kBitFieldOffset));
-  __ tst(r3, Operand(1 << Map::kIsAccessCheckNeeded));
-  __ b(ne, &miss);
-
-  __ bind(&probe);
-  GenerateDictionaryLoad(masm, &miss, r0, r2, r0, r1, r3, r4, CHECK_DICTIONARY);
+  // r1: elements
+  GenerateDictionaryLoad(masm, &miss, r1, r2, r0, r3, r4);
   __ Ret();
 
-  // Global object access: Check access rights.
-  __ bind(&global);
-  __ CheckAccessGlobalProxy(r0, r1, &miss);
-  __ b(&probe);
-
   // Cache miss: Jump to runtime.
   __ bind(&miss);
   GenerateMiss(masm);
@@ -870,6 +835,8 @@
   //  -- sp[0] : receiver
   // -----------------------------------
 
+  __ IncrementCounter(&Counters::load_miss, 1, r3, r4);
+
   __ mov(r3, r0);
   __ Push(r3, r2);
 
@@ -963,7 +930,7 @@
   // Patch the map check.
   Address ldr_map_instr_address =
       inline_end_address -
-      (CodeGenerator::kInlinedKeyedLoadInstructionsAfterPatch *
+      (CodeGenerator::GetInlinedKeyedLoadInstructionsAfterPatch() *
       Assembler::kInstrSize);
   Assembler::set_target_address_at(ldr_map_instr_address,
                                    reinterpret_cast<Address>(map));
@@ -1013,6 +980,8 @@
   //  -- r1     : receiver
   // -----------------------------------
 
+  __ IncrementCounter(&Counters::keyed_load_miss, 1, r3, r4);
+
   __ Push(r1, r0);
 
   ExternalReference ref = ExternalReference(IC_Utility(kKeyedLoadIC_Miss));
@@ -1045,14 +1014,15 @@
   Register key = r0;
   Register receiver = r1;
 
-  GenerateKeyedLoadReceiverCheck(masm, receiver, r2, r3, &slow);
-
   // Check that the key is a smi.
   __ BranchOnNotSmi(key, &check_string);
   __ bind(&index_smi);
   // Now the key is known to be a smi. This place is also jumped to from below
   // where a numeric string is converted to a smi.
 
+  GenerateKeyedLoadReceiverCheck(
+      masm, receiver, r2, r3, Map::kHasIndexedInterceptor, &slow);
+
   GenerateFastArrayLoad(
       masm, receiver, key, r4, r3, r2, r0, &check_pixel_array, &slow);
   __ IncrementCounter(&Counters::keyed_load_generic_smi, 1, r2, r3);
@@ -1095,12 +1065,15 @@
   __ bind(&check_string);
   GenerateKeyStringCheck(masm, key, r2, r3, &index_string, &slow);
 
+  GenerateKeyedLoadReceiverCheck(
+      masm, receiver, r2, r3, Map::kHasNamedInterceptor, &slow);
+
   // If the receiver is a fast-case object, check the keyed lookup
   // cache. Otherwise probe the dictionary.
   __ ldr(r3, FieldMemOperand(r1, JSObject::kPropertiesOffset));
-  __ ldr(r3, FieldMemOperand(r3, HeapObject::kMapOffset));
+  __ ldr(r4, FieldMemOperand(r3, HeapObject::kMapOffset));
   __ LoadRoot(ip, Heap::kHashTableMapRootIndex);
-  __ cmp(r3, ip);
+  __ cmp(r4, ip);
   __ b(eq, &probe_dictionary);
 
   // Load the map of the receiver, compute the keyed lookup cache hash
@@ -1148,9 +1121,14 @@
   // Do a quick inline probe of the receiver's dictionary, if it
   // exists.
   __ bind(&probe_dictionary);
+  // r1: receiver
+  // r0: key
+  // r3: elements
+  __ ldr(r2, FieldMemOperand(r1, HeapObject::kMapOffset));
+  __ ldrb(r2, FieldMemOperand(r2, Map::kInstanceTypeOffset));
+  GenerateGlobalInstanceTypeCheck(masm, r2, &slow);
   // Load the property to r0.
-  GenerateDictionaryLoad(
-      masm, &slow, r1, r0, r0, r2, r3, r4, DICTIONARY_CHECK_DONE);
+  GenerateDictionaryLoad(masm, &slow, r3, r0, r0, r2, r4);
   __ IncrementCounter(&Counters::keyed_load_generic_symbol, 1, r2, r3);
   __ Ret();
 
Index: src/arm/jump-target-arm.cc
===================================================================
--- src/arm/jump-target-arm.cc	(revision 4925)
+++ src/arm/jump-target-arm.cc	(working copy)
@@ -61,9 +61,17 @@
   } else {
     // Clone the current frame to use as the expected one at the target.
     set_entry_frame(cgen()->frame());
+    // Zap the fall-through frame since the jump was unconditional.
     RegisterFile empty;
     cgen()->SetFrame(NULL, &empty);
   }
+  if (entry_label_.is_bound()) {
+    // You can't jump backwards to an already bound label unless you admitted
+    // up front that this was a bidirectional jump target.  Bidirectional jump
+    // targets will zap their type info when bound in case some later virtual
+    // frame with less precise type info branches to them.
+    ASSERT(direction_ != FORWARD_ONLY);
+  }
   __ jmp(&entry_label_);
 }
 
@@ -83,6 +91,13 @@
     // Clone the current frame to use as the expected one at the target.
     set_entry_frame(cgen()->frame());
   }
+  if (entry_label_.is_bound()) {
+    // You can't branch backwards to an already bound label unless you admitted
+    // up front that this was a bidirectional jump target.  Bidirectional jump
+    // targets will zap their type info when bound in case some later virtual
+    // frame with less precise type info branches to them.
+    ASSERT(direction_ != FORWARD_ONLY);
+  }
   __ b(cc, &entry_label_);
   if (cc == al) {
     cgen()->DeleteFrame();
@@ -121,6 +136,7 @@
   ASSERT(!cgen()->has_valid_frame() || cgen()->HasValidEntryRegisters());
 
   if (cgen()->has_valid_frame()) {
+    if (direction_ != FORWARD_ONLY) cgen()->frame()->ForgetTypeInfo();
     // If there is a current frame we can use it on the fall through.
     if (!entry_frame_set_) {
       entry_frame_ = *cgen()->frame();
Index: src/arm/macro-assembler-arm.cc
===================================================================
--- src/arm/macro-assembler-arm.cc	(revision 4925)
+++ src/arm/macro-assembler-arm.cc	(working copy)
@@ -1548,6 +1548,8 @@
 
 
 void MacroAssembler::Abort(const char* msg) {
+  Label abort_start;
+  bind(&abort_start);
   // We want to pass the msg string like a smi to avoid GC
   // problems, however msg is not guaranteed to be aligned
   // properly. Instead, we pass an aligned pointer that is
@@ -1571,6 +1573,17 @@
   push(r0);
   CallRuntime(Runtime::kAbort, 2);
   // will not return here
+  if (is_const_pool_blocked()) {
+    // If the calling code cares about the exact number of
+    // instructions generated, we insert padding here to keep the size
+    // of the Abort macro constant.
+    static const int kExpectedAbortInstructions = 10;
+    int abort_instructions = InstructionsGeneratedSince(&abort_start);
+    ASSERT(abort_instructions <= kExpectedAbortInstructions);
+    while (abort_instructions++ < kExpectedAbortInstructions) {
+      nop();
+    }
+  }
 }
 
 
Index: src/heap.h
===================================================================
--- src/heap.h	(revision 4925)
+++ src/heap.h	(working copy)
@@ -1005,6 +1005,7 @@
   static void CheckNewSpaceExpansionCriteria();
 
   static inline void IncrementYoungSurvivorsCounter(int survived) {
+    young_survivors_after_last_gc_ = survived;
     survived_since_last_expansion_ += survived;
   }
 
@@ -1272,6 +1273,55 @@
   // be replaced with a lazy compilable version.
   static void FlushCode();
 
+  static void UpdateSurvivalRateTrend(int start_new_space_size);
+
+  enum SurvivalRateTrend { INCREASING, STABLE, DECREASING, FLUCTUATING };
+
+  static const int kYoungSurvivalRateThreshold = 90;
+  static const int kYoungSurvivalRateAllowedDeviation = 15;
+
+  static int young_survivors_after_last_gc_;
+  static int high_survival_rate_period_length_;
+  static double survival_rate_;
+  static SurvivalRateTrend previous_survival_rate_trend_;
+  static SurvivalRateTrend survival_rate_trend_;
+
+  static void set_survival_rate_trend(SurvivalRateTrend survival_rate_trend) {
+    ASSERT(survival_rate_trend != FLUCTUATING);
+    previous_survival_rate_trend_ = survival_rate_trend_;
+    survival_rate_trend_ = survival_rate_trend;
+  }
+
+  static SurvivalRateTrend survival_rate_trend() {
+    if (survival_rate_trend_ == STABLE) {
+      return STABLE;
+    } else if (previous_survival_rate_trend_ == STABLE) {
+      return survival_rate_trend_;
+    } else if (survival_rate_trend_ != previous_survival_rate_trend_) {
+      return FLUCTUATING;
+    } else {
+      return survival_rate_trend_;
+    }
+  }
+
+  static bool IsStableOrIncreasingSurvivalTrend() {
+    switch (survival_rate_trend()) {
+      case STABLE:
+      case INCREASING:
+        return true;
+      default:
+        return false;
+    }
+  }
+
+  static bool IsIncreasingSurvivalTrend() {
+    return survival_rate_trend() == INCREASING;
+  }
+
+  static bool IsHighSurvivalRate() {
+    return high_survival_rate_period_length_ > 0;
+  }
+
   static const int kInitialSymbolTableSize = 2048;
   static const int kInitialEvalCacheSize = 64;
 
@@ -1877,6 +1927,10 @@
   struct Element {
     uint32_t in[2];
     Object* output;
+// alignment
+#ifdef NACL
+    uint32_t dummy;
+#endif   
   };
   union Converter {
     double dbl;
Index: src/execution.cc
===================================================================
--- src/execution.cc	(revision 4925)
+++ src/execution.cc	(working copy)
@@ -74,7 +74,7 @@
   if (receiver->IsGlobalObject()) {
     Handle<GlobalObject> global = Handle<GlobalObject>::cast(receiver);
     receiver = Handle<JSObject>(global->global_receiver());
-  }
+ }
 
   // Make sure that the global object of the context we're about to
   // make the current one is indeed a global object.
@@ -403,11 +403,11 @@
 
 // --- C a l l s   t o   n a t i v e s ---
 
-#define RETURN_NATIVE_CALL(name, argc, argv, has_pending_exception) \
+#define RETURN_NATIVE_CALL(name, argv, has_pending_exception) \
   do {                                                              \
-    Object** args[argc] = argv;                                     \
+    Object** args[1] = { argv };                                 \
     ASSERT(has_pending_exception != NULL);                          \
-    return Call(Top::name##_fun(), Top::builtins(), argc, args,     \
+    return Call(Top::name##_fun(), Top::builtins(), 1, args,        \
                 has_pending_exception);                             \
   } while (false)
 
@@ -429,44 +429,44 @@
 
 
 Handle<Object> Execution::ToNumber(Handle<Object> obj, bool* exc) {
-  RETURN_NATIVE_CALL(to_number, 1, { obj.location() }, exc);
+  RETURN_NATIVE_CALL(to_number, obj.location(), exc);
 }
 
 
 Handle<Object> Execution::ToString(Handle<Object> obj, bool* exc) {
-  RETURN_NATIVE_CALL(to_string, 1, { obj.location() }, exc);
+  RETURN_NATIVE_CALL(to_string, obj.location(), exc);
 }
 
 
 Handle<Object> Execution::ToDetailString(Handle<Object> obj, bool* exc) {
-  RETURN_NATIVE_CALL(to_detail_string, 1, { obj.location() }, exc);
+  RETURN_NATIVE_CALL(to_detail_string, obj.location(), exc);
 }
 
 
 Handle<Object> Execution::ToObject(Handle<Object> obj, bool* exc) {
   if (obj->IsJSObject()) return obj;
-  RETURN_NATIVE_CALL(to_object, 1, { obj.location() }, exc);
+  RETURN_NATIVE_CALL(to_object, obj.location(), exc);
 }
 
 
 Handle<Object> Execution::ToInteger(Handle<Object> obj, bool* exc) {
-  RETURN_NATIVE_CALL(to_integer, 1, { obj.location() }, exc);
+  RETURN_NATIVE_CALL(to_integer, obj.location(), exc);
 }
 
 
 Handle<Object> Execution::ToUint32(Handle<Object> obj, bool* exc) {
-  RETURN_NATIVE_CALL(to_uint32, 1, { obj.location() }, exc);
+  RETURN_NATIVE_CALL(to_uint32, obj.location(), exc);
 }
 
 
 Handle<Object> Execution::ToInt32(Handle<Object> obj, bool* exc) {
-  RETURN_NATIVE_CALL(to_int32, 1, { obj.location() }, exc);
+  RETURN_NATIVE_CALL(to_int32, obj.location(), exc);
 }
 
 
 Handle<Object> Execution::NewDate(double time, bool* exc) {
   Handle<Object> time_obj = Factory::NewNumber(time);
-  RETURN_NATIVE_CALL(create_date, 1, { time_obj.location() }, exc);
+  RETURN_NATIVE_CALL(create_date, time_obj.location(), exc);
 }
 
 
@@ -561,7 +561,7 @@
   Object** args[argc] = { recv.location(),
                           Handle<Object>::cast(fun).location(),
                           pos.location(),
-                          is_global.location() };
+                          is_global.location()};
   bool caught_exception = false;
   Handle<Object> result = TryCall(Top::get_stack_trace_line_fun(),
                                   Top::builtins(), argc, args,
Index: src/liveedit.cc
===================================================================
--- src/liveedit.cc	(revision 4925)
+++ src/liveedit.cc	(working copy)
@@ -1210,8 +1210,8 @@
 
   Address unused_stack_top = top_frame->sp();
   Address unused_stack_bottom = bottom_js_frame->fp()
-      - Debug::kFrameDropperFrameSize * kPointerSize  // Size of the new frame.
-      + kPointerSize;  // Bigger address end is exclusive.
+      - Debug::kFrameDropperFrameSize * kStackPointerSize  // Size of the new frame.
+      + kStackPointerSize;  // Bigger address end is exclusive.
 
   if (unused_stack_top > unused_stack_bottom) {
     return "Not enough space for frame dropper frame";
@@ -1231,7 +1231,7 @@
 
   for (Address a = unused_stack_top;
       a < unused_stack_bottom;
-      a += kPointerSize) {
+      a += kStackPointerSize) {
     Memory::Object_at(a) = Smi::FromInt(0);
   }
 
Index: src/version.cc
===================================================================
--- src/version.cc	(revision 4925)
+++ src/version.cc	(working copy)
@@ -34,9 +34,9 @@
 // cannot be changed without changing the SCons build script.
 #define MAJOR_VERSION     2
 #define MINOR_VERSION     2
-#define BUILD_NUMBER      19
+#define BUILD_NUMBER      20
 #define PATCH_LEVEL       0
-#define CANDIDATE_VERSION false
+#define CANDIDATE_VERSION true
 
 // Define SONAME to have the SCons build the put a specific SONAME into the
 // shared library instead the generic SONAME generated from the V8 version
Index: SConstruct
===================================================================
--- SConstruct	(revision 4925)
+++ SConstruct	(working copy)
@@ -54,6 +54,11 @@
 else:
   ARM_LINK_FLAGS = []
 
+NACLSDK=os.environ.get('NACLSDK')
+if NACLSDK is None:
+  NACLSDK=""
+
+
 # TODO: Sort these issues out properly but as a temporary solution for gcc 4.4
 # on linux we need these compiler flags to avoid crashes in the v8 test suite
 # and avoid dtoa.c strict aliasing issues
@@ -186,6 +191,16 @@
       'LIBPATH' : ['/usr/local/lib'],
       'CCFLAGS':      ['-ansi'],
     },
+    'os:nacl': {
+      'CCFLAGS'   : ['-ansi', '-fno-tree-vrp', '-fno-strict-aliasing', '-march=native'] + GCC_EXTRA_CCFLAGS,
+      'CPPDEFINES': ['NACL'],
+      'CPP'       : os.path.join(NACLSDK,'nacl-cpp'),
+      'CXX'       : os.path.join(NACLSDK,'nacl-g++'),
+      'CC'        : os.path.join(NACLSDK,'nacl-gcc'),
+      'AR'        : os.path.join(NACLSDK,'nacl-ar'),
+      'LD'        : os.path.join(NACLSDK,'nacl-ld'),
+      'RANLIB'    : os.path.join(NACLSDK,'nacl-ranlib'),
+    },
     'os:win32': {
       'CCFLAGS':      ['-DWIN32'],
       'CXXFLAGS':     ['-DWIN32'],
@@ -300,7 +315,6 @@
   'gcc': {
     'all': {
       'WARNINGFLAGS': ['-Wall',
-                       '-Werror',
                        '-W',
                        '-Wno-unused-parameter',
                        '-Wnon-virtual-dtor']
@@ -316,6 +330,9 @@
         }
       }
     },
+    'os:nacl': {
+      'WARNINGFLAGS': ['-pedantic']
+    },
     'os:macos': {
       'WARNINGFLAGS': ['-pedantic']
     },
@@ -403,7 +420,11 @@
     },
     'os:linux': {
       'LIBS':         ['pthread'],
+      'LINKFLAGS': [ "-Wl,--section-start,.rodata=0x10000000"],
     },
+    'os:nacl': {
+      'LIBS':         ['pthread'],
+    },
     'os:macos': {
       'LIBS':         ['pthread'],
     },
@@ -468,6 +489,16 @@
     'os:linux': {
       'LIBS':         ['pthread'],
     },
+    'os:nacl': {
+      'LIBS':     ['pthread'],
+      'CPP'       : os.path.join(NACLSDK,'nacl-cpp'),
+      'CXX'       : os.path.join(NACLSDK,'nacl-g++'),
+      'CC'        : os.path.join(NACLSDK,'nacl-gcc'),
+      'AR'        : os.path.join(NACLSDK,'nacl-ar'),
+      'LD'        : os.path.join(NACLSDK,'nacl-ld'),
+      'RANLIB'    : os.path.join(NACLSDK,'nacl-ranlib'),
+      'LINKFLAGS' : ["-Wl,--section-start,.rodata=0x10000000"],
+    },
     'os:macos': {
       'LIBS':         ['pthread'],
     },
@@ -671,7 +702,7 @@
     'help': 'the toolchain to use (' + TOOLCHAIN_GUESS + ')'
   },
   'os': {
-    'values': ['freebsd', 'linux', 'macos', 'win32', 'android', 'openbsd', 'solaris'],
+    'values': ['freebsd', 'linux', 'macos', 'win32', 'android', 'openbsd', 'solaris', 'nacl'],
     'default': OS_GUESS,
     'help': 'the os to build for (' + OS_GUESS + ')'
   },
